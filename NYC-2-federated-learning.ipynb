{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NYC_FL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    " # Original Federated Learning Model for the NYC Taxi DataSet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "69Zus3q5KKpv"
   },
   "source": [
    "#!pip install --upgrade tensorflow_federated_nightly\n",
    "#!pip install --upgrade nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O0AtRRZwKfnz",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "outputId": "ec8f53e1-bede-41f7-a6a1-2bf8249bbf6b"
   },
   "source": [
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Test the TFF is working:\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "b'Hello, World!'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRSBBlWyLG9h"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EiraMJg9uOb9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "bd845c7d-9072-46d5-ef12-b26e94566280"
   },
   "source": [
    "# read the dataset from Drive\n",
    "df = pd.read_csv(\"./ma_results/trips_with_zones_final.csv\")\n",
    "df = df.head(10000000)\n",
    "df.head()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                          medallion  pickup_week_day  pickup_hour  pickup_day  \\\n0  00005007A9F30E289E760362F69E4EAD                1            0           1   \n1  00005007A9F30E289E760362F69E4EAD                1            0           1   \n2  00005007A9F30E289E760362F69E4EAD                1            0           1   \n3  00005007A9F30E289E760362F69E4EAD                1            1           1   \n4  00005007A9F30E289E760362F69E4EAD                1            1           1   \n\n   pickup_month  dropoff_week_day  dropoff_hour  dropoff_day  dropoff_month  \\\n0             1                 1             0            1              1   \n1             1                 1             0            1              1   \n2             1                 1             1            1              1   \n3             1                 1             1            1              1   \n4             1                 1             1            1              1   \n\n   pickup_location_id  dropoff_location_id  \n0               162.0                262.0  \n1               262.0                239.0  \n2               239.0                236.0  \n3               236.0                 41.0  \n4                41.0                211.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>medallion</th>\n      <th>pickup_week_day</th>\n      <th>pickup_hour</th>\n      <th>pickup_day</th>\n      <th>pickup_month</th>\n      <th>dropoff_week_day</th>\n      <th>dropoff_hour</th>\n      <th>dropoff_day</th>\n      <th>dropoff_month</th>\n      <th>pickup_location_id</th>\n      <th>dropoff_location_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00005007A9F30E289E760362F69E4EAD</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>162.0</td>\n      <td>262.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00005007A9F30E289E760362F69E4EAD</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>262.0</td>\n      <td>239.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00005007A9F30E289E760362F69E4EAD</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>239.0</td>\n      <td>236.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00005007A9F30E289E760362F69E4EAD</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>236.0</td>\n      <td>41.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00005007A9F30E289E760362F69E4EAD</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>41.0</td>\n      <td>211.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ggIfgQxKuOcC",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "4a248d3b-8b73-4f2b-d7f3-8f9bcaaedbd6"
   },
   "source": [
    "# Check dtypes of the attributes\n",
    "df.dtypes"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "medallion               object\npickup_week_day          int64\npickup_hour              int64\npickup_day               int64\npickup_month             int64\ndropoff_week_day         int64\ndropoff_hour             int64\ndropoff_day              int64\ndropoff_month            int64\npickup_location_id     float64\ndropoff_location_id    float64\ndtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P_EEwnknuOcG",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "0904a854-cabc-4bac-ffc9-d268c1e214a1"
   },
   "source": [
    "# Cast the columns type to int32\n",
    "dictionary = {'pickup_week_day': 'int32', 'pickup_hour': 'int32', 'pickup_day': 'int32', 'pickup_month': 'int32', 'dropoff_week_day': 'int32', 'dropoff_hour': 'int32', 'dropoff_day': 'int32', 'dropoff_month': 'int32', 'pickup_location_id':'int32', 'dropoff_location_id':'int32'}\n",
    "df = df.astype(dictionary, copy=True)\n",
    "df.dtypes"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "medallion              object\npickup_week_day         int32\npickup_hour             int32\npickup_day              int32\npickup_month            int32\ndropoff_week_day        int32\ndropoff_hour            int32\ndropoff_day             int32\ndropoff_month           int32\npickup_location_id      int32\ndropoff_location_id     int32\ndtype: object"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UoTlUvLIPQFN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "333c768f-1188-4d8e-f4d1-2c62aabff0b3"
   },
   "source": [
    "df.medallion.value_counts().loc[df.medallion.value_counts().index[100]]"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "1765"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ep1BmXDVPxC1"
   },
   "source": [
    "Because there are too many taxis (over 9000) it is better to take the 100 taxi with the major number of records"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-jsCxF6RPQC5"
   },
   "source": [
    "# Pick taxis with at least 1000 records\n",
    "count = df.medallion.value_counts()\n",
    "\n",
    "medallions = count.loc[count.index[:100]].index # count >= 1000\n",
    "test_medallions = count.loc[count.index[100:105]].index\n",
    "val_medallions = count.loc[count.index[105:110]].index\n",
    "\n",
    "df_test = df.loc[df.medallion.isin(test_medallions)].copy()\n",
    "df_val = df.loc[df.medallion.isin(val_medallions)].copy()\n",
    "df = df.loc[df.medallion.isin(medallions)]"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oH6GxKzwc7K"
   },
   "source": [
    "We can use the other taxis to create a local test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WESdy69_Ljgq"
   },
   "source": [
    "# function to remove duplicates\n",
    "def create_sequence(locations): \n",
    "  # Flatten the list of places\n",
    "  sequence = np.reshape(locations.values, [-1])\n",
    "\n",
    "  # Create a temporary array of the same lenght of the sequece of locations\n",
    "  copy = np.zeros(sequence.shape[0], dtype=np.int32)\n",
    "\n",
    "  # Copy the sequence of location in the copy array but shifted right by 1 position\n",
    "  # The last location does not need to be copied, it can't be a duplicate\n",
    "  copy[1:] = sequence[:sequence.shape[0]-1]\n",
    "\n",
    "  # Where we get 0 it can be a possible duplicated\n",
    "  duplicated = sequence - copy\n",
    "\n",
    "  # indices where the subtraction gives 0\n",
    "  idx = np.where(duplicated == 0)[0]\n",
    "\n",
    "  # Find where the position of the zeros are even\n",
    "  even = idx%2 == 0\n",
    "\n",
    "  # List the indices where the position is even and the subtraction gave 0\n",
    "  to_drop = idx[even]\n",
    "\n",
    "  # Remove the duplicates\n",
    "  clean_sequence = np.delete(sequence, to_drop)\n",
    "  return clean_sequence, to_drop"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHECUQleRxHH"
   },
   "source": [
    "Now we need to create the location sequence for each user"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uzze3vF-R8OZ"
   },
   "source": [
    "def df_to_location_sequence(df):\n",
    "  \n",
    "  # take just the columns we need\n",
    "  locations = df[['pickup_location_id','dropoff_location_id']].copy()\n",
    "  locations = locations.astype('int32')\n",
    "\n",
    "\n",
    "  # define the indices to keep trace of the locations\n",
    "  x = np.arange(0, locations.values.shape[0])\n",
    "\n",
    "  pos = np.array([x,x]).T\n",
    "  pos = np.reshape(pos, [-1])\n",
    "\n",
    "  # Represent whether the location is a pickup or a dropoff\n",
    "  pick = np.zeros(locations.values.shape[0], dtype=int)\n",
    "  drop = np.ones(locations.values.shape[0], dtype=int)\n",
    "\n",
    "  loc = np.array([pick,drop]).T\n",
    "  loc = np.reshape(loc, [-1])\n",
    "\n",
    "  # Generate the sequence of places\n",
    "  sequence, duplicates = create_sequence(locations)\n",
    "\n",
    "  # We use now the indices of the duplicated locations to clean also the array of rows and the array of location types\n",
    "  pos = np.delete(pos, duplicates)\n",
    "  loc = np.delete(loc, duplicates)\n",
    "\n",
    "  # Select the indices of records we want the pickup location\n",
    "  pick_pos = pos[pos[loc == 0]]\n",
    "\n",
    "  # Select the indices of records we want the dropoff location\n",
    "  drop_pos = pos[pos[loc == 1]]\n",
    "\n",
    "  \n",
    "  records_pick = df.iloc[pick_pos][['medallion', 'pickup_location_id', 'pickup_week_day',\t'pickup_hour',\t'pickup_day',\t'pickup_month']]\n",
    "  records_pick = records_pick.rename(columns={'pickup_location_id': 'location_id', 'pickup_week_day':'week_day' ,\t'pickup_hour':'hour' ,\t'pickup_day':\t'day' ,\t'pickup_month':'month' })\n",
    "\n",
    "  idx_drop = np.nonzero(loc == 0)[0]\n",
    "  records_drop = df.iloc[drop_pos][['medallion', 'dropoff_location_id', 'dropoff_week_day',\t'dropoff_hour',\t'dropoff_day',\t'dropoff_month']]\n",
    "  records_drop = records_drop.rename(columns={'dropoff_location_id': 'location_id', 'dropoff_week_day':'week_day' ,\t'dropoff_hour':'hour' ,\t'dropoff_day':\t'day' ,\t'dropoff_month':'month' })\n",
    "\n",
    "  locations_sequence = pd.concat([records_pick, records_drop])\n",
    "\n",
    "  # reset the index\n",
    "  locations_sequence.reset_index(inplace=True)\n",
    "\n",
    "  # From hour to sin-cos representation\n",
    "  locations_sequence['hour_sin'] = np.sin(locations_sequence.hour*(2.*np.pi/24))\n",
    "  locations_sequence['hour_cos'] = np.cos(locations_sequence.hour*(2.*np.pi/24))\n",
    "\n",
    "  locations_sequence['week_day_sin'] = np.sin(locations_sequence.week_day*(2.*np.pi/7))\n",
    "  locations_sequence['week_day_cos'] = np.cos(locations_sequence.week_day*(2.*np.pi/7))\n",
    "\n",
    "\n",
    "  # Drop the original column\n",
    "  locations_sequence.drop(['hour'], axis=1, inplace=True)\n",
    "  \n",
    "\n",
    "  # Helper function to encode the day_type\n",
    "  def is_weekend(days):\n",
    "    weekends = np.zeros(len(days))\n",
    "    weekends[((days == 5) | (days == 6))] = 1\n",
    "    return weekends\n",
    "\n",
    "  # Apply the helper function to all the records\n",
    "  locations_sequence['weekend'] = is_weekend(locations_sequence['week_day'])\n",
    "\n",
    "  # the column is not needed anymore\n",
    "  locations_sequence.drop(['week_day'], axis=1, inplace=True)\n",
    "\n",
    "  # Correct the weekend feature type\n",
    "  dictionary = {'weekend': 'int32'}\n",
    "  locations_sequence = locations_sequence.astype(dictionary, copy=True)\n",
    "  \n",
    "  return locations_sequence, pos, loc\n",
    "\n",
    "# Call the function\n",
    "locations_sequence, pos, loc = df_to_location_sequence(df)\n",
    "\n",
    "print(locations_sequence)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          index                         medallion  location_id  day  month  \\\n",
      "0         18752  003EEA559FA61800874D4F6805C4A084          170    1      1   \n",
      "1         18752  003EEA559FA61800874D4F6805C4A084          170    1      1   \n",
      "2         18755  003EEA559FA61800874D4F6805C4A084          141    1      1   \n",
      "3         18757  003EEA559FA61800874D4F6805C4A084          239    1      1   \n",
      "4         18758  003EEA559FA61800874D4F6805C4A084           41    1      1   \n",
      "...         ...                               ...          ...  ...    ...   \n",
      "249801  7268314  7C7F7C78F1ECB5625E1F611E711DB449          233   24      1   \n",
      "249802  7268315  7C7F7C78F1ECB5625E1F611E711DB449           79   24      1   \n",
      "249803  7268315  7C7F7C78F1ECB5625E1F611E711DB449           79   24      1   \n",
      "249804  7268316  7C7F7C78F1ECB5625E1F611E711DB449          162   24      1   \n",
      "249805  7268317  7C7F7C78F1ECB5625E1F611E711DB449           43   24      1   \n",
      "\n",
      "        hour_sin      hour_cos  week_day_sin  week_day_cos  weekend  \n",
      "0       0.000000  1.000000e+00      0.781831      0.623490        0  \n",
      "1       0.000000  1.000000e+00      0.781831      0.623490        0  \n",
      "2       0.258819  9.659258e-01      0.781831      0.623490        0  \n",
      "3       0.258819  9.659258e-01      0.781831      0.623490        0  \n",
      "4       0.258819  9.659258e-01      0.781831      0.623490        0  \n",
      "...          ...           ...           ...           ...      ...  \n",
      "249801 -0.965926 -2.588190e-01      0.433884     -0.900969        0  \n",
      "249802 -1.000000 -1.836970e-16      0.433884     -0.900969        0  \n",
      "249803 -1.000000 -1.836970e-16      0.433884     -0.900969        0  \n",
      "249804 -1.000000 -1.836970e-16      0.433884     -0.900969        0  \n",
      "249805 -1.000000 -1.836970e-16      0.433884     -0.900969        0  \n",
      "\n",
      "[249806 rows x 10 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YbQmLj9kSYqp",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "07af026d-8765-4323-8204-953df7b1eccc"
   },
   "source": [
    "# List the df for each user\n",
    "users_locations = []\n",
    "\n",
    "# For each user\n",
    "for medallion in tqdm(medallions):\n",
    "  # Call the function\n",
    "  locations_sequence, pos, loc = df_to_location_sequence(df.loc[df.medallion == medallion].copy())\n",
    "  # Add the sequence df of the user to the list\n",
    "  users_locations.append(locations_sequence)\n"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 79.18it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ltjLgzdf1LYY"
   },
   "source": [
    "test_locations_sequence, pos, loc = df_to_location_sequence(df_test)\n",
    "val_locations_sequence, pos, loc = df_to_location_sequence(df_val)\n",
    "\n",
    "test_locations_sequence.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
    "val_locations_sequence.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
    "\n",
    "# Split the data into chunks\n",
    "N = 17\n",
    "\n",
    "# Test\n",
    "# Get a list of dataframes of length n records \n",
    "list_test = [test_locations_sequence[i:i+N] for i in range(0, test_locations_sequence.shape[0], N)]\n",
    "\n",
    "# Test\n",
    "# Get a list of dataframes of length n records \n",
    "list_val = [val_locations_sequence[i:i+N] for i in range(0, val_locations_sequence.shape[0], N)]\n",
    "list_test[0]\n",
    "\n",
    "if len(list_val[-1]) < N:\n",
    "  diff_val = 1\n",
    "else:\n",
    "  diff_val = 0\n",
    "\n",
    "if len(list_test[-1]) < N:\n",
    "  diff_test = 1\n",
    "else:\n",
    "  diff_test = 0\n",
    "\n",
    "\n",
    "# Define the input features of the  dataset\n",
    "val_input_dict = {\n",
    "  'start_place':np.array([list_val[i]['location_id'].values[:-1] for i in range(0, len(list_val)-diff_val)]), \n",
    "  'start_hour_sin':np.array([list_val[i]['hour_sin'].values[:-1] for i in range(0, len(list_val)-diff_val)]),\n",
    "  'start_hour_cos':np.array([list_val[i]['hour_cos'].values[:-1] for i in range(0, len(list_val)-diff_val)]), \n",
    "  'weekend':np.array([list_val[i]['weekend'].values[:-1] for i in range(0, len(list_val)-diff_val)]),\n",
    "  'week_day_sin':np.array([list_val[i]['week_day_sin'].values[:-1] for i in range(0, len(list_val)-diff_val)]),\n",
    "  'week_day_cos':np.array([list_val[i]['week_day_cos'].values[:-1] for i in range(0, len(list_val)-diff_val)]),\n",
    "}\n",
    "\n",
    "# Define the input features of the  dataset\n",
    "test_input_dict = {\n",
    "  'start_place':np.array([list_test[i]['location_id'].values[:-1] for i in range(0, len(list_test)-diff_test)]), \n",
    "  'start_hour_sin':np.array([list_test[i]['hour_sin'].values[:-1] for i in range(0, len(list_test)-diff_test)]),\n",
    "  'start_hour_cos':np.array([list_test[i]['hour_cos'].values[:-1] for i in range(0, len(list_test)-diff_test)]), \n",
    "  'weekend':np.array([list_test[i]['weekend'].values[:-1] for i in range(0, len(list_test)-diff_test)]),\n",
    "  'week_day_sin':np.array([list_test[i]['week_day_sin'].values[:-1] for i in range(0, len(list_test)-diff_test)]),\n",
    "  'week_day_cos':np.array([list_test[i]['week_day_cos'].values[:-1] for i in range(0, len(list_test)-diff_test)]),\n",
    "}\n",
    "\n",
    "# Create training examples / targets, we are going to predict the next location\n",
    "trips_dataset_val = tf.data.Dataset.from_tensor_slices((val_input_dict, np.array([list_val[i]['location_id'].values[1:] for i in range(0, len(list_val)-diff_val)])) )\n",
    "trips_dataset_test = tf.data.Dataset.from_tensor_slices((test_input_dict, np.array([list_test[i]['location_id'].values[1:] for i in range(0, len(list_test)-diff_test)])) )\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Create the dataset by creating batches\n",
    "# Uncomment the shuffle function in case we want to shuffle the sequences\n",
    "val_dataset = trips_dataset_val.batch(BATCH_SIZE, drop_remainder=True) #.shuffle(BUFFER_SIZE)\n",
    "test_dataset = trips_dataset_test.batch(BATCH_SIZE, drop_remainder=True) #.shuffle(BUFFER_SIZE)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sEMnEoqY9CAv",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "37048dd7-05ce-4780-fda7-70351e948503"
   },
   "source": [
    "sizes = []\n",
    "# Number of locations for each user\n",
    "for user_df in users_locations:\n",
    "  sizes.append(user_df.shape[0])\n",
    "\n",
    "print('Mean number of locations: ', np.mean(np.array(sizes)))\n",
    "print('Max number of locations: ', np.max(np.array(sizes)))\n",
    "print('Min number of locations: ', np.min(np.array(sizes)))"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of locations:  2498.07\n",
      "Max number of locations:  2924\n",
      "Min number of locations:  2306\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dklBpuI4YSbf"
   },
   "source": [
    "Create the validation and test sets for each user"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bHDBtZVwYXWT"
   },
   "source": [
    "# List the dfs fo train, val and test for each user\n",
    "users_locations_train = []\n",
    "users_locations_val = []\n",
    "users_locations_test = []\n",
    "\n",
    "for user_df in users_locations:\n",
    "  # Split in train, test and validation\n",
    "  train, test = train_test_split(user_df, test_size=0.2, shuffle=False)\n",
    "  train, val = train_test_split(train, test_size=0.2, shuffle=False)\n",
    "\n",
    "  # Append the sets\n",
    "  users_locations_train.append(train)\n",
    "  users_locations_val.append(val)\n",
    "  users_locations_test.append(test)\n"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N8IIiHFcBJJI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "fafe94bc-e851-461f-aa9c-f2b59fd1522f"
   },
   "source": [
    "sizes = []\n",
    "# Number of locations for each user in the validation set\n",
    "for user_df in users_locations_val:\n",
    "  sizes.append(user_df.shape[0])\n",
    "\n",
    "print('Mean number of locations: ', np.mean(np.array(sizes)))\n",
    "print('Max number of locations: ', np.max(np.array(sizes)))\n",
    "print('Min number of locations: ', np.min(np.array(sizes)))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of locations:  400.01\n",
      "Max number of locations:  468\n",
      "Min number of locations:  369\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYtsvsO0aUi2"
   },
   "source": [
    "Create sequences for each client"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "15nXfT6MT9AF"
   },
   "source": [
    "# Merge back the dataframes\n",
    "df_train = pd.concat(users_locations_train)\n",
    "df_train.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
    "\n",
    "# Merge back the dataframes\n",
    "df_val = pd.concat(users_locations_val)\n",
    "df_val.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
    "\n",
    "# Merge back the dataframes\n",
    "df_test = pd.concat(users_locations_test)\n",
    "df_test.drop(['index', 'day', 'month'], axis=1, inplace=True)"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z77E_KMGvy9R",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "b0f9b691-d41a-42a9-b92f-b57cc8eae4e3"
   },
   "source": [
    "# list of unique medallions\n",
    "medallions_list = df_train.medallion.unique()\n",
    "\n",
    "# number of unique medallions\n",
    "medallions_num = len(medallions_list)\n",
    "print(medallions_num)"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QTQqAoZaaXL1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "outputId": "d2ac02ef-3ca5-4f05-a02f-9722c1bbcb32"
   },
   "source": [
    "# Split the data into chunks of N+1\n",
    "N = 17\n",
    "\n",
    "# dictionary of list of df \n",
    "df_dictionary = {}\n",
    "\n",
    "for medallion in tqdm(medallions_list):\n",
    "\n",
    "  # Get the records of the user\n",
    "  user_df_train = df_train.loc[df_train.medallion == medallion].copy()\n",
    "  user_df_val = df_val.loc[df_val.medallion == medallion].copy()\n",
    "  user_df_test = df_test.loc[df_test.medallion == medallion].copy()\n",
    "\n",
    "  # Get a list of dataframes of length N records \n",
    "  user_list_train = [user_df_train[i:i+N] for i in range(0, user_df_train.shape[0], N)]\n",
    "  user_list_val = [user_df_val[i:i+N] for i in range(0, user_df_val.shape[0], N)]\n",
    "  user_list_test = [user_df_test[i:i+N] for i in range(0, user_df_test.shape[0], N)]\n",
    "\n",
    "  # Save the list of dataframes into a dictionary\n",
    "  df_dictionary[medallion] = {\n",
    "      'train': user_list_train,\n",
    "      'val': user_list_val,\n",
    "      'test': user_list_test\n",
    "  }\n",
    "\n",
    "'''\n",
    "# Validation\n",
    "# Get a list of dataframes of length n records \n",
    "list_val = [val[i:i+N] for i in range(0, val.shape[0], N)]\n",
    "\n",
    "# Test\n",
    "# Get a list of dataframes of length n records \n",
    "list_test = [test[i:i+N] for i in range(0, test.shape[0], N)]\n",
    "list_test[0]'''"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 78.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\n# Validation\\n# Get a list of dataframes of length n records \\nlist_val = [val[i:i+N] for i in range(0, val.shape[0], N)]\\n\\n# Test\\n# Get a list of dataframes of length n records \\nlist_test = [test[i:i+N] for i in range(0, test.shape[0], N)]\\nlist_test[0]'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "38O5t3bizJVM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "b5fd5ea3-e2cf-4ea3-e32e-756dc13cde65"
   },
   "source": [
    "df_train.columns.values"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['medallion', 'location_id', 'hour_sin', 'hour_cos', 'week_day_sin',\n       'week_day_cos', 'weekend'], dtype=object)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1pbQL_4svmnY"
   },
   "source": [
    "# Create the dictionary to create a clientData\n",
    "columns_names = df_train.columns.values[1:]\n",
    "\n",
    "# Takes a dictionary with train, validation an test sets and the desired set type\n",
    "def create_clients_dict(df_dictionary, set_type):\n",
    "  \n",
    "  dataset_dict = {}\n",
    "\n",
    "  for medallion in tqdm(medallions_list):\n",
    "\n",
    "    c_data = collections.OrderedDict()\n",
    "    values = df_dictionary[medallion][set_type]\n",
    "\n",
    "    # If the last dataframe of the list is not complete\n",
    "    if len(values[-1]) < N:\n",
    "      diff = 1\n",
    "    else:\n",
    "      diff = 0\n",
    "\n",
    "    if len(values) > 0:\n",
    "      for header in columns_names:\n",
    "        #c_data[header] = values[header].values.tolist()\n",
    "        c_data[header] = [values[i][header].values for i in range(0, len(values)-diff)] #[:-1]\n",
    "        #c_data['y'] = values['dropoff_location_id'].values.tolist()\n",
    "      dataset_dict[medallion] = c_data\n",
    "      \n",
    "  return dataset_dict\n"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jc41ZQRM066G",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "a1a09f75-4f96-491a-b656-2ea6bdf76e7d"
   },
   "source": [
    "# Generate the dictionaries for each set\n",
    "clients_train_dict = create_clients_dict(df_dictionary, 'train')\n",
    "clients_val_dict = create_clients_dict(df_dictionary, 'val')\n",
    "clients_test_dict = create_clients_dict(df_dictionary, 'test')"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 86.36it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 490.20it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 241.55it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9_1c1VkYu9TM"
   },
   "source": [
    "# Convert the dictionary to a dataset\n",
    "client_train_data = tff.simulation.FromTensorSlicesClientData(clients_train_dict)\n",
    "client_val_data = tff.simulation.FromTensorSlicesClientData(clients_val_dict)\n",
    "client_test_data = tff.simulation.FromTensorSlicesClientData(clients_test_dict)"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "syED5r3_A-Ej",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "eeb96625-1fd8-473c-f0c5-c4caf9583d5c"
   },
   "source": [
    "client_train_data.create_tf_dataset_for_client(medallions_list[0]).element_spec"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('location_id',\n              TensorSpec(shape=(17,), dtype=tf.int32, name=None)),\n             ('hour_sin',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('hour_cos',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('week_day_sin',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('week_day_cos',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('weekend', TensorSpec(shape=(17,), dtype=tf.int32, name=None))])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqWe5BAN20MQ"
   },
   "source": [
    "Retrieve and example dataset from client_data to take a look at its structure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2UIBQ2Gv3MA5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "outputId": "d7aafc51-b3eb-4583-8ca1-59a930afeb00"
   },
   "source": [
    "example_dataset = client_train_data.create_tf_dataset_for_client(\n",
    "    client_train_data.client_ids[1])\n",
    "\n",
    "example_element = next(iter(example_dataset))\n",
    "example_element"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('location_id',\n              <tf.Tensor: shape=(17,), dtype=int32, numpy=\n              array([249, 262, 133, 244, 141, 141, 186, 186, 237, 237, 143, 229, 230,\n                      79, 137, 233, 114])>),\n             ('hour_sin',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([ 0.00000000e+00,  0.00000000e+00,  7.07106781e-01,  8.66025404e-01,\n                      5.00000000e-01,  5.00000000e-01,  5.00000000e-01,  5.00000000e-01,\n                      2.58819045e-01,  2.58819045e-01,  2.58819045e-01,  1.22464680e-16,\n                      1.22464680e-16,  1.22464680e-16,  1.22464680e-16, -2.58819045e-01,\n                     -2.58819045e-01])>),\n             ('hour_cos',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([ 1.        ,  1.        ,  0.70710678,  0.5       , -0.8660254 ,\n                     -0.8660254 , -0.8660254 , -0.8660254 , -0.96592583, -0.96592583,\n                     -0.96592583, -1.        , -1.        , -1.        , -1.        ,\n                     -0.96592583, -0.96592583])>),\n             ('week_day_sin',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([0.78183148, 0.78183148, 0.78183148, 0.78183148, 0.78183148,\n                     0.78183148, 0.78183148, 0.78183148, 0.78183148, 0.78183148,\n                     0.78183148, 0.78183148, 0.78183148, 0.78183148, 0.78183148,\n                     0.78183148, 0.78183148])>),\n             ('week_day_cos',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898,\n                     0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898,\n                     0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898])>),\n             ('weekend',\n              <tf.Tensor: shape=(17,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>)])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coraDX-adRjf"
   },
   "source": [
    "*Shuffle the individual examples, organize them into batches and renames the target feature from `dropoff_location_id ` to y for use with Keras. We also throw in a repeat over the data set to run several epochs.*\n",
    "\n",
    "Because `tff.learning.from_keras_model` wants as input_spec a dictionary of 2 elements (x,y) and we have multiple inputs, we have to make also x a ditionary.\n",
    "\n",
    "In this way we will be able to process each feature separately in the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lIVbzPidc4bi"
   },
   "source": [
    "NUM_CLIENTS = medallions_num\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 5\n",
    "\n",
    "def preprocess(dataset):\n",
    "  def batch_format_fn(element):\n",
    "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
    "    return collections.OrderedDict(\n",
    "        x=collections.OrderedDict(\n",
    "          start_place=tf.reshape(element['location_id'][:, :-1], [-1, N-1]),\n",
    "          start_hour_sin=tf.reshape(element['hour_sin'][:, :-1], [-1, N-1]),\n",
    "          start_hour_cos=tf.reshape(element['hour_cos'][:, :-1], [-1, N-1]),\n",
    "          week_day_sin=tf.reshape(element['week_day_sin'][:, :-1], [-1, N-1]),\n",
    "          week_day_cos=tf.reshape(element['week_day_cos'][:, :-1], [-1, N-1]),\n",
    "          weekend=tf.reshape(element['weekend'][:, :-1], [-1, N-1])\n",
    "          ),\n",
    "        y=tf.reshape(element['location_id'][:, 1:], [-1, N-1]))\n",
    "  return dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE, drop_remainder=True).map(batch_format_fn).prefetch(PREFETCH_BUFFER) # .shuffle(SHUFFLE_BUFFER)"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irPtpimwe7Y-"
   },
   "source": [
    "Test the preprocessing on a single client dataset\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fklCZxNpeXg4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "de02d78a-2053-45c4-c4e9-038eb36f6ed9"
   },
   "source": [
    "preprocessed_example_dataset = preprocess(example_dataset)\n",
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
    "                                     next(iter(preprocessed_example_dataset)))\n",
    "\n",
    "sample_batch['x']['start_place'].shape"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(16, 16)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DJLyl_cr61jR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "3218507f-ca83-4d43-801e-f9ae4603c695"
   },
   "source": [
    "preprocessed_example_dataset"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset shapes: OrderedDict([(x, OrderedDict([(start_place, (16, 16)), (start_hour_sin, (16, 16)), (start_hour_cos, (16, 16)), (week_day_sin, (16, 16)), (week_day_cos, (16, 16)), (weekend, (16, 16))])), (y, (16, 16))]), types: OrderedDict([(x, OrderedDict([(start_place, tf.int32), (start_hour_sin, tf.float64), (start_hour_cos, tf.float64), (week_day_sin, tf.float64), (week_day_cos, tf.float64), (weekend, tf.int32)])), (y, tf.int32)])>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R5Dy0rOI5V_F",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "outputId": "8f8ce6d6-ee19-4a70-b499-7458ec00ab5c"
   },
   "source": [
    "preprocessed_example_dataset.element_spec"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('x',\n              OrderedDict([('start_place',\n                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None)),\n                           ('start_hour_sin',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('start_hour_cos',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('week_day_sin',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('week_day_cos',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('weekend',\n                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])),\n             ('y', TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-x7fxr63JFr"
   },
   "source": [
    "The ways to feed federated data to TFF in a simulation is simply as a Python list, with each element of the list holding the data of an individual user, whether as a list or as a tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pOCHe8zE2yNP"
   },
   "source": [
    "def make_federated_data(client_data, client_ids):\n",
    "  return [\n",
    "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
    "      for x in tqdm(client_ids)\n",
    "  ]"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvczxgol3Z1l"
   },
   "source": [
    "Of course, we are in a simulation environment, and all the data is locally available. Typically then, when running simulations, we would simply sample a random subset of the clients to be involved in each round of training, generally different in each round.\n",
    "\n",
    "That said, as you can find out by studying the paper on the Federated Averaging algorithm, achieving convergence in a system with randomly sampled subsets of clients in each round can take a while, and it would be impractical to have to run hundreds of rounds in this interactive tutorial.\n",
    "\n",
    "What we'll do instead is sample the set of clients once, and reuse the same set across rounds to speed up convergence (intentionally over-fitting to these few user's data). We leave it as an exercise for the reader to modify this tutorial to simulate random sampling - it is fairly easy to do (once you do, keep in mind that getting the model to converge may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FM4vvnXK3B4X",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "outputId": "548a4ba2-cc4c-472b-9561-b49e7fe4ec92"
   },
   "source": [
    "# Select the clients\n",
    "sample_clients = client_train_data.client_ids[0:NUM_CLIENTS]\n",
    "\n",
    "# Federate the clients datasets\n",
    "federated_train_data = make_federated_data(client_train_data, sample_clients)\n",
    "federated_val_data = make_federated_data(client_val_data, sample_clients)\n",
    "federated_test_data = make_federated_data(client_test_data, sample_clients)\n",
    "\n",
    "\n",
    "print('\\nNumber of client datasets: {l}'.format(l=len(federated_train_data)))\n",
    "print('First dataset: {d}'.format(d=federated_train_data[0]))"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.88it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.79it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 60.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of client datasets: 100\n",
      "First dataset: <PrefetchDataset shapes: OrderedDict([(x, OrderedDict([(start_place, (16, 16)), (start_hour_sin, (16, 16)), (start_hour_cos, (16, 16)), (week_day_sin, (16, 16)), (week_day_cos, (16, 16)), (weekend, (16, 16))])), (y, (16, 16))]), types: OrderedDict([(x, OrderedDict([(start_place, tf.int32), (start_hour_sin, tf.float64), (start_hour_cos, tf.float64), (week_day_sin, tf.float64), (week_day_cos, tf.float64), (weekend, tf.int32)])), (y, tf.int32)])>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9Oq_He_Osxb"
   },
   "source": [
    "# Federated Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "noO2-Ca6_7oj"
   },
   "source": [
    "# All the different places in the dataset\n",
    "indices = np.concatenate((df.pickup_location_id.values, df.dropoff_location_id.values))\n",
    "\n",
    "# Length of the vocabulary of places (e.g. 11)\n",
    "vocab_size = int(np.max(indices) + 1) # + 1 because of 0\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 256\n",
    "\n",
    "# List of numerical column names\n",
    "numerical_column_names = ['start_hour_sin', 'start_hour_cos', 'weekend', 'week_day']\n",
    "\n",
    "# Number of different places\n",
    "number_of_places =  max(locations_sequence.location_id.max(), locations_sequence.location_id.max()) + 1"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diSvfc1xMoOH"
   },
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Eqk9JcOcGpWY"
   },
   "source": [
    "# Create a model\n",
    "def create_keras_model(number_of_places, batch_size):\n",
    "  \n",
    "\t# Shortcut to the layers package\n",
    "  l = tf.keras.layers\n",
    "\t\n",
    "  # List of numeric feature columns to pass to the DenseLayer\n",
    "  numeric_feature_columns = []\n",
    "\n",
    "\n",
    "  # Handling numerical columns \n",
    "  for header in numerical_column_names:\n",
    "\t\t# Append all the numerical columns defined into the list\n",
    "    numeric_feature_columns.append(feature_column.numeric_column(header, shape=N-1))\n",
    "\n",
    "  # Now we need to define an input dictionary.\n",
    "\t# Where the keys are the column names\n",
    "\t# This is a model with multiple inputs, so we need to declare and input layer for each feature\n",
    "  feature_inputs = {\n",
    "    'start_hour_sin': tf.keras.Input((N-1, ), batch_size=batch_size, name='start_hour_sin'),\n",
    "    'start_hour_cos': tf.keras.Input((N-1, ), batch_size=batch_size, name='start_hour_cos'),\n",
    "    'weekend': tf.keras.Input((N-1, ), batch_size=batch_size, name='weekend'),\n",
    "    'week_day_sin': tf.keras.Input((N-1, ), batch_size=batch_size, name='week_day_sin'),\n",
    "    'week_day_cos': tf.keras.Input((N-1, ), batch_size=batch_size, name='week_day_cos'),\n",
    "  }\n",
    "\n",
    "  # We declare two DenseFeature layers, one for the numeric columns which do not require\\ \n",
    "\t# Any training, and one for the categorical. It is easier to do it like this\n",
    "  '''numerical_features = l.DenseFeatures(numeric_feature_columns)(feature_inputs)'''\n",
    "  \n",
    "  # We cannot use anarray of features as always because we have sequences and we cannot match the shape otherwise\n",
    "  # We have to do one by one\n",
    "  start_hour_sin = feature_column.numeric_column(\"start_hour_sin\", shape=(N-1))\n",
    "  hour_sin_feature = l.DenseFeatures(start_hour_sin)(feature_inputs)\n",
    "\n",
    "  start_hour_cos = feature_column.numeric_column(\"start_hour_cos\", shape=(N-1))\n",
    "  hour_cos_feature = l.DenseFeatures(start_hour_cos)(feature_inputs)\n",
    "\n",
    "  weekend = feature_column.numeric_column(\"weekend\", shape=(N-1))\n",
    "  weekend_feature = l.DenseFeatures(weekend)(feature_inputs)\n",
    "\n",
    "  week_day_sin = feature_column.numeric_column(\"week_day_sin\", shape=(N-1))\n",
    "  week_day_sin_feature = l.DenseFeatures(week_day_sin)(feature_inputs)\n",
    "\n",
    "  week_day_cos = feature_column.numeric_column(\"week_day_cos\", shape=(N-1))\n",
    "  week_day_cos_feature = l.DenseFeatures(week_day_cos)(feature_inputs)\n",
    "  \n",
    "\t# We have also to add a dimension to then concatenate\n",
    "  hour_sin_feature = tf.expand_dims(hour_sin_feature, -1)\n",
    "  hour_cos_feature = tf.expand_dims(hour_cos_feature, -1)\n",
    "  weekend_feature = tf.expand_dims(weekend_feature, -1)\n",
    "  week_day_sin_feature = tf.expand_dims(week_day_sin_feature, -1)\n",
    "  week_day_cos_feature = tf.expand_dims(week_day_cos_feature, -1)\n",
    "\n",
    "  # Declare the dictionary for the places sequence as before\n",
    "  sequence_input = {\n",
    "      'start_place': tf.keras.Input((N-1,), batch_size=batch_size, dtype=tf.dtypes.int32, name='start_place') # add batch_size=batch_size in case of stateful GRU\n",
    "  }\n",
    "\n",
    "\n",
    "  # Handling the categorical feature sequence using one-hot\n",
    "  places_one_hot = feature_column.sequence_categorical_column_with_vocabulary_list(\n",
    "      'start_place', [i for i in range(number_of_places)])\n",
    "  \n",
    "  # Embed the one-hot encoding\n",
    "  places_embed = feature_column.embedding_column(places_one_hot, embedding_dim)\n",
    "\n",
    "\n",
    "  # With an input sequence we can't use the DenseFeature layer, we need to use the SequenceFeatures\n",
    "  sequence_features, sequence_length = tf.keras.experimental.SequenceFeatures(places_embed)(sequence_input)\n",
    "\n",
    "  input_sequence = l.Concatenate(axis=2)([ sequence_features, hour_sin_feature, hour_cos_feature, weekend_feature, week_day_sin_feature, week_day_cos_feature])\n",
    "\n",
    "  # Rnn\n",
    "  recurrent = l.GRU(rnn_units,\n",
    "                        batch_size=batch_size, #in case of stateful\n",
    "                        dropout=0.3,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform')(input_sequence)\n",
    "\n",
    "\n",
    "\t# Last layer with an output for each places\n",
    "  dense_1 = layers.Dense(number_of_places)(recurrent)\n",
    "\n",
    "\t# Softmax output layer\n",
    "  output = l.Softmax()(dense_1)\n",
    "\t\n",
    "\t# To return the Model, we need to define its inputs and outputs\n",
    "\t# In out case, we need to list all the input layers we have defined \n",
    "  inputs = list(feature_inputs.values()) + list(sequence_input.values())\n",
    "\n",
    "\t# Return the Model\n",
    "  return tf.keras.Model(inputs=inputs, outputs=output)"
   ],
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3ItQ7X8CYHu"
   },
   "source": [
    "Function to evaluate the federated model on the server.\n",
    "With and without round number."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "do9fvQMwCT6-"
   },
   "source": [
    "def keras_evaluate(state, round_num, dataset, tb=0):\n",
    "  # Take our global model weights and push them back into a Keras model to\n",
    "  # use its standard `.evaluate()` method.\n",
    "  keras_model = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
    "  keras_model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\t# Load state server parameters into the Keras model\n",
    "  state.model.assign_weights_to(keras_model)\n",
    "  loss, accuracy = keras_model.evaluate(dataset)\n",
    "  if tb == 1:\n",
    "    with eval_summary_writer.as_default():\n",
    "        for name, value in dict(val_metrics).items():\n",
    "          tf.summary.scalar('epoch_loss', loss, step=round_num)\n",
    "          tf.summary.scalar('epoch_sparse_categorical_accuracy', accuracy, step=round_num)\n",
    "  print('\\tEVAL: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))"
   ],
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J_ZDdLJrh864"
   },
   "source": [
    "def keras_evaluate(state, dataset, tb=0):\n",
    "  # Take our global model weights and push them back into a Keras model to\n",
    "  # use its standard `.evaluate()` method.\n",
    "  keras_model = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
    "  keras_model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\t# Load state server parameters into the Keras model\n",
    "  state.model.assign_weights_to(keras_model)\n",
    "  loss, accuracy = keras_model.evaluate(dataset)\n",
    "  print('\\tEVAL: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3c6-MzlM0UO"
   },
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ShnjsxGb7nXt",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "7924dde7-2c8e-4aa6-da55-9122c746250b"
   },
   "source": [
    "keras_model = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
    "keras_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "keras_model.summary()"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "start_hour_cos (InputLayer)     [(16, 16)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "start_hour_sin (InputLayer)     [(16, 16)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "week_day_cos (InputLayer)       [(16, 16)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "week_day_sin (InputLayer)       [(16, 16)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "weekend (InputLayer)            [(16, 16)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "start_place (InputLayer)        [(16, 16)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_features (DenseFeatures)  (16, 16)             0           start_hour_cos[0][0]             \n",
      "                                                                 start_hour_sin[0][0]             \n",
      "                                                                 week_day_cos[0][0]               \n",
      "                                                                 week_day_sin[0][0]               \n",
      "                                                                 weekend[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_features_1 (DenseFeatures (16, 16)             0           start_hour_cos[0][0]             \n",
      "                                                                 start_hour_sin[0][0]             \n",
      "                                                                 week_day_cos[0][0]               \n",
      "                                                                 week_day_sin[0][0]               \n",
      "                                                                 weekend[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_features_2 (DenseFeatures (16, 16)             0           start_hour_cos[0][0]             \n",
      "                                                                 start_hour_sin[0][0]             \n",
      "                                                                 week_day_cos[0][0]               \n",
      "                                                                 week_day_sin[0][0]               \n",
      "                                                                 weekend[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_features_3 (DenseFeatures (16, 16)             0           start_hour_cos[0][0]             \n",
      "                                                                 start_hour_sin[0][0]             \n",
      "                                                                 week_day_cos[0][0]               \n",
      "                                                                 week_day_sin[0][0]               \n",
      "                                                                 weekend[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_features_4 (DenseFeatures (16, 16)             0           start_hour_cos[0][0]             \n",
      "                                                                 start_hour_sin[0][0]             \n",
      "                                                                 week_day_cos[0][0]               \n",
      "                                                                 week_day_sin[0][0]               \n",
      "                                                                 weekend[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequence_features (SequenceFeat ((None, None, 256),  67584       start_place[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(16, 16, 1)]        0           dense_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(16, 16, 1)]        0           dense_features_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_2 (Tenso [(16, 16, 1)]        0           dense_features_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_3 (Tenso [(16, 16, 1)]        0           dense_features_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_4 (Tenso [(16, 16, 1)]        0           dense_features_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (16, 16, 261)        0           sequence_features[0][0]          \n",
      "                                                                 tf_op_layer_ExpandDims[0][0]     \n",
      "                                                                 tf_op_layer_ExpandDims_1[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_2[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_3[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (16, 16, 256)        398592      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (16, 16, 264)        67848       gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (16, 16, 264)        0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 534,024\n",
      "Trainable params: 534,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK5r_WJra672"
   },
   "source": [
    "TFF serializes all TensorFlow computations so they can potentially be run in a non-Python environment (even though at the moment, only a simulation runtime implemented in Python is available). Even though we are running in eager mode, (TF 2.0), currently TFF serializes TensorFlow computations by constructing the necessary ops inside the context of a \"with tf.Graph.as_default()\" statement. Thus, we need to provide a function that TFF can use to introduce our model into a graph it controls. We do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xgfd00ifa7ry"
   },
   "source": [
    "# Clone the keras_model inside `create_tff_model()`, which TFF will\n",
    "# call to produce a new copy of the model inside the graph that it will \n",
    "# serialize. Note: we want to construct all the necessary objects we'll need \n",
    "# _inside_ this method.\n",
    "def create_tff_model():\n",
    "  # TFF uses an `input_spec` so it knows the types and shapes\n",
    "  # that your model expects.\n",
    "  input_spec = preprocessed_example_dataset.element_spec\n",
    "  keras_model_clone = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
    "  return tff.learning.from_keras_model(\n",
    "      keras_model_clone,\n",
    "      input_spec=input_spec,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xs7Dj5tOcEOS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "outputId": "f0de70b2-c742-4025-ef96-a07c1ea2ff8e"
   },
   "source": [
    "preprocessed_example_dataset.element_spec"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('x',\n              OrderedDict([('start_place',\n                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None)),\n                           ('start_hour_sin',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('start_hour_cos',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('week_day_sin',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('week_day_cos',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('weekend',\n                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])),\n             ('y', TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ywd5cFRbYG-"
   },
   "source": [
    "We use a compiled Keras model to perform standard (non-federated) evaluation after each round of federated training. This is useful for research purposes when doing simulated federated learning and there is a standard test dataset.\n",
    "\n",
    "In a realistic production setting this same technique might be used to take models trained with federated learning and evaluate them on a centralized benchmark dataset for testing or quality assurance purposes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9swVmhFTbYte"
   },
   "source": [
    "# This command builds all the TensorFlow graphs and serializes them: \n",
    "fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.002),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.06))"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W_gUHP6WbkX_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "cc691ef6-fabe-4b48-9206-47a1b8d83ec2"
   },
   "source": [
    "# EMBEDDED\n",
    "str(fed_avg.initialize.type_signature)"
   ],
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "'( -> <model=<trainable=<float32[264,256],float32[261,768],float32[256,768],float32[2,768],float32[256,264],float32[264]>,non_trainable=<>>,optimizer_state=<int64,float32[264,256],float32[261,768],float32[256,768],float32[2,768],float32[256,264],float32[264],float32[264,256],float32[261,768],float32[256,768],float32[2,768],float32[256,264],float32[264]>,delta_aggregate_state=<value_sum_process=<>,weight_sum_process=<>>,model_broadcast_state=<>>@SERVER)'"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RHZHKAs7qHBD"
   },
   "source": [
    "state = fed_avg.initialize()"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hzyL7EwcqJVS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "outputId": "2b813fd5-844c-4421-da96-a8718c89336a"
   },
   "source": [
    "# State Embedded\n",
    "str(state)"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "\"ServerState(model=ModelWeights(trainable=[array([[-0.03832111,  0.06345753,  0.02792387, ..., -0.06383753,\\n        -0.07725416, -0.04139237],\\n       [-0.04978357, -0.0059065 , -0.06005683, ...,  0.02992821,\\n        -0.09823437, -0.10912035],\\n       [ 0.02096535,  0.08714084,  0.03277419, ...,  0.00409269,\\n        -0.04394473,  0.01000651],\\n       ...,\\n       [-0.04548226,  0.01850693, -0.04152045, ..., -0.07707153,\\n        -0.00912718, -0.01960481],\\n       [ 0.11825164,  0.01977342,  0.05901024, ...,  0.06455778,\\n         0.00617709,  0.03220361],\\n       [-0.01983552, -0.01471853, -0.03114764, ..., -0.01729118,\\n         0.06285027, -0.08649164]], dtype=float32), array([[-0.05133128, -0.00804715, -0.06648964, ..., -0.05323223,\\n         0.07372577, -0.05255451],\\n       [-0.01055259, -0.070721  , -0.02611122, ..., -0.0069702 ,\\n        -0.05364567,  0.0629574 ],\\n       [ 0.00867224, -0.00455649, -0.06494258, ..., -0.02242834,\\n         0.0377905 ,  0.0453207 ],\\n       ...,\\n       [-0.06672531, -0.05393545,  0.07368851, ..., -0.06730682,\\n         0.00260677, -0.00294419],\\n       [-0.02672825,  0.00164847, -0.05140662, ...,  0.0678576 ,\\n        -0.06799928,  0.0261051 ],\\n       [-0.07042403, -0.01535094,  0.04214609, ...,  0.04824115,\\n         0.0321442 , -0.01147148]], dtype=float32), array([[-0.01969186, -0.01887428,  0.03733522, ...,  0.03778834,\\n        -0.06222594,  0.01234373],\\n       [ 0.03636155,  0.05151025,  0.07095902, ...,  0.04750183,\\n        -0.03675144, -0.03757332],\\n       [-0.02271879,  0.02024201, -0.03170517, ..., -0.02761355,\\n         0.07553349, -0.0491926 ],\\n       ...,\\n       [-0.00999227,  0.00898382, -0.07078253, ..., -0.05097599,\\n        -0.06149424, -0.05546165],\\n       [ 0.04759595, -0.03940973,  0.03380561, ..., -0.0296244 ,\\n        -0.01878503, -0.06722996],\\n       [ 0.03180531,  0.04256728,  0.0516542 , ...,  0.00015659,\\n         0.05441614,  0.02493235]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[-7.95812085e-02, -5.81936389e-02,  1.00543566e-01, ...,\\n        -5.80663979e-03,  5.41783869e-04,  9.33203921e-02],\\n       [-9.18188170e-02, -7.71361142e-02, -1.12478733e-02, ...,\\n         2.37169117e-03,  8.48838165e-02, -1.04225941e-01],\\n       [-7.60245770e-02,  1.26544908e-02,  1.40519664e-02, ...,\\n        -1.00739457e-01, -4.18892503e-02,  7.44469687e-02],\\n       ...,\\n       [ 7.85595849e-02, -7.42433667e-02,  3.21978703e-02, ...,\\n        -9.07853246e-05, -2.89519429e-02, -5.36420569e-02],\\n       [ 3.35446075e-02,  1.00778289e-01,  9.49303582e-02, ...,\\n         5.86038604e-02, -2.96725407e-02,  1.16543323e-02],\\n       [-5.83116785e-02,  2.01799646e-02,  5.67997470e-02, ...,\\n        -6.30337447e-02, -7.93907940e-02,  1.06773950e-01]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)], non_trainable=[]), optimizer_state=[0, array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)], delta_aggregate_state=OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())]), model_broadcast_state=())\""
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mblk51KhRzeh"
   },
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn=create_tff_model)"
   ],
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBLe93Dfm4sA"
   },
   "source": [
    "Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7jaGN6Tfm4R5"
   },
   "source": [
    "#@test {\"skip\": true}\n",
    "# Log directory where we want to save the logs\n",
    "train_logdir = './FL/tb_final/fl_rnn/train'\n",
    "val_logdir = './FL/tb_final/fl_rnn/val'\n",
    "#eval_logdir = baseURL + 'tb_final/fl_rnn/eval'\n",
    "\n",
    "# Summary writer to save the logs\n",
    "train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_logdir)\n",
    "#eval_summary_writer = tf.summary.create_file_writer(eval_logdir)"
   ],
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hDVjgsQIvMkP",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "6e577da9-9f39-40ce-f10d-6d22beb804aa"
   },
   "source": [
    "# Run this cell to clean your directory of old output for future graphs from this directory.\n",
    "#!rm -R '/content/gdrive/My Drive/tb/fl_rnn/'"
   ],
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLTlfunoO0Yd"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Drbv1ZxLnngJ"
   },
   "source": [
    "Then we just need to wrap our training loop with the summary_writer:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WMF6528LqMmg"
   },
   "source": [
    "NUM_ROUNDS = 15\n",
    "#Plot the relevant scalar metrics with the same summary writer.\n",
    "#@test {\"skip\": true}\n",
    "with train_summary_writer.as_default():\n",
    "  for round_num in range(1, NUM_ROUNDS + 1):\n",
    "    print('Round {r}'.format(r=round_num))\n",
    "\n",
    "    # Uncomment to simulate sparse availabily of clients\n",
    "    # data_for_this_round = sample(federated_train_data)\n",
    "\n",
    "    state, metrics = fed_avg.next(state, federated_train_data)\n",
    "\n",
    "    # Federated train\n",
    "    train_metrics = metrics['train']\n",
    "    print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "    # Federated evaluation\n",
    "    val_metrics = evaluation(state.model, federated_val_data)\n",
    "    print('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
    "    \n",
    "    # Centralized Evaluation\n",
    "    #keras_evaluate(state, round_num, val_dataset, tb=1)\n",
    "    #print(' ')\n",
    "\n",
    "    print('\\twriting..')\n",
    "    # Iterate across the train metrics and write their data\n",
    "    for name, value in dict(train_metrics).items():\n",
    "      # print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
    "      tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "      \n",
    "    # Validation metrics\n",
    "    with val_summary_writer.as_default():\n",
    "      for name, value in dict(val_metrics).items():\n",
    "        # print('\\twriting..')\n",
    "         #print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
    "        tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "train_summary_writer.close()\n",
    "val_summary_writer.close()\n",
    "#eval_summary_writer.close()"
   ],
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "\tTrain: loss=4.529, accuracy=0.061\n",
      "\tValidation: loss=4.949, accuracy=0.154\n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=4.348, accuracy=0.153\n",
      "\tValidation: loss=5.648, accuracy=0.096\n",
      "\twriting..\n",
      "Round 3\n",
      "\tTrain: loss=3.990, accuracy=0.184\n",
      "\tValidation: loss=4.307, accuracy=0.170\n",
      "\twriting..\n",
      "Round 4\n",
      "\tTrain: loss=3.729, accuracy=0.195\n",
      "\tValidation: loss=4.090, accuracy=0.130\n",
      "\twriting..\n",
      "Round 5\n",
      "\tTrain: loss=3.683, accuracy=0.179\n",
      "\tValidation: loss=3.965, accuracy=0.161\n",
      "\twriting..\n",
      "Round 6\n",
      "\tTrain: loss=3.663, accuracy=0.193\n",
      "\tValidation: loss=3.800, accuracy=0.198\n",
      "\twriting..\n",
      "Round 7\n",
      "\tTrain: loss=3.604, accuracy=0.203\n",
      "\tValidation: loss=3.941, accuracy=0.231\n",
      "\twriting..\n",
      "Round 8\n",
      "\tTrain: loss=3.640, accuracy=0.205\n",
      "\tValidation: loss=3.707, accuracy=0.247\n",
      "\twriting..\n",
      "Round 9\n",
      "\tTrain: loss=3.570, accuracy=0.206\n",
      "\tValidation: loss=3.699, accuracy=0.249\n",
      "\twriting..\n",
      "Round 10\n",
      "\tTrain: loss=3.560, accuracy=0.208\n",
      "\tValidation: loss=3.674, accuracy=0.249\n",
      "\twriting..\n",
      "Round 11\n",
      "\tTrain: loss=3.541, accuracy=0.209\n",
      "\tValidation: loss=3.680, accuracy=0.248\n",
      "\twriting..\n",
      "Round 12\n",
      "\tTrain: loss=3.531, accuracy=0.210\n",
      "\tValidation: loss=3.672, accuracy=0.255\n",
      "\twriting..\n",
      "Round 13\n",
      "\tTrain: loss=3.517, accuracy=0.210\n",
      "\tValidation: loss=3.622, accuracy=0.258\n",
      "\twriting..\n",
      "Round 14\n",
      "\tTrain: loss=3.498, accuracy=0.212\n",
      "\tValidation: loss=3.625, accuracy=0.256\n",
      "\twriting..\n",
      "Round 15\n",
      "\tTrain: loss=3.495, accuracy=0.213\n",
      "\tValidation: loss=3.609, accuracy=0.261\n",
      "\twriting..\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsAUXDwrNdHY"
   },
   "source": [
    "Training with early-stopping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qTci_WPx8qVb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "36dc7051-b360-4b94-eb5d-613deda81903"
   },
   "source": [
    "# Local model for evaluation\n",
    "'''keras_model = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
    "keras_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])'''\n",
    "\n",
    "tolerance = 7\n",
    "best_state = 0\n",
    "lowest_loss = 100.00\n",
    "stop = tolerance\n",
    "\n",
    "NUM_ROUNDS = 40\n",
    "with train_summary_writer.as_default():\n",
    "  for round_num in range(1, NUM_ROUNDS + 1):\n",
    "    print('Round {r}'.format(r=round_num))\n",
    "\n",
    "    # Uncomment to simulate sparse availabily of clients\n",
    "    # train_data_for_this_round, val_data_for_this_round = sample((federated_train_data, federated_val_data), 20, NUM_CLIENTS)\n",
    "\n",
    "    state, metrics = fed_avg.next(state, federated_train_data)\n",
    "\n",
    "    train_metrics = metrics['train']\n",
    "    print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "    val_metrics = evaluation(state.model, federated_val_data)\n",
    "    print('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
    "    \n",
    "    # Check for decreasing validation loss\n",
    "    if lowest_loss > val_metrics['loss']:\n",
    "      print('\\tSaving best model..')\n",
    "      lowest_loss = val_metrics['loss']\n",
    "      best_state = state\n",
    "      stop = tolerance - 1 \n",
    "    else:\n",
    "      stop = stop - 1\n",
    "      if stop <= 0:\n",
    "        print('\\tEarly stopping...')\n",
    "        break;\n",
    "    \n",
    "    # keras_evaluate(state, round_num, val_dataset)\n",
    "    # Evaluation\n",
    "    '''state.model.assign_weights_to(keras_model)\n",
    "    loss, accuracy = keras_model.evaluate(val_dataset)\n",
    "    print('\\tEVAL: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))'''\n",
    "    print(' ')\n",
    "    print('\\twriting..')\n",
    "\n",
    "    # Iterate across the metrics and write their data\n",
    "    for name, value in dict(train_metrics).items():\n",
    "      # print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
    "      tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "    with val_summary_writer.as_default():\n",
    "      for name, value in dict(val_metrics).items():\n",
    "        # print('\\twriting..')\n",
    "        # print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
    "        tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "train_summary_writer.close()\n",
    "val_summary_writer.close()"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SummaryWriter is already closed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23824\\1140986529.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[0mNUM_ROUNDS\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m40\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[1;32mwith\u001B[0m \u001B[0mtrain_summary_writer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_default\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m   \u001B[1;32mfor\u001B[0m \u001B[0mround_num\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mNUM_ROUNDS\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Round {r}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mround_num\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\contextlib.py\u001B[0m in \u001B[0;36m__enter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[1;32mdel\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    111\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 112\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    113\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    114\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"generator didn't yield\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py\u001B[0m in \u001B[0;36mas_default\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    260\u001B[0m     \u001B[1;34m\"\"\"Returns a context manager that enables summary writing.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    261\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_v2\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_closed\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 262\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"SummaryWriter is already closed\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    263\u001B[0m     \u001B[0mold\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_summary_state\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriter\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    264\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: SummaryWriter is already closed"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bka_CJCN8Ce"
   },
   "source": [
    "Test best saved model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8V-qfu-LZlQX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "2e810b5f-90fc-408a-f999-3170dd7096c1"
   },
   "source": [
    "# Test the model\n",
    "test_metrics = evaluation(best_state.model, federated_test_data)\n",
    "print('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M3goiwxoC0GN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "22b17eef-8498-4215-d1d3-079fee0b3d94"
   },
   "source": [
    "# Centralized test\n",
    "keras_evaluate(best_state, dataset=test_dataset)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkYXS--qN_le"
   },
   "source": [
    "Test last model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eLrl_0QwRAO7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "e1311cba-bad0-4bfa-f1b6-fc5ec3cda060"
   },
   "source": [
    "# Test the model\n",
    "test_metrics = evaluation(state.model, federated_test_data)\n",
    "print('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yM2UUiSLRAO-",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "dfbfc6a1-2f2a-4bb4-93cc-baec79870f88"
   },
   "source": [
    "# Centralized test\n",
    "keras_evaluate(state, dataset=test_dataset)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAO-inmtOCYK"
   },
   "source": [
    "Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Chncva45qDiP",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "outputId": "6ecb39d6-b47e-4fcc-8e16-8e654748a1ff"
   },
   "source": [
    "import datetime, os\n",
    "#!kill 719 # If you want to kill the process with the PID\n",
    "#%reload_ext tensorboard\n",
    "\n",
    "# We cannot use the logdir variable, we need to define it manually\n",
    "#%tensorboard --logdir '/content/gdrive/My Drive/NYC Dataset/tb/'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58sCHv3kkpBV"
   },
   "source": [
    "### Training by sampling clients at each round"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z7fXx_3mypnK"
   },
   "source": [
    "# sample a subset of clients\n",
    "# federated_data is a tuple with the train and the validation data\n",
    "def sample(federate_data, n, n_clients):\n",
    "  client_ids = np.random.choice(n_clients, n, replace=False).astype(int)\n",
    "  return [federate_data[0][i] for i in client_ids], [federate_data[1][i] for i in client_ids]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbNLo4IS7kRb"
   },
   "source": [
    "Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oSZ10fVz7kRc"
   },
   "source": [
    "#@test {\"skip\": true}\n",
    "# Log directory where we want to save the logs\n",
    "train_logdir = './FL/tb/fl_rnn_sampling_new/train'\n",
    "val_logdir = './FL/tb/fl_rnn_sampling_new/val'\n",
    "eval_logdir = './FL/tb/fl_rnn_sampling_new/eval'\n",
    "\n",
    "# Summary writer to save the logs\n",
    "train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_logdir)\n",
    "eval_summary_writer = tf.summary.create_file_writer(eval_logdir)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JuuEVKxJhK3l",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "outputId": "670ad6c6-e01f-449a-ff2b-684aca2076a8"
   },
   "source": [
    "NUM_ROUNDS = 10\n",
    "with train_summary_writer.as_default():\n",
    "  for round_num in range(1, NUM_ROUNDS + 1):\n",
    "    print('Round {r}'.format(r=round_num))\n",
    "    train_data_for_this_round, val_data_for_this_round = sample((federated_train_data, federated_val_data), 50, NUM_CLIENTS)\n",
    "    state, metrics = fed_avg.next(state, train_data_for_this_round)\n",
    "    train_metrics = metrics['train']\n",
    "    print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
    "    val_metrics = evaluation(state.model, federated_val_data)\n",
    "    print('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
    "    #keras_evaluate(state, round_num, val_dataset, tb=1)\n",
    "    print(' ')\n",
    "\n",
    "    print('\\twriting..')  \n",
    "    # Iterate across the metrics and write their data\n",
    "    for name, value in dict(train_metrics).items():\n",
    "      \n",
    "       #print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
    "      tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "    with val_summary_writer.as_default():\n",
    "      for name, value in dict(val_metrics).items():\n",
    "        #print('\\twriting..')\n",
    "        #print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
    "        tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "        \n",
    "train_summary_writer.close()\n",
    "val_summary_writer.close()\n",
    "eval_summary_writer.close()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bbgNpWlbhBuf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "c3310b56-074f-4493-ed7d-c743c002c00d"
   },
   "source": [
    "# test the model\n",
    "test_metrics = evaluation(state.model, federated_test_data)\n",
    "print('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Centralized test\n",
    "keras_evaluate(state, dataset=test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
