{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "BM_JfZIb8RJ4",
    "ldkjKrsJ4pp2",
    "6xXnGhPG847U",
    "VaUoDXQH8nma",
    "TiOAYiPoRHX8",
    "9mO9GhIX-eOs",
    "KeFa_8ib-lsh",
    "1Fwxl63f-r1o",
    "0y3PXv_HIKtG",
    "_P3Bz5ybCP8n",
    "x9M6nRTRtnnp",
    "IAQiWiQuAm5m",
    "IbGL4H46Fr4m",
    "OXiYUurFb6W-",
    "HWL1y_J2aCKP",
    "kkHTMHF6dVul",
    "Ez2uY1aeH13X",
    "WdOPilYwFr4o",
    "dAek8SYHFr4r",
    "5jKXypPuFr4u",
    "Y9xXMfnyHGxb",
    "So1tZSjCHa_T",
    "wohxHfkYNzRN",
    "jLUEgiFHR0ML",
    "t4VPhByfNLzD",
    "merBb7bGFyEC",
    "51pbHveUIWfN",
    "3vs08BCsKOex",
    "tnQGwcaQZWe2",
    "H8OiUkguIswO"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "114ayRZRLiIV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NYC Taxi Data Preprocessing\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VxN3oGVaCaiS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cde4bf92-8ffa-41eb-ae18-231822f23b89",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!pip install pyshp\n",
    "!pip install haversine\n",
    "!pip install geopandas\n",
    "!pip install pygeos"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyshp\n",
      "  Downloading pyshp-2.3.1-py2.py3-none-any.whl (46 kB)\n",
      "Installing collected packages: pyshp\n",
      "Successfully installed pyshp-2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\julia\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting haversine\n",
      "  Downloading haversine-2.8.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: haversine\n",
      "Successfully installed haversine-2.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\julia\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Downloading geopandas-0.10.2-py2.py3-none-any.whl (1.0 MB)\n",
      "Collecting fiona>=1.8\n",
      "  Downloading Fiona-1.9.1-cp37-cp37m-win_amd64.whl (22.0 MB)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Downloading pyproj-3.2.1-cp37-cp37m-win_amd64.whl (6.2 MB)\n",
      "Collecting shapely>=1.6\n",
      "  Downloading shapely-2.0.1-cp37-cp37m-win_amd64.whl (1.4 MB)\n",
      "Requirement already satisfied: pandas>=0.25.0 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from geopandas) (1.3.5)\n",
      "Collecting click-plugins>=1.0\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from fiona>=1.8->geopandas) (60.2.0)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from fiona>=1.8->geopandas) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from fiona>=1.8->geopandas) (19.3.0)\n",
      "Collecting munch>=2.3.2\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: click~=8.0 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from fiona>=1.8->geopandas) (8.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from pandas>=0.25.0->geopandas) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from pandas>=0.25.0->geopandas) (2022.7)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from click~=8.0->fiona>=1.8->geopandas) (4.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from click~=8.0->fiona>=1.8->geopandas) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from importlib-metadata->click~=8.0->fiona>=1.8->geopandas) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from importlib-metadata->click~=8.0->fiona>=1.8->geopandas) (4.4.0)\n",
      "Installing collected packages: munch, cligj, click-plugins, shapely, pyproj, fiona, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.1 geopandas-0.10.2 munch-2.5.0 pyproj-3.2.1 shapely-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\julia\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygeos\n",
      "  Downloading pygeos-0.14-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: numpy>=1.13 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from pygeos) (1.21.6)\n",
      "Installing collected packages: pygeos\n",
      "Successfully installed pygeos-0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\julia\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vcmo_cKrInMd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d97c06ab-133b-4024-cabd-16efc3c991e3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!sudo apt install libspatialindex-dev"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XLZ8aicbIstr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0902c2c4-4b6c-4b73-aac2-a4bb2d542290",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!pip install Rtree"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Rtree\n",
      "  Downloading Rtree-1.0.1-cp37-cp37m-win_amd64.whl (433 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7 in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (from Rtree) (4.4.0)\n",
      "Installing collected packages: Rtree\n",
      "Successfully installed Rtree-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\julia\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pyshp"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXulHYfTDvGt",
    "outputId": "d8574522-d08a-4d08-e073-ea70da220d40",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyshp in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (2.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\julia\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install haversine"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97DbIsyAEA3Y",
    "outputId": "8bae2cdc-ac56-4181-ff61-5a01b2140334",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haversine in c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\julia\\pycharmprojects\\humanmobilitypredictionma\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\julia\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use SQL-Alchemy for queries to data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gGkwKfqGfFof",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "import gc\n",
    "\n",
    "# libraries to visualize data\n",
    "import shapefile\n",
    "from shapely.geometry import Polygon\n",
    "from descartes.patch import PolygonPatch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Access to a database to operate on big data\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "nyc_database = create_engine('sqlite:///nyc_database.db')\n",
    "\n",
    "# compute additional features\n",
    "from haversine import haversine\n",
    "from pandas import DataFrame\n",
    "from numba import njit\n",
    "from math import radians, cos, sin, asin, sqrt"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'descartes'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_10084\\3651969768.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mshapefile\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mshapely\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgeometry\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPolygon\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mdescartes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpatch\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPolygonPatch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mmpl\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'descartes'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "7dICVdilaJLA",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import to SQL database for the relevant columns into Alchemy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Insert datasets to the db\n",
    "j, chunksize = 1, 100000\n",
    "for month in range(1,2):\n",
    "    fp = \"./trip_data/trip_data_{0}.csv\".format(month)\n",
    "    for df in pd.read_csv(fp, chunksize=chunksize, iterator=True):\n",
    "        df = df.rename(columns={c: c.replace(' ', '_') for c in df.columns})\n",
    "        df.drop(['vendor_id', 'rate_code', 'store_and_fwd_flag', 'trip_time_in_secs', 'trip_distance'],\n",
    "\t\t\t\t\t\t axis=1, inplace=True)\n",
    "        df.index += j\n",
    "        df.to_sql('trip_record', nyc_database, if_exists='append')\n",
    "        j = df.index[-1] + 1\n",
    "del df"
   ],
   "metadata": {
    "id": "okGBRfwIaIir",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show first 100 records for the database"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check what has been inserted\n",
    "pd.read_sql_query('SELECT * FROM trip_record LIMIT 100', nyc_database)"
   ],
   "metadata": {
    "id": "jTnqAY2oaQ44",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "outputId": "2b944725-fab3-4daf-9326-3a7db95305ff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper values to verify data integrity and validity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper values to define validity\n",
    "\n",
    "# Find the NYC outliers\n",
    "min_lat = 40.5774\n",
    "max_lat = 40.9176\n",
    "min_long = -74.15\n",
    "max_long = -73.7004\n",
    "\n",
    "# valid area (bounding box around the 4 states)\n",
    "min_lat = 38.8472\n",
    "max_lat = 45.0153\n",
    "min_long = -80.5243\n",
    "max_long = -71.7517\n",
    "\n",
    "\n",
    "# upper and lower limits for the datetimes\n",
    "upper = '2013-12-31 23:59:59'\n",
    "lower = '2013-01-01 00:00:00'\n",
    "\n",
    "# Logic operators to connect where clauses\n",
    "OR = ' OR '\n",
    "AND = ' AND '\n",
    "NOT = 'NOT '"
   ],
   "metadata": {
    "id": "e8WX-tffbXfQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build queries to check for incorrect rows, outliers (boundary box NYC), malformed data (timestamps, NULL-values, etc.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Records with not null coordinates\n",
    "c1 = ( '((pickup_latitude IS NOT NULL AND dropoff_latitude IS NOT NULL) AND ' + \n",
    "         '(pickup_longitude IS NOT NULL AND dropoff_longitude IS NOT NULL))')\n",
    "\n",
    "# Records with coordinates in the range\n",
    "c2 = ('((dropoff_longitude BETWEEN :min_long AND :max_long ) AND '+\n",
    "         '(dropoff_latitude BETWEEN :min_lat AND :max_lat) AND '+\n",
    "      '(pickup_longitude BETWEEN :min_long AND :max_long ) AND '+\n",
    "         '(pickup_latitude BETWEEN :min_lat AND :max_lat))')\n",
    "\n",
    "# Records with pickup coordinates not equal to dropoff\n",
    "c3 = ('((pickup_latitude != dropoff_latitude) OR ' +\n",
    "         '(pickup_longitude != dropoff_longitude ))')\n",
    "\n",
    "# Records with different coordinates but distance greater 0\n",
    "c4 = '(trip_distance > 0.1)'\n",
    "\n",
    "# Records with pickup time smaller than dropoff time\n",
    "c5 = '(pickup_datetime < dropoff_datetime)'\n",
    "\n",
    "# Records with dates in the right range\n",
    "c6 = ('((pickup_datetime <= :upper AND pickup_datetime >= :lower ) AND (dropoff_datetime <= :upper AND dropoff_datetime >= :lower ))')\n",
    "\n",
    "# Not Zero passengers and passenger count in the range\n",
    "c7 = '((passenger_count BETWEEN 1 AND 6) AND passenger_count IS NOT null )'\n",
    "\n",
    "# Records with not null datetimes\n",
    "c8 = '(pickup_datetime IS NOT NULL AND dropoff_datetime IS NOT NULL)'\n",
    "\n",
    "where_clause = '(' + c1 + AND + c2 + AND + c3 + AND + c5 + AND + c6 + AND + c7 + AND + c8 + ')'\n",
    "\n",
    "# Count valid rows\n",
    "query = ('SELECT count(*) as number FROM trip_record WHERE ' + where_clause)\n",
    "res = pd.read_sql_query( query, nyc_database, params={\"min_long\":min_long, \"max_long\":max_long, \"min_lat\":min_lat, \"max_lat\":max_lat, \"upper\":upper, \"lower\":lower})\n",
    "\n",
    "# Save the result \n",
    "valid = res['number'][0]\n",
    "print('Number of valid rows: ', valid)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1UyazO_YbCfS",
    "outputId": "3261f2e9-f023-462a-8afb-8d61a0a3a9ee",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Select the valid records\n",
    "query = ('SELECT *  FROM trip_record WHERE '+ where_clause )\n",
    "valid_records = pd.read_sql_query( query, nyc_database, params={\"min_long\":min_long, \"max_long\":max_long, \"min_lat\":min_lat, \"max_lat\":max_lat, \"upper\":upper, \"lower\":lower})\n",
    "\n",
    "# Shape of the result\n",
    "valid_records.shape"
   ],
   "metadata": {
    "id": "w6rsDAx9bJ1M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper function to compute haversine distance (computed distance between two locations), trip time and time differences as well as speed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from haversine import haversine\n",
    "\n",
    "# Compute the haversine distance of a single trip (var_row) in Km\n",
    "def haversine_distance(var_row):\n",
    "  h_distance =  haversine((var_row[\"pickup_latitude\"], var_row[\"pickup_longitude\"]), \n",
    "                     (var_row[\"dropoff_latitude\"], var_row[\"dropoff_longitude\"]))\n",
    "\t# Round the computed distance to the third decimal\n",
    "  return round(h_distance, 3)"
   ],
   "metadata": {
    "id": "Cu0XF4G0br85",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Time difference within a trip\n",
    "def compute_trip_time(var_row):\n",
    "  elapsedTime = datetime.strptime(var_row[\"dropoff_datetime\"], '%Y-%m-%d %H:%M:%S') - datetime.strptime(var_row[\"pickup_datetime\"], '%Y-%m-%d %H:%M:%S')\n",
    "  duration_seconds = elapsedTime.total_seconds()\n",
    "  duration_hours = duration_seconds/3600\n",
    "\t# Round the computed hours to the third decimal\n",
    "  return round(duration_hours, 3)\n",
    "\n",
    "# Time difference between two different trips\n",
    "def compute_time_difference_trips(datetime_1, datetime_2):\n",
    "  elapsedTime = datetime.strptime(datetime_2, '%Y-%m-%d %H:%M:%S') - datetime.strptime(datetime_1, '%Y-%m-%d %H:%M:%S')\n",
    "  duration_seconds = elapsedTime.total_seconds()\n",
    "  duration_hours = duration_seconds/3600\n",
    "\t# Round the computed hours to the third decimal\n",
    "  return round(duration_hours, 3)"
   ],
   "metadata": {
    "id": "Pdl93Tg9bseP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Estimate the average estimated speed of the trip in km/h\n",
    "def estimate_speed(var_row):\n",
    "  speed = var_row[\"haversine_distance\"]/var_row[\"trip_time\"]\n",
    "\t# Round the estimated speed to the first decimal\n",
    "  return round(speed, 1)"
   ],
   "metadata": {
    "id": "9d9xYaNTbxMQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert datetimes type\n",
    "valid_records[\"pickup_datetime\"] = pd.to_datetime(valid_records[\"pickup_datetime\"])\n",
    "valid_records[\"dropoff_datetime\"] = pd.to_datetime(valid_records[\"dropoff_datetime\"])"
   ],
   "metadata": {
    "id": "dYg5E-J3cg7b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute time difference in seconds\n",
    "valid_records['trip_time'] = (valid_records['dropoff_datetime'] - valid_records['pickup_datetime']).dt.total_seconds()"
   ],
   "metadata": {
    "id": "dt6Q69U5ciNb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from numba import njit\n",
    "\n",
    "# fix problematic times and convert time from seconds into hours\n",
    "@njit\n",
    "def refine_time(time_difference):\n",
    "  res = np.empty(time_difference.shape)\n",
    "\t# Cycle through all the trip_times\n",
    "  for i in range(len(time_difference)):\n",
    "\t\t# Convert from seconds to hours\n",
    "    duration_hours = time_difference[i]/3600.0\n",
    "\t\t# If time is to small, set it to the minimum\n",
    "    if duration_hours < 0.005:\n",
    "      trip_time = 0.005\n",
    "    else:\n",
    "      trip_time = round(duration_hours, 3)\n",
    "    res[i] = trip_time\n",
    "  return res"
   ],
   "metadata": {
    "id": "XIRc2xvScnHc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert the trip_times\n",
    "valid_records['trip_time'] = refine_time(valid_records['trip_time'].values)"
   ],
   "metadata": {
    "id": "HDcXuqt7cprH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine_np( lat1, lon1, lat2, lon2):\n",
    "    # earth radius\n",
    "    R = 6371.0\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = R * c\n",
    "    return np.round(km, 3)"
   ],
   "metadata": {
    "id": "x7fLxBiQcrle",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute haversine_distance\n",
    "valid_records['haversine_distance'] = haversine_np(valid_records[\"pickup_latitude\"], valid_records[\"pickup_longitude\"], \n",
    "                     valid_records[\"dropoff_latitude\"], valid_records[\"dropoff_longitude\"])"
   ],
   "metadata": {
    "id": "N68LMQcPcv41",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute the estimated speed\n",
    "valid_records['speed'] = round(valid_records['haversine_distance']/valid_records['trip_time'], 1)"
   ],
   "metadata": {
    "id": "mKQJuXrbcyUq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Tresholds\n",
    "haversine_distance_t = (0.1, 70)\n",
    "trip_time_t = (0.1, 4.0)\n",
    "speed_t = (5, 60)\n",
    "\n",
    "# Define more restrictive thresholds for the intermediate trips\n",
    "intermediate_distance_t = (0.5, 20)\n",
    "intermediate_trip_time_t = (0.1, 1.0)\n",
    "intermediate_speed_t = (5, 40)"
   ],
   "metadata": {
    "id": "_m7Od9jDc-8c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove outliers in terms of speed (must be incorrect data points)."
   ],
   "metadata": {
    "id": "zOWUZj859fnQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove the records\n",
    "valid_records = valid_records.loc[(valid_records.speed >= speed_t[0]) &\n",
    "(valid_records.speed <= speed_t[1])]\n",
    "\n",
    "# Plot the new curve\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(valid_records[\"speed\"])), np.sort(valid_records[\"speed\"].values), color = 'green')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('speed in km/h')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "gTeFQXY1c6Tg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove the records\n",
    "d_records = valid_records.loc[(valid_records.haversine_distance >= haversine_distance_t[0]) &\n",
    "(valid_records.haversine_distance <= haversine_distance_t[1])]\n",
    "\n",
    "# Plot the new curve\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(valid_records[\"haversine_distance\"])), np.sort(valid_records[\"haversine_distance\"]), color = 'blue')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('haversine_distance in km')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "CyTjvRlCdAPl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove the records\n",
    "valid_records = valid_records.loc[(valid_records.trip_time >= trip_time_t[0]) &\n",
    "(valid_records.trip_time <= trip_time_t[1])]\n",
    "\n",
    "# Plot the new curve\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(valid_records[\"trip_time\"])), np.sort(valid_records[\"trip_time\"]), color = 'red')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('trip_time in km')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "wCrjKmwWdBiI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate the trips for our dataset."
   ],
   "metadata": {
    "id": "nqtk2mhf9h9X",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Retrieve the valid trips\n",
    "query = ('SELECT medallion, hack_license, pickup_datetime, dropoff_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude '+\n",
    "         'FROM trip_record WHERE '+ where_clause + ' ORDER BY medallion, pickup_datetime')\n",
    "t1 = datetime.now()\n",
    "valid_records = pd.read_sql_query( query, nyc_database, params={\"min_long\":min_long, \"max_long\":max_long, \"min_lat\":min_lat, \"max_lat\":max_lat, \"upper\":upper, \"lower\":lower})\n",
    "t2 = datetime.now()\n",
    "delta = t2 - t1\n",
    "print(\"time for query: {}\".format(delta.seconds))"
   ],
   "metadata": {
    "id": "GzppvJKE9e1B",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_records[\"pickup_datetime\"] = pd.to_datetime(valid_records[\"pickup_datetime\"])\n",
    "valid_records[\"dropoff_datetime\"] = pd.to_datetime(valid_records[\"dropoff_datetime\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rename columns and concatenate in order to have aligned rows for the trips."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Rename the columns for the dataframe copy\n",
    "print(\"Renaming the new columns...\")\n",
    "new_columns_name = np.apply_along_axis(lambda column: column+'_b', 0, np.array(valid_records.columns))\n",
    "\n",
    "# Copy the dataframe and use the new columns names (takes some time..)\n",
    "print(\"Copying the dataframe...\")\n",
    "valid_records_copy  = valid_records.copy()\n",
    "valid_records_copy.columns = new_columns_name\n",
    "\n",
    "# Shift the copy dataframe\n",
    "print(\"Shifting the copied dataframe...\")\n",
    "valid_records_copy = valid_records_copy.shift(-1)\n",
    "\n",
    "# concatenate the dataframes \n",
    "# NB we have used dropna() because the last row will have no a next trip\n",
    "print(\"Concatenating the two dataframes...\")\n",
    "concatenated_dataframes = pd.concat([valid_records, valid_records_copy], axis=1, sort=False).dropna()"
   ],
   "metadata": {
    "id": "kYokDMod-Axm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# check for columns types\n",
    "concatenated_dataframes.dtypes"
   ],
   "metadata": {
    "id": "ucOBx8_D-FJk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute the features of the intermediate trip\n",
    "\n",
    "# intermediate time\n",
    "concatenated_dataframes['intermediate_time'] = (concatenated_dataframes['pickup_datetime_b'] - \n",
    "                                                concatenated_dataframes['dropoff_datetime']).dt.total_seconds()\n",
    "concatenated_dataframes['intermediate_time'] = refine_time(concatenated_dataframes['intermediate_time'].values)\n",
    "\n",
    "# intermediate distance\n",
    "concatenated_dataframes['intermediate_distance'] = haversine_np(concatenated_dataframes[\"pickup_latitude_b\"].values, concatenated_dataframes[\"pickup_longitude_b\"].values, \n",
    "                     concatenated_dataframes[\"dropoff_latitude\"].values, concatenated_dataframes[\"dropoff_longitude\"].values)\n",
    "\n",
    "# intermediate speed\n",
    "concatenated_dataframes['intermediate_speed'] = round(concatenated_dataframes['intermediate_distance']/concatenated_dataframes['intermediate_time'], 1)"
   ],
   "metadata": {
    "id": "CAUR7rCB-HhP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Min time allowed between two trips and min intermediate trip length\n",
    "min_time = intermediate_trip_time_t[0]\n",
    "\n",
    "# Max time allowed between two trips and max intermediate trip length\n",
    "max_time = intermediate_trip_time_t[1]\n",
    "\n",
    "# Min distance allowed between two trips and min intermediate trip distance\n",
    "min_distance = intermediate_distance_t[0]\n",
    "\n",
    "# Max distance allowed between two trips and max intermediate trip distance\n",
    "max_distance = intermediate_distance_t[1]\n",
    "\n",
    "# Min speed allowed between two trips and min intermediate trip speed\n",
    "min_speed = intermediate_speed_t[0]\n",
    "\n",
    "# Max speed allowed between two trips and max intermediate trip speed\n",
    "max_speed = intermediate_speed_t[1]"
   ],
   "metadata": {
    "id": "Ivjursbw-IQx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "CCheck for all the possible trips that lie within the defined ranges for time, distance and speed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Filter the possible trips using the ranges\n",
    "candidates = concatenated_dataframes.loc[((concatenated_dataframes['medallion'] == concatenated_dataframes['medallion_b']) & (concatenated_dataframes['hack_license'] == concatenated_dataframes['hack_license_b'])) &\n",
    "                                         ((concatenated_dataframes['intermediate_time'] <= max_time) & (concatenated_dataframes['intermediate_time'] >= min_time)) & \n",
    "                                         ((concatenated_dataframes['intermediate_distance'] <= max_distance) & (concatenated_dataframes['intermediate_distance'] >= min_distance)) &\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ((concatenated_dataframes['intermediate_speed'] <= max_speed) & (concatenated_dataframes['intermediate_speed'] >= min_speed))]\n",
    "\n",
    "candidates.shape"
   ],
   "metadata": {
    "id": "E3xA_VRw-Lw3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Select just the desired columns\n",
    "intermediate_trips = candidates[['medallion', 'hack_license', 'dropoff_datetime', 'pickup_datetime_b', 'dropoff_longitude', 'dropoff_latitude',\n",
    "                                 'pickup_longitude_b','pickup_latitude_b', 'intermediate_distance', 'intermediate_time','intermediate_speed']]\n",
    "\n",
    "# Rename the columns\n",
    "intermediate_trips = intermediate_trips.rename(columns={'medallion':'medallion', 'hack_license':'hack_license', 'dropoff_datetime':'pickup_datetime', 'pickup_datetime_b':'dropoff_datetime', \n",
    "                                   'pickup_longitude_b':'dropoff_longitude',\n",
    "                                   'pickup_latitude_b':'dropoff_latitude', \n",
    "                                    'dropoff_longitude':'pickup_longitude',\n",
    "                                   'dropoff_latitude':'pickup_latitude', \n",
    "                                   'intermediate_distance': 'haversine_distance',\n",
    "                                   'intermediate_time':'trip_time', 'intermediate_speed':'speed'})"
   ],
   "metadata": {
    "id": "2TmlpJNf-QTC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Select just the desired columns\n",
    "intermediate_trips = candidates[['medallion', 'hack_license', 'dropoff_datetime', 'pickup_datetime_b', 'dropoff_longitude', 'dropoff_latitude',\n",
    "                                 'pickup_longitude_b','pickup_latitude_b', 'intermediate_distance', 'intermediate_time','intermediate_speed']]\n",
    "\n",
    "# Rename the columns\n",
    "intermediate_trips = intermediate_trips.rename(columns={'medallion':'medallion', 'hack_license':'hack_license', 'dropoff_datetime':'pickup_datetime', 'pickup_datetime_b':'dropoff_datetime', \n",
    "                                   'pickup_longitude_b':'dropoff_longitude',\n",
    "                                   'pickup_latitude_b':'dropoff_latitude', \n",
    "                                    'dropoff_longitude':'pickup_longitude',\n",
    "                                   'dropoff_latitude':'pickup_latitude', \n",
    "                                   'intermediate_distance': 'haversine_distance',\n",
    "                                   'intermediate_time':'trip_time', 'intermediate_speed':'speed'})"
   ],
   "metadata": {
    "id": "A-jdLETU-T99",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate all the valid records from the trip data and valid records data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Union of the original valid trips and the new intermediate trips\n",
    "all_valid_records = pd.concat([valid_records, intermediate_trips], axis=0)\n",
    "\n",
    "# Sort the records by medallion and pickup_time\n",
    "all_valid_records = all_valid_records.sort_values(by=['medallion', 'pickup_datetime'])\n",
    "\n",
    "all_valid_records.shape"
   ],
   "metadata": {
    "id": "8gjHkzb--dqm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# speed\n",
    "all_valid_records = all_valid_records.loc[(all_valid_records.speed >= speed_t[0]) & \n",
    "                                          (all_valid_records.speed <= speed_t[1])]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(all_valid_records[\"speed\"])), np.sort(all_valid_records[\"speed\"].values), color = 'green')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('speed in km/h')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "MqQAAjMc-hzZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# haversine_distance\n",
    "all_valid_records = all_valid_records.loc[(all_valid_records.haversine_distance >= haversine_distance_t[0]) & \n",
    "                                          (all_valid_records.haversine_distance <= haversine_distance_t[1])]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(all_valid_records[\"haversine_distance\"])), np.sort(all_valid_records[\"haversine_distance\"]), color = 'blue')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('haversine_distance in km')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "CHhqSTfE-jg8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# trip_time\n",
    "all_valid_records = all_valid_records.loc[(all_valid_records.trip_time >= trip_time_t[0]) & \n",
    "                                          (all_valid_records.trip_time <= trip_time_t[1])]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(all_valid_records[\"trip_time\"])), np.sort(all_valid_records[\"trip_time\"]), color = 'red')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('trip_time in km')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "TTGWlWKb-k8k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# pickup\n",
    "max_pickup_date = all_valid_records[\"pickup_datetime\"].max()\n",
    "min_pickup_date= all_valid_records[\"pickup_datetime\"].min()\n",
    "\n",
    "\n",
    "# dropoff\n",
    "max_dropoff_date = all_valid_records[\"dropoff_datetime\"].max()\n",
    "min_dropoff_date= all_valid_records[\"dropoff_datetime\"].min()\n",
    "\n",
    "print(\"Pickup: \")\n",
    "print(\"Min Date: \", min_pickup_date)\n",
    "print(\"Max Date: \", max_pickup_date)\n",
    "print(\" \")\n",
    "print(\"Dropoff: \")\n",
    "print(\"Min Date: \", min_dropoff_date)\n",
    "print(\"Max Date: \", max_dropoff_date)"
   ],
   "metadata": {
    "id": "oihyr-_O-qpP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# pickup\n",
    "max_pickup_longitude = all_valid_records[\"pickup_longitude\"].max()\n",
    "min_pickup_longitude= all_valid_records[\"pickup_longitude\"].min()\n",
    "max_pickup_latitude = all_valid_records[\"pickup_latitude\"].max()\n",
    "min_pickup_latitude= all_valid_records[\"pickup_latitude\"].min()\n",
    "\n",
    "# dropoff\n",
    "max_dropoff_latitude = all_valid_records[\"dropoff_latitude\"].max()\n",
    "min_dropoff_latitude= all_valid_records[\"dropoff_latitude\"].min()\n",
    "max_dropoff_longitude = all_valid_records[\"dropoff_longitude\"].max()\n",
    "min_dropoff_longitude= all_valid_records[\"dropoff_longitude\"].min()\n",
    "\n",
    "print(\"Pickup: \")\n",
    "print(\"Min Latitude: \", min_pickup_latitude)\n",
    "print(\"Min Longitude: \", min_pickup_longitude)\n",
    "print(\"Max Latitude: \", max_pickup_latitude)\n",
    "print(\"Max Longitude: \", max_pickup_longitude)\n",
    "print(\" \")\n",
    "print(\"Dropoff: \")\n",
    "print(\"Min Latitude: \", min_dropoff_latitude)\n",
    "print(\"Min Longitude: \", min_dropoff_longitude)\n",
    "print(\"Max Latitude: \", max_dropoff_latitude)\n",
    "print(\"Max Longitude: \", max_dropoff_longitude)"
   ],
   "metadata": {
    "id": "grQLMI_a-szY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Medallion (Unique Taxi ID) value counts are displayed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Medallions\n",
    "all_valid_records['medallion'].value_counts()"
   ],
   "metadata": {
    "id": "PbgJgsWF_-Sf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most used Taxis have over 500 trips."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display mean statistics on haversine distance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# haversine_distance\n",
    "mean_d = all_valid_records[\"haversine_distance\"].mean()\n",
    "max_d = all_valid_records[\"haversine_distance\"].max()\n",
    "min_d = all_valid_records[\"haversine_distance\"].min()\n",
    "\n",
    "print(\"The mean distance is: \", mean_d)\n",
    "print(\"The max distance is: \", max_d)\n",
    "print(\"The min distance is: \", min_d)\n",
    "print(\"The distances are represented in Km unit\")"
   ],
   "metadata": {
    "id": "j5lPtzxMAC2A",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# trip_time\n",
    "mean_t = all_valid_records[\"trip_time\"].mean()\n",
    "max_t = all_valid_records[\"trip_time\"].max()\n",
    "min_t = all_valid_records[\"trip_time\"].min()\n",
    "\n",
    "print(\"The mean trip_time is: \", mean_t)\n",
    "print(\"The max trip_time is: \", max_t)\n",
    "print(\"The min trip_time is: \", min_t)\n",
    "print(\"The trip_times are represented in hours\")"
   ],
   "metadata": {
    "id": "I7hDuxWaAFCV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same for speed:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# speed\n",
    "mean_speed = all_valid_records[\"speed\"].mean()\n",
    "max_speed = all_valid_records[\"speed\"].max()\n",
    "min_speed = all_valid_records[\"speed\"].min()\n",
    "\n",
    "print(\"The mean speed is: \", mean_speed)\n",
    "print(\"The max speed is: \", max_speed)\n",
    "print(\"The min speed is: \", min_speed)"
   ],
   "metadata": {
    "id": "Fcb5RwOFAS86",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate new features for the time components."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract the new features\n",
    "all_valid_records[\"pickup_week_day\"] = all_valid_records.pickup_datetime.dt.dayofweek\n",
    "all_valid_records[\"pickup_hour\"] = all_valid_records.pickup_datetime.dt.hour\n",
    "all_valid_records[\"pickup_month\"] = all_valid_records.pickup_datetime.dt.month"
   ],
   "metadata": {
    "id": "0OnQPWdUAW8c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract the required columns\n",
    "# we use .copy() to unlink this dataframe from the original\n",
    "locations = all_valid_records[[\"pickup_longitude\", \"pickup_latitude\",\"dropoff_longitude\", \"dropoff_latitude\"]].copy()"
   ],
   "metadata": {
    "id": "50sPJqT-A1Ft",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Optional) Remove records outside the bounding box (NYC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# remove records outside nyc\n",
    "locations_nyc = locations.loc[(locations.pickup_latitude >= min_lat) &\n",
    "                                          (locations.pickup_latitude <= max_lat )]\n",
    "locations_nyc = locations_nyc.loc[(locations_nyc.pickup_longitude >= min_long ) &\n",
    "                                          (locations_nyc.pickup_longitude <= max_long )]\n",
    "\n",
    "# remove records outside nyc\n",
    "locations_nyc = locations_nyc.loc[(locations_nyc.dropoff_latitude >= min_lat) &\n",
    "                                          (locations_nyc.dropoff_latitude <= max_lat )]\n",
    "locations_nyc = locations_nyc.loc[(locations_nyc.dropoff_longitude >= min_long ) &\n",
    "                                          (locations_nyc.dropoff_longitude <= max_long )]"
   ],
   "metadata": {
    "id": "AZJClzwZA3P9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "GeoPandas Library to map lat/lon to areas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# GeoPandas library\n",
    "import geopandas as gpd\n",
    "# As Pandas, also Gpd has its kind of Series\n",
    "from geopandas import GeoSeries\n",
    "# We need shapely to import and define objects of Spatial types\n",
    "from shapely.geometry import Point, LineString, Polygon, asMultiPoint\n",
    "# I will introduce crs later\n",
    "import fiona.crs"
   ],
   "metadata": {
    "id": "xEnVlDQWA-df",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import shape file for custom generated zones (.shp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Path to the shape file\n",
    "fp = \"./zones/geo_export_be42d9eb-5829-406d-a057-01bd027a191b.shp\"\n",
    "\n",
    "# Read the data\n",
    "zones = gpd.read_file(fp)\n",
    "zones.head()"
   ],
   "metadata": {
    "id": "r0-dtZbOBBB6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert lon/lat stack to (Geo-)Points"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Stack latitude and longitude together into a Numpy array of N x 2 elements\n",
    "pickup = np.vstack((locations_nyc[\"pickup_longitude\"].values, locations_nyc[\"pickup_latitude\"].values)).T \n",
    "\n",
    "# Convert each pair of coordinates into a Point object\n",
    "t1 = datetime.now()\n",
    "points = [Point(i) for i in pickup]\n",
    "t2 = datetime.now()\n",
    "delta = t2 - t1\n",
    "print(\"time for conversion: {}\".format(delta.seconds))"
   ],
   "metadata": {
    "id": "ghXBhmBiBi_b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a GeoSeries from the Points and use Fiona and GeoPandas to generate a GeoDataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a GeoSeries from the set of Points\n",
    "gs = GeoSeries(points)\n",
    "\n",
    "# Use Fiona to set the correct crs to the GeoSeries\n",
    "gs.crs = fiona.crs.from_epsg(4326)\n",
    "\n",
    "# Create a GeoDataFrame from the GeoSeries\n",
    "gd = gpd.GeoDataFrame(geometry=gs)\n",
    "\n",
    "# Drop any possible Nan or None records\n",
    "gd =  gd.loc[gd.is_valid]\n",
    "gd.head()"
   ],
   "metadata": {
    "id": "azHGtKCvBkrF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Add pickup coordinates to the geodataframe\n",
    "gd['pickup_longitude'] = locations_nyc[\"pickup_longitude\"].values\n",
    "gd['pickup_latitude'] = locations_nyc[\"pickup_latitude\"].values\n",
    "gd.head()"
   ],
   "metadata": {
    "id": "qv2iy4t5Bl7f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#check\n",
    "gd.crs"
   ],
   "metadata": {
    "id": "hxwQP7ZPBoNE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spatial join the zones on the geo coordinates."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Perform the spatial join\n",
    "join = gpd.sjoin(gd, zones, how=\"inner\", op=\"within\")\n",
    "join.head()"
   ],
   "metadata": {
    "id": "L0baPmJZBrXf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# pickup_longitude | pickup_latitude | borough | location_i | zone\n",
    "pickup_non_spacial_data = pd.DataFrame({'pickup_longitude': join['pickup_longitude'], 'pickup_latitude':join['pickup_latitude'] ,\n",
    "                                 'pickup_borough': join['borough'], 'pickup_location_id': join['location_i'], 'pickup_zone': join['zone']})\n",
    "pickup_non_spacial_data.head()"
   ],
   "metadata": {
    "id": "wKTjax6ABvGD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same procedure for the dropoff coordinates:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# stack latitude and longitude together \n",
    "dropoff = np.vstack((locations_nyc[\"dropoff_longitude\"].values, locations_nyc[\"dropoff_latitude\"].values)).T \n",
    "\n",
    "# Convert each couple into a point object\n",
    "t1 = datetime.now()\n",
    "points = [Point(i) for i in dropoff]\n",
    "t2 = datetime.now()\n",
    "delta = t2 - t1\n",
    "print(\"time for conversion: {}\".format(delta.seconds))"
   ],
   "metadata": {
    "id": "qg5ddfapB0Pj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a GeoSeries from the set of Points\n",
    "gs = GeoSeries(points)\n",
    "\n",
    "# Use Fiona to set the correct crs to the GeoSeries\n",
    "gs.crs = fiona.crs.from_epsg(4326)\n",
    "\n",
    "# Create a GeoDataFrame from the GeoSeries\n",
    "gd = gpd.GeoDataFrame(geometry=gs)\n",
    "\n",
    "# Drop any possible Nan or None records\n",
    "gd =  gd.loc[gd.is_valid]\n",
    "gd.head()"
   ],
   "metadata": {
    "id": "TDnKkD_6B3j7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add pickup coordinates to the geodataframe\n",
    "gd['dropoff_longitude'] = locations_nyc[\"dropoff_longitude\"].values\n",
    "gd['dropoff_latitude'] = locations_nyc[\"dropoff_latitude\"].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform the spatial Join\n",
    "join = gpd.sjoin(gd, zones, how=\"inner\", op=\"within\")\n",
    "join.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dropoff_longitude | dropoff_latitude | borough | location_i | zone\n",
    "dropoff_non_spacial_data = pd.DataFrame({'dropoff_longitude': join['dropoff_longitude'], 'dropoff_latitude':join['dropoff_latitude'] ,\n",
    "                                 'dropoff_borough': join['borough'], 'dropoff_location_id': join['location_i'], 'dropoff_zone': join['zone']})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check in the full dataframe\n",
    "all_valid_records[all_valid_records.index.duplicated()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check in the pickup dataframe\n",
    "pickup_non_spacial_data[pickup_non_spacial_data.index.duplicated()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Drop duplicates\n",
    "dropoff_non_spacial_data = dropoff_non_spacial_data.loc[~dropoff_non_spacial_data.index.duplicated(keep='first')]"
   ],
   "metadata": {
    "id": "5xj7KM_bB-G6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate pickup and dropoff zones from the spacial data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# pickup zones\n",
    "pickup_zones = pickup_non_spacial_data[ ['pickup_longitude', 'pickup_latitude', 'pickup_location_id']].copy()\n",
    "\n",
    "# dropoff zones\n",
    "dropoff_zones = dropoff_non_spacial_data[ ['dropoff_longitude', 'dropoff_latitude', 'dropoff_location_id']].copy()\n",
    "\n",
    "# And to be sure let's save them into a csv\n",
    "PATH = './zones_results'\n",
    "pickup_zones.to_csv(PATH + '/pickup_zones_1.csv', index=False)\n",
    "dropoff_zones.to_csv(PATH + '/dropoff_zones_1.csv', index=False)"
   ],
   "metadata": {
    "id": "6DUzbWr-B_6w",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate the trips with zones dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# join the pickup zones\n",
    "trips_with_zones = pd.merge(all_valid_records, pickup_zones,  how='left',\n",
    "                  left_on=['pickup_longitude','pickup_latitude'],\n",
    "                  right_on = ['pickup_longitude','pickup_latitude'])\n",
    "trips_with_zones.shape"
   ],
   "metadata": {
    "id": "qJr0NjRRCP5V",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminate duplicates and join the pickup zones"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# drop duplicates\n",
    "pickup_zones = pickup_zones.drop_duplicates(subset=['pickup_longitude','pickup_latitude'])"
   ],
   "metadata": {
    "id": "b36ubPEPCVE-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# join the pickup zones again\n",
    "trips_with_zones = pd.merge(all_valid_records, pickup_zones,  how='left',\n",
    "                  left_on=['pickup_longitude','pickup_latitude'],\n",
    "                  right_on = ['pickup_longitude','pickup_latitude'])"
   ],
   "metadata": {
    "id": "HVZuj4bGCWDa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# drop duplicates\n",
    "dropoff_zones = dropoff_zones.drop_duplicates(subset=['dropoff_longitude','dropoff_latitude'])\n",
    "\n",
    "# Join\n",
    "trips_with_zones = pd.merge(trips_with_zones, dropoff_zones,  how='left',\n",
    "                  left_on = ['dropoff_longitude','dropoff_latitude'],\n",
    "                  right_on = ['dropoff_longitude','dropoff_latitude'])"
   ],
   "metadata": {
    "id": "XFVgdAdmCXEh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Find null locations id\n",
    "trips_with_zones.loc[(trips_with_zones.dropoff_location_id.isnull()) | (trips_with_zones.pickup_location_id.isnull())]"
   ],
   "metadata": {
    "id": "jQDUmEkPCZW-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Out of City ID for trips when data was not limited to the boundary box (optional code above)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Out if City ID\n",
    "OoC_ID = 0"
   ],
   "metadata": {
    "id": "pwawoRo_Cbwl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate code for trips in january only (smaller data size for faster model training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Select the desired columns\n",
    "january = trips_with_zones[['medallion', 'pickup_week_day', 'pickup_hour', 'pickup_month', 'pickup_location_id', 'dropoff_location_id']].copy()"
   ],
   "metadata": {
    "id": "QaHBL02SCfeM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# From NaN to 0 pickup\n",
    "january.loc[january.pickup_location_id.isnull(), 'pickup_location_id'] = 0.0\n",
    "\n",
    "# From NaN to 0 dropoff\n",
    "january.loc[january.dropoff_location_id.isnull(), 'dropoff_location_id'] = 0.0"
   ],
   "metadata": {
    "id": "q_kQMt7pCj1Y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "january.to_csv('./zones_results/trips_with_zones_1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmVGw_Qr1r4c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ORIGINAL CODE:\n",
    "Below is the original unedited code of the referenced work for the preprocessing with the original notes and conclusions.\n",
    "\n",
    "# Pre-Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eupnxOb45Cn8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import the dataset into the database"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RD6v7eu65HOH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Insert datasets to the db\n",
    "# Insert datasets from january to ....\n",
    "months = 2\n",
    "j, chunksize = 1, 100000\n",
    "for month in range(1,months):\n",
    "    fp = \"./trip_data/trip_data_{0}.csv\".format(month)\n",
    "    for df in pd.read_csv(fp, chunksize=chunksize, iterator=True):\n",
    "        df = df.rename(columns={c: c.replace(' ', '_') for c in df.columns})\n",
    "        df['pickup_hour'] = [x[11:16] for x in df['pickup_datetime']]\n",
    "        df['dropoff_hour'] = [x[11:16] for x in df['dropoff_datetime']]\n",
    "        df['pickup_date'] = [x[0:11] for x in df['pickup_datetime']]\n",
    "        df['dropoff_date'] = [x[0:11] for x in df['dropoff_datetime']]\n",
    "        df.drop(['vendor_id', 'rate_code', 'store_and_fwd_flag', 'trip_time_in_secs'], axis=1, inplace=True)\n",
    "        df.index += j\n",
    "        df.to_sql('trip_record', nyc_database, if_exists='append')\n",
    "        j = df.index[-1] + 1\n",
    "del df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HVW1xqgS5Z60",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# If we have some problem, just drop the table\n",
    "\n",
    "# DROP TABLE\n",
    "#pd.read_sql_query('DROP TABLE IF EXISTS trip_record', nyc_database)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6bhgPRWC5ofk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Total number of records\n",
    "number = pd.read_sql_query('SELECT count(*) as number FROM trip_record', nyc_database)\n",
    "print(\"Total number of records: \", number['number'][0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzZdrnZI6Kfr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Find invalid records"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oW2b-rNa6443",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Helper values to define validity\n",
    "\n",
    "# Find the outliers\n",
    "min_lat= 40.5774\n",
    "max_lat= 40.9176\n",
    "min_long= -74.15\n",
    "max_long= -73.7004\n",
    "\n",
    "# valid area (bounding box around the states)\n",
    "min_lat= 38.8472\n",
    "max_lat= 45.0153\n",
    "min_long= -80.5243\n",
    "max_long= -71.7517\n",
    "\n",
    "\n",
    "# Dates in years in the past or in the future\n",
    "upper = '2013-12-31 23:59:59'\n",
    "lower = '2013-01-01 00:00:00'\n",
    "\n",
    "# Logic operators\n",
    "OR = ' OR '\n",
    "AND = ' AND '\n",
    "NOT = 'NOT '"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eCuXLKlQ6PFY",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "7b17c4db-6cc1-4596-bb10-666d1dd5b4aa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Records with null coordinates\n",
    "c1 = ( '((pickup_latitude IS NULL OR dropoff_latitude IS NULL) OR ' + \n",
    "         '(pickup_longitude IS NULL OR dropoff_longitude IS NULL))')\n",
    "\n",
    "# Records with Coordinates out of range\n",
    "c3 = ('((dropoff_longitude NOT BETWEEN :min_long AND :max_long ) OR '+\n",
    "         '(dropoff_latitude NOT BETWEEN :min_lat AND :max_lat)) OR '+\n",
    "      '((pickup_longitude NOT BETWEEN  :min_long AND :max_long ) OR '+\n",
    "         '(pickup_latitude NOT BETWEEN  :min_lat AND :max_lat))')\n",
    "\n",
    "# Records with pickup coordinates equal to dropoff\n",
    "c2 = ('((pickup_latitude = dropoff_latitude) AND ' +\n",
    "         '(pickup_longitude = dropoff_longitude ))')\n",
    "\n",
    "# Records with distance 0\n",
    "c4 = '(trip_distance <= 0.1)'\n",
    "\n",
    "# Records with pickup time equal or greater than dropoff time\n",
    "c5_7_9_10 = '(pickup_datetime >= dropoff_datetime)'\n",
    "\n",
    "# Records with dates in years in the past or in the future\n",
    "c6 = ('((pickup_datetime > :upper OR pickup_datetime < :lower ) OR (dropoff_datetime > :upper OR dropoff_datetime < :lower ))')\n",
    "\n",
    "# Records with zero or null passengers\n",
    "c8 = '(passenger_count < 1 OR passenger_count IS null OR passenger_count > 6)'\n",
    "\n",
    "# null datetimes\n",
    "c9 = '(pickup_datetime IS NULL OR dropoff_datetime IS NULL)'\n",
    "\n",
    "not_where_clause = '(' + c1 + OR + c3 + OR + c2 + OR + c5_7_9_10 + OR + c6+ OR + c8 + OR + c9 +')'\n",
    "# count invalid rows\n",
    "query = ('SELECT count(*) as number FROM trip_record WHERE '+ not_where_clause) # c1 + OR + c3 + OR + c2 + OR + c5_7_9_10 + OR + c6  \n",
    "res = pd.read_sql_query( query, nyc_database, params={\"min_long\":min_long, \"max_long\":max_long, \"min_lat\":min_lat, \"max_lat\":max_lat, \"upper\":upper, \"lower\":lower})\n",
    "invalid = res['number'][0]\n",
    "print('Number of invalid rows: ', invalid)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RthBzGo7i5R",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Find valid records"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GOR3c9qLDY4B",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Records with not null coordinates\n",
    "c1 = ( '((pickup_latitude IS NOT NULL AND dropoff_latitude IS NOT NULL) AND ' + \n",
    "         '(pickup_longitude IS NOT NULL AND dropoff_longitude IS NOT NULL))')\n",
    "\n",
    "# Records with coordinates in the range\n",
    "c3 = ('((dropoff_longitude BETWEEN :min_long AND :max_long ) AND '+\n",
    "         '(dropoff_latitude BETWEEN :min_lat AND :max_lat) AND '+\n",
    "      '(pickup_longitude BETWEEN :min_long AND :max_long ) AND '+\n",
    "         '(pickup_latitude BETWEEN :min_lat AND :max_lat))')\n",
    "\n",
    "# Records with pickup coordinates not equal to dropoff\n",
    "c2 = ('((pickup_latitude != dropoff_latitude) OR ' +\n",
    "         '(pickup_longitude != dropoff_longitude ))')\n",
    "\n",
    "# Records with different coordinates but distance greater 0\n",
    "c4 = '(trip_distance > 0.1)'\n",
    "\n",
    "# Records with pickup time smaller than dropoff time\n",
    "c5_7_9_10 = '(pickup_datetime < dropoff_datetime)'\n",
    "\n",
    "# Records with dates in the right range\n",
    "c6 = ('((pickup_datetime <= :upper AND pickup_datetime >= :lower ) AND (dropoff_datetime <= :upper AND dropoff_datetime >= :lower ))')\n",
    "\n",
    "# Not Zero passengers and passenger count in range\n",
    "c8 = '((passenger_count BETWEEN 1 AND 6) AND passenger_count IS NOT null )'\n",
    "\n",
    "# not null datetimes\n",
    "c9 = '(pickup_datetime IS NOT NULL AND dropoff_datetime IS NOT NULL)'\n",
    "\n",
    "where_clause = '(' + c1 + AND + c3 + AND + c2 + AND + c5_7_9_10 + AND + c6 + AND + c8 + AND + c9 + ')'\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KtsTCIbX7yLf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "605c59da-059a-4929-bdcd-06c4596afbbf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "query = ('SELECT count(*) as number FROM trip_record WHERE '+where_clause ) # c1 + AND + c3 + AND + c2 + AND + c5_7_9_10 + AND + c6\n",
    "res = pd.read_sql_query( query, nyc_database, params={\"min_long\":min_long, \"max_long\":max_long, \"min_lat\":min_lat, \"max_lat\":max_lat, \"upper\":upper, \"lower\":lower})\n",
    "valid = res['number'][0]\n",
    "print('Number of valid rows: ', valid)\n",
    "\n",
    "# to check if number is correct\n",
    "#print( 'Tot: ', valid + invalid)\n",
    "#print( 'Error: ', 14776615 - (valid + invalid))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BM_JfZIb8RJ4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Retrieve valid records"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cm3LXEMZ8Vut",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "query = ('SELECT *  FROM trip_record WHERE '+ where_clause + ' LIMIT 1000') #+ ' LIMIT 1000000'\n",
    "valid_records = pd.read_sql_query( query, nyc_database, params={\"min_long\":min_long, \"max_long\":max_long, \"min_lat\":min_lat, \"max_lat\":max_lat, \"upper\":upper, \"lower\":lower})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WUwdT1tb8rCA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "outputId": "cc44d751-2ca0-404e-abca-ac24f45336ec",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "valid_records.shape "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt4KO2xOIcET",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read the records in the valid area chunk by chunk to avoid memory overflow"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NkTDaOM239aw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "outputId": "48f8498d-4879-43a1-efd4-b32486b469bc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas.io.sql as psql\n",
    "\n",
    "chunk_size = 5000000\n",
    "offset = 0\n",
    "dfs = []\n",
    "header = True\n",
    "while True:\n",
    "  print(offset)\n",
    "  sql = \"SELECT * FROM trip_record WHERE \" + where_clause + \" ORDER BY pickup_longitude LIMIT %d offset %d \" % (chunk_size,offset) \n",
    "  df = pd.read_sql_query( sql, nyc_database, params={\"min_long\":min_long, \"max_long\":max_long, \"min_lat\":min_lat, \"max_lat\":max_lat, \"upper\":upper, \"lower\":lower})\n",
    "  print('Query done')\n",
    "  df.to_csv('./ma_results/january.csv', header=header, mode='a')\n",
    "\n",
    "  \n",
    "\n",
    "  header = False\n",
    "  offset += chunk_size\n",
    "  if len(df) < chunk_size:\n",
    "    break\n",
    "  del df\n",
    "  gc.collect()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b-EczCK62gOS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "103aead7-651c-46e9-ab3f-916c9d7c0444",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "  # Empty space in the memory\n",
    "  del df\n",
    "  gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IKROjuMG1-uj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Save the partial result\n",
    "df = pd.read_csv('./ma_results/january.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldkjKrsJ4pp2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Google Drive version"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oDwMRZL44sXg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!pip install pyshp\n",
    "!pip install haversine\n",
    "!pip install geopandas\n",
    "!pip install pygeos\n",
    "\n",
    "!sudo apt install libspatialindex-dev\n",
    "\n",
    "!pip install Rtree\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "\n",
    "# libraries to visualize data\n",
    "import shapefile\n",
    "from shapely.geometry import Polygon\n",
    "from descartes.patch import PolygonPatch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# compute additional features\n",
    "from haversine import haversine\n",
    "from pandas import DataFrame\n",
    "from numba import njit\n",
    "from math import radians, cos, sin, asin, sqrt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ptuT_FbP5wd9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dtypes = {'Unnamed: 0':'str',\t'index':'str',\t'medallion':'str',\t'hack_license':'str',\t'pickup_datetime':'str',\t'dropoff_datetime':'str',\t'passenger_count':'str',\t'trip_distance':'str',\n",
    "          'pickup_longitude':'str',\t'pickup_latitude':'str',\t'dropoff_longitude':'str',\t'dropoff_latitude':'str',\t'pickup_hour':'str',\t'dropoff_hour':'str',\t'pickup_date':'str',\t'dropoff_date':'str'}\n",
    "usecols=['medallion',\t'hack_license',\t'pickup_datetime',\t'dropoff_datetime',\t'passenger_count',\t'trip_distance',\n",
    "          'pickup_longitude',\t'pickup_latitude',\t'dropoff_longitude',\t'dropoff_latitude',\t'pickup_hour',\t'dropoff_hour',\t'pickup_date',\t'dropoff_date']\n",
    "\n",
    "path = './ma_results'\n",
    "valid_records = pd.read_csv(path + '/january.csv', usecols=usecols)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sQNWLUPyCjr4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "cea77bcb-f70a-48e8-9929-ff302cef9d6a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "valid_records.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xXnGhPG847U",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Elaborate the data\n",
    "Computing extra features to clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZnUmDbh9BnP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d50Smfde9ErS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from haversine import haversine\n",
    "from datetime import datetime\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uiO6Hc5j88Km",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# https://www.kaggle.com/kartikkannapur/nyc-taxi-trips-exploratory-data-analysis\n",
    "# compute haversine distance of a trip\n",
    "# *The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes.*"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPn96fCZ7rab",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Moving to numba to improve performances**\n",
    "\n",
    "https://stackoverflow.com/questions/52673285/performance-of-pandas-apply-vs-np-vectorize-to-create-new-column-from-existing-c"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iIrsrW_c7rEx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from numba import njit\n",
    "\n",
    "# fix problematic times and conver time into hours\n",
    "@njit\n",
    "def refine_time(time_difference):\n",
    "  res = np.empty(time_difference.shape)\n",
    "  for i in range(len(time_difference)):\n",
    "    duration_hours = time_difference[i]/3600.0\n",
    "    if duration_hours < 0.005:\n",
    "      trip_time = 0.005\n",
    "    else:\n",
    "      trip_time = round(duration_hours, 3)\n",
    "    res[i] = trip_time\n",
    "  return res"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1DlJL71y6Pp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "Duration of the trips\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1cwGvIJC_t3Z",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "9dea2615-42f1-4291-f70f-e20026b0bcca",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "valid_records[\"pickup_datetime\"] = pd.to_datetime(valid_records[\"pickup_datetime\"])\n",
    "valid_records[\"dropoff_datetime\"] = pd.to_datetime(valid_records[\"dropoff_datetime\"])\n",
    "valid_records['trip_time'] = (valid_records['dropoff_datetime'] - valid_records['pickup_datetime']).dt.total_seconds()\n",
    "#%timeit valid_records['seconds'] = refine_time(valid_records['seconds'].values)\n",
    "#print(refine_time(valid_records['seconds'].values))\n",
    "valid_records['trip_time'] = refine_time(valid_records['trip_time'].values)\n",
    "valid_records['trip_time']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVUQ97rYawAB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**%timeit in front a that function cause problem, the values returned become all 0.005**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9dKr34tcC8a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "Haversine Distance of the trips\n",
    "\n",
    "https://stackoverflow.com/questions/29545704/fast-haversine-approximation-python-pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3RMbMgJOcCUm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine_np( lat1, lon1, lat2, lon2):\n",
    "    # earth radius\n",
    "    R = 6371.0\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = R * c\n",
    "    return np.round(km, 3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xHq2tOGIahkC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "valid_records['haversine_distance'] = haversine_np(valid_records[\"pickup_latitude\"], valid_records[\"pickup_longitude\"], \n",
    "                     valid_records[\"dropoff_latitude\"], valid_records[\"dropoff_longitude\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OVG13XNKfEJW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "971bb4ab-cae2-4b6c-8b5b-b3cc9bae593f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "valid_records['haversine_distance']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfP1Yi9yfUvd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "Speed of the trips\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rp4Ypx0GfWbM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "b8be5d04-dda9-462b-812e-43abf17c0564",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "valid_records['speed'] = round(valid_records['haversine_distance']/valid_records['trip_time'], 1)\n",
    "valid_records['speed']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaUoDXQH8nma",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statistics on valid records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiOAYiPoRHX8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Number of passengers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BShHEIiNRKAN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "outputId": "57eb8a98-7bab-4779-d200-3f9d7ce5841b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "mean_p = valid_records[\"passenger_count\"].mean()\n",
    "max_p = valid_records[\"passenger_count\"].max()\n",
    "min_p = valid_records[\"passenger_count\"].min()\n",
    "\n",
    "print(\"The mean number of passengers is: \", mean_p)\n",
    "print(\"The max number of passengers is: \", max_p)\n",
    "print(\"The min number of passengers is: \", min_p)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mO9GhIX-eOs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute mean, min and max ***trip distance***\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JeIA6d8_-iSr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "outputId": "be9ca9b9-9b7d-4dfe-d646-325eeede1f38",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "mean_d = valid_records[\"haversine_distance\"].mean()\n",
    "max_d = valid_records[\"haversine_distance\"].max()\n",
    "min_d = valid_records[\"haversine_distance\"].min()\n",
    "\n",
    "print(\"The mean distance is: \", mean_d)\n",
    "print(\"The max distance is: \", max_d)\n",
    "print(\"The min distance is: \", min_d)\n",
    "print(\"The distances are represented in Km unit\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeFa_8ib-lsh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute mean, min and max ***trip time***\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1VXsLUPK-lsi",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "outputId": "3ea61c60-ca7f-4f81-9aa0-3483081af533",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "mean_t = valid_records[\"trip_time\"].mean()\n",
    "max_t = valid_records[\"trip_time\"].max()\n",
    "min_t = valid_records[\"trip_time\"].min()\n",
    "\n",
    "print(\"The mean trip_time is: \", mean_t)\n",
    "print(\"The max trip_time is: \", max_t)\n",
    "print(\"The min trip_time is: \", min_t)\n",
    "print(\"The trip_times are represented in hours\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Fwxl63f-r1o",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute mean, min and max ***speed***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ckCs1sSo-r1p",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "outputId": "e661ec9a-bf42-4195-9781-b2e339431870",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "mean_speed = valid_records[\"speed\"].mean()\n",
    "max_speed = valid_records[\"speed\"].max()\n",
    "min_speed = valid_records[\"speed\"].min()\n",
    "\n",
    "print(\"The mean speed is: \", mean_speed)\n",
    "print(\"The max speed is: \", max_speed)\n",
    "print(\"The min speed is: \", min_speed)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VL2WX5MsFwG0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "aefc3fcb-7ccd-461d-e8aa-1c091485c9c6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# because haversine distance is the semicircle on  the sphere which connect 2 points, in reality the distance between A and B is greater, \n",
    "# so the real speed is higher than the one computed with haversine\n",
    "\n",
    "print(\"records: \", len(valid_records.medallion))\n",
    "valid_records.haversine_distance.hist(bins=30, figsize=(10,5), color = 'blue')\n",
    "plt.xlabel('Haversine ditance')\n",
    "plt.title('Histogram');\n",
    "plt.show() \n",
    "\n",
    "valid_records.trip_time[valid_records.trip_time<8].hist(bins=30, figsize=(10,5), color = 'red')\n",
    "plt.xlabel('Trip time')\n",
    "plt.title('Histogram');\n",
    "plt.show() \n",
    "\n",
    "valid_records.speed[valid_records.speed<100].hist(bins=20, figsize=(8,5), color = 'green')\n",
    "plt.xlabel('Speed')\n",
    "plt.title('Histogram');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0y3PXv_HIKtG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statistics to find correct speed, time and distance ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0z5yCbhJoaY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Time\n",
    "distance\n",
    "speed\n",
    "\n",
    "https://medium.com/datadriveninvestor/finding-outliers-in-dataset-using-python-efc3fce6ce32\n",
    "\n",
    "\n",
    "step 1:\n",
    "* Arrange the data in increasing order\n",
    "* Calculate first(q1) and third quartile(q3)\n",
    "* Find interquartile range (q3-q1)\n",
    "* Find lower bound q1*1.5\n",
    "* Find upper bound q3*1.5\n",
    "* Anything that lies outside of lower and upper bound is an outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jZbq6DGXIRb_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "42f12cd8-aa36-455c-a6c8-b8087e96a76f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from scipy import stats\n",
    "# Haversine distance\n",
    "distances = np.array(valid_records.sort_values(by=['haversine_distance']).haversine_distance)\n",
    "q1, q3= np.percentile(distances,[25,75])\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - (1.5 * iqr) \n",
    "upper_bound = q3 + (1.5 * iqr) \n",
    "print(\"IQR\")\n",
    "print(\"Lower bound: {}, Upper bound: {}\".format(lower_bound,upper_bound))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWC_5RD9VrR5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Try with the Z score"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B5jDUQDqWx3m",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "487c8de7-83bb-4289-bf61-70a4a5bb7a86",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#distances = np.array(valid_records.sort_values(by=['haversine_distance']).haversine_distance)\n",
    "z = np.abs(stats.zscore(distances))\n",
    "threshold = 3\n",
    "idx = np.where(z > threshold)\n",
    "lower_bound = distances[idx].min()\n",
    "upper_bound = distances[idx].max()\n",
    "print(\"Z-score\")\n",
    "print(\"Upper bound from: {}, to: {}\".format(lower_bound,upper_bound))\n",
    "\n",
    "idx = np.where(z < -threshold)\n",
    "if(len(idx[0]) >0):\n",
    "  distances[idx].max()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Anf7p437dPQ_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Trip time"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kkgeS6zhdSTK",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "outputId": "877ade52-9a8d-4ea2-e0bb-29f8cb8a9d6e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "times = np.array(valid_records.sort_values(by=['trip_time']).trip_time)\n",
    "q1, q3= np.percentile(times,[25,75])\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 -(1.5 * iqr) \n",
    "upper_bound = q3 +(1.5 * iqr) \n",
    "print(\"IQR\")\n",
    "print(\"Lower bound: {}, Upper bound: {}\".format(lower_bound,upper_bound))\n",
    "print(\"\")\n",
    "# Z score\n",
    "z = np.abs(stats.zscore(times))\n",
    "threshold = 3\n",
    "idx = np.where(z > threshold)\n",
    "lower_bound = times[idx].min()\n",
    "upper_bound = times[idx].max()\n",
    "print(\"Z-score\")\n",
    "print(\"Upper bound from: {}, to: {}\".format(lower_bound,upper_bound))\n",
    "\n",
    "idx = np.where(z < -threshold)\n",
    "if(len(idx[0]) >0):\n",
    "  times[idx].max()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLFBWIIL_79o",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at the trips with triptime greater than 22 hours they seems weird. The number of these records is very low so we can remove them"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1QR2NSD6_GfU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "outputId": "4cd880bf-e0b5-4be4-946e-1f568b8283c0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "valid_records.loc[valid_records['trip_time']>22.0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lih6jcItDtIM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Trips longer than 4 hourse"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qgi0JqVpAZI5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "cb8a8fb0-59f8-407c-a234-8497586fcb58",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "valid_records.loc[valid_records['trip_time']>4.0].sort_values(by=['trip_time'], ascending=False).shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_p0kQGRyMJ8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Speed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IYkSNuhmyL4k",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "outputId": "5d2e0377-fb3c-43f9-ebd4-8b4483b1de2b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "speeds = np.array(valid_records.sort_values(by=['speed']).speed)\n",
    "q1, q3= np.percentile(speeds,[25,75])\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 -(1.5 * iqr) \n",
    "upper_bound = q3 +(1.5 * iqr) \n",
    "print(\"IQR\")\n",
    "print(\"Lower bound: {}, Upper bound: {}\".format(lower_bound,upper_bound))\n",
    "print(\"\")\n",
    "# Z score\n",
    "z = np.abs(stats.zscore(speeds))\n",
    "threshold = 3\n",
    "idx = np.where(z > threshold)\n",
    "lower_bound = speeds[idx].min()\n",
    "upper_bound = speeds[idx].max()\n",
    "print(\"Z-score\")\n",
    "print(\"Upper bound from: {}, to: {}\".format(lower_bound,upper_bound))\n",
    "\n",
    "idx = np.where(z < -threshold)\n",
    "if(len(idx[0]) >0):\n",
    "  speeds[idx].max()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P3Bz5ybCP8n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX9PhPsICTit",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Is it not possible to relay just on this statistical measures, considering the context i will use these treesholds:\n",
    "\n",
    "**Haversine Distance:**\n",
    "*   Lower bound: 0.1 km\n",
    "*   Upper bound: 70 km\n",
    "\n",
    "---\n",
    "**Trip time:**\n",
    "With our location granularity a movement of less than 6 minutes probably would not be enough to change zone\n",
    "\n",
    "4 hours, looking at the data seems the extreme case\n",
    "*   Lower bound: 0.1 h\n",
    "*   Upper bound: 4.0 h\n",
    "\n",
    "---\n",
    "From: https://www.kaggle.com/headsortails/nyc-taxi-eda-update-the-fast-the-curious#direct-distance-of-the-trip\n",
    "\n",
    "*Well, after removing the most extreme values this looks way better than I would have expected. An average speed of around 15 km/h sounds probably reasonable for NYC. Everything above 50 km/h certainly requires magical cars (or highway travel). Also keep in mind that this refers to the direct distance and that the real velocity would have been always higher.*\n",
    "\n",
    "*In a similar way as the average duration per day and hour we can also investigate the average speed for these time bins:*\n",
    "\n",
    "https://www.latimes.com/nation/la-na-new-york-traffic-manhattan-20180124-story.html\n",
    "\n",
    "**Speed:**\n",
    "*   Lower bound: 5 km/h\n",
    "*   Upper bound: 60 km/h\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X5ma4VtcIECC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Treesholds\n",
    "haversine_distance_t = (0.1, 70)\n",
    "trip_time_t = (0.1, 4.0)\n",
    "speed_t = (5, 60)\n",
    "\n",
    "# define more restrictive thresholds for the intermediate trips\n",
    "intermediate_distance_t = (0.5, 20)\n",
    "intermediate_trip_time_t = (0.1, 1.0)\n",
    "intermediate_speed_t = (5, 40)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9M6nRTRtnnp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remove outliers before intermediate trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdbtkfBItnnt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove trips with unreal speed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-YCi4sC1tnnu",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "outputId": "d4d4fef7-3b7b-47fb-8c02-8d56b05ec9d5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# speed\n",
    "valid_records = valid_records.loc[(valid_records.speed >= speed_t[0]) & \n",
    "                                          (valid_records.speed <= speed_t[1])]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(valid_records[\"speed\"])), np.sort(valid_records[\"speed\"].values), color = 'green')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('speed in km/h')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx2S6Hhstnnx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove trips with unreal haversine distance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tFNkMWtBtnnx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "outputId": "2db85380-f86d-4a5d-d155-84d9cd52b8ce",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# haversine_distance\n",
    "valid_records = valid_records.loc[(valid_records.haversine_distance >= haversine_distance_t[0]) & \n",
    "                                          (valid_records.haversine_distance <= haversine_distance_t[1])]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(valid_records[\"haversine_distance\"])), np.sort(valid_records[\"haversine_distance\"]), color = 'blue')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('haversine_distance in km')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL6ZDTE7tnn0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove trips with unreal trip time"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FLjD7-Cwtnn0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "outputId": "00bb2cf1-c699-4256-896c-3de34d4df678",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# trip_time\n",
    "valid_records = valid_records.loc[(valid_records.trip_time >= trip_time_t[0]) & \n",
    "                                          (valid_records.trip_time <= trip_time_t[1])]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(valid_records[\"trip_time\"])), np.sort(valid_records[\"trip_time\"]), color = 'red')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('trip_time in km')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5-EL13dJkpA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Final amount of valid records"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HBfSAFSAuYel",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "valid_records.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAQiWiQuAm5m",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Add intermediate trips"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tS8mb6kyplxf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Treesholds\n",
    "haversine_distance_t = (0.1, 70)\n",
    "trip_time_t = (0.1, 4.0)\n",
    "speed_t = (5, 60)\n",
    "\n",
    "# define more restrictive thresholds for the intermediate trips\n",
    "intermediate_distance_t = (0.5, 20)\n",
    "intermediate_trip_time_t = (0.1, 1.0)\n",
    "intermediate_speed_t = (5, 40)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qi4Rmp5gt1gS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "b46ca132-7f74-4a2e-aff5-17afcb67e144",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Retrieve the valid trips\n",
    "query = ('SELECT medallion, hack_license, pickup_datetime, dropoff_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude '+\n",
    "         'FROM trip_record WHERE '+ where_clause + ' ORDER BY medallion, pickup_datetime') #LIMIT 100000\n",
    "t1 = datetime.now()\n",
    "valid_records = pd.read_sql_query( query, nyc_database, params={\"min_long\":min_long, \"max_long\":max_long, \"min_lat\":min_lat, \"max_lat\":max_lat, \"upper\":upper, \"lower\":lower})\n",
    "t2 = datetime.now()\n",
    "delta = t2 - t1\n",
    "print(\"time for query: {}\".format(delta.seconds))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taVDrIJyCovM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**To iterate across pairs of rows we need to make a copy of our dataframe and shif it by -1, so the line 1 will be concatenate with the line 2 and so on.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kae9eadD4hLB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Convert the datetimes from strings to datetimes\n",
    "valid_records[\"pickup_datetime\"] = pd.to_datetime(valid_records[\"pickup_datetime\"])\n",
    "valid_records[\"dropoff_datetime\"] = pd.to_datetime(valid_records[\"dropoff_datetime\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nSc1dXWl22NF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# sort the records\n",
    "valid_records.sort_values(by=['medallion', 'pickup_datetime'], inplace=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yFw0T8ztCtJA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "outputId": "b735cd52-559f-4348-e0ba-e8d563092800",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Rename the columns for the dataframe copy\n",
    "print(\"Renaming the new columns...\")\n",
    "new_columns_name = np.apply_along_axis(lambda column: column+'_b', 0, np.array(valid_records.columns))\n",
    "\n",
    "# Copy the dataframe and use the new columns names (takes some time..)\n",
    "print(\"Copying the dataframe...\")\n",
    "#valid_records_copy = pd.DataFrame(data=valid_records.values, columns=new_columns_name)\n",
    "valid_records_copy  = valid_records.copy()\n",
    "valid_records_copy.columns = new_columns_name\n",
    "\n",
    "# Shift the copy dataframe\n",
    "print(\"Shifting the copied dataframe...\")\n",
    "valid_records_copy = valid_records_copy.shift(-1)\n",
    "\n",
    "# concatenate the dataframes \n",
    "print(\"Concatenating the two dataframes...\")\n",
    "concatenated_dataframes = pd.concat([valid_records, valid_records_copy], axis=1, sort=False).dropna()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_XT9txAo42R2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "outputId": "8d580b4e-46f6-432a-95f7-113ea6e5dfbe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "concatenated_dataframes.dtypes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8haxzr2hCxVr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To generate the intermediate trips we need a function to apply on each row.\n",
    "\n",
    "This function will make some checks and then return the new computed row.\n",
    "\n",
    "If an intermediate trip is not possibile, the function returns a None row.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LZlyUM0Dhsgd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "863087ca-5d5e-4b2d-a87f-ea098e3a1659",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Min time allowed between two trips and min intermediate trip length\n",
    "min_time = intermediate_trip_time_t[0]\n",
    "# Max time allowed between two trips and max intermediate trip length\n",
    "max_time = intermediate_trip_time_t[1]\n",
    "\n",
    "# Min distance allowed between two trips and min intermediate trip distance\n",
    "min_distance = intermediate_distance_t[0]\n",
    "# Max distance allowed between two trips and max intermediate trip distance\n",
    "max_distance = intermediate_distance_t[1]\n",
    "\n",
    "# Min speed allowed between two trips and min intermediate trip speed\n",
    "min_speed = intermediate_speed_t[0]\n",
    "# Max speed allowed between two trips and max intermediate trip speed\n",
    "max_speed = intermediate_speed_t[1]\n",
    "\n",
    "# we need to reduce the number of rows first\n",
    "concatenated_dataframes['intermediate_time'] = (concatenated_dataframes['pickup_datetime_b'] - \n",
    "                                                concatenated_dataframes['dropoff_datetime']).dt.total_seconds()\n",
    "concatenated_dataframes['intermediate_time'] = refine_time(concatenated_dataframes['intermediate_time'].values)\n",
    "\n",
    "concatenated_dataframes['intermediate_distance'] = haversine_np(concatenated_dataframes[\"pickup_latitude_b\"].values, concatenated_dataframes[\"pickup_longitude_b\"].values, \n",
    "                     concatenated_dataframes[\"dropoff_latitude\"].values, concatenated_dataframes[\"dropoff_longitude\"].values)\n",
    "concatenated_dataframes['intermediate_speed'] = round(concatenated_dataframes['intermediate_distance']/concatenated_dataframes['intermediate_time'], 1)\n",
    "\n",
    "candidates = concatenated_dataframes.loc[((concatenated_dataframes['medallion'] == concatenated_dataframes['medallion_b']) & (concatenated_dataframes['hack_license'] == concatenated_dataframes['hack_license_b'])) &\n",
    "                                         ((concatenated_dataframes['intermediate_time'] <= max_time) & (concatenated_dataframes['intermediate_time'] >= min_time)) & \n",
    "                                         ((concatenated_dataframes['intermediate_distance'] <= max_distance) & (concatenated_dataframes['intermediate_distance'] >= min_distance)) &\n",
    "                                         ((concatenated_dataframes['intermediate_speed'] <= max_speed) & (concatenated_dataframes['intermediate_speed'] >= min_speed))]\n",
    "candidates.shape\n",
    "                              "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C8tOVSRIp7rU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Change the columns names\n",
    "intermediate_trips = candidates[['medallion', 'hack_license', 'dropoff_datetime', 'pickup_datetime_b', 'dropoff_longitude', 'dropoff_latitude',\n",
    "                                 'pickup_longitude_b','pickup_latitude_b', 'intermediate_distance', 'intermediate_time','intermediate_speed']]\n",
    "\n",
    "intermediate_trips = intermediate_trips.rename(columns={'medallion':'medallion', 'hack_license':'hack_license', 'dropoff_datetime':'pickup_datetime', 'pickup_datetime_b':'dropoff_datetime', \n",
    "                                   'pickup_longitude_b':'dropoff_longitude',\n",
    "                                   'pickup_latitude_b':'dropoff_latitude', \n",
    "                                    'dropoff_longitude':'pickup_longitude',\n",
    "                                   'dropoff_latitude':'pickup_latitude', \n",
    "                                   'intermediate_distance': 'haversine_distance',\n",
    "                                   'intermediate_time':'trip_time', 'intermediate_speed':'speed'})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AhWIeVxn8oki",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "outputId": "1dbb6d3e-f606-47da-d136-79b160aa83a2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "intermediate_trips"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DNP80tm2NW0k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# compute extra features now, before now it would have just added complexity to the big operations we made to compute the intermediate trips\n",
    "valid_records['trip_time'] = (valid_records['dropoff_datetime'] - valid_records['pickup_datetime']).dt.total_seconds()\n",
    "valid_records['trip_time'] = refine_time(valid_records['trip_time'].values)\n",
    "valid_records['haversine_distance'] = haversine_np(valid_records[\"pickup_latitude\"], valid_records[\"pickup_longitude\"], \n",
    "                     valid_records[\"dropoff_latitude\"], valid_records[\"dropoff_longitude\"])\n",
    "valid_records['speed'] = round(valid_records['haversine_distance']/valid_records['trip_time'], 1)\n",
    "\n",
    "# Union of the original valid trips and the new intermediate trips\n",
    "all_valid_records = pd.concat([valid_records, intermediate_trips], axis=0)\n",
    "\n",
    "# Sort the records by medallion and pickup_time\n",
    "all_valid_records = all_valid_records.sort_values(by=['medallion', 'pickup_datetime'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iZInb6emEsy3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "e1126408-7397-44d8-ad17-797829772bf0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records.head(20)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dLGZqWsHsaOC",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "dc6777d5-a05c-49ae-ed8d-e961861a4d41",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbGL4H46Fr4m",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statistics on full dataset\n",
    "*From this analysis we can see that there are few outlier considering the values of distance, time and speed*\n",
    "\n",
    "*We should remove them*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXiYUurFb6W-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First and Last date\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cbcYca33b6XA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "outputId": "777df3d4-3833-410a-d0ef-d24afa96d4f2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# pickup\n",
    "max_pickup_date = all_valid_records[\"pickup_datetime\"].max()\n",
    "min_pickup_date= all_valid_records[\"pickup_datetime\"].min()\n",
    "\n",
    "\n",
    "# dropoff\n",
    "max_dropoff_date = all_valid_records[\"dropoff_datetime\"].max()\n",
    "min_dropoff_date= all_valid_records[\"dropoff_datetime\"].min()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Pickup: \")\n",
    "print(\"Min Date: \", min_pickup_date)\n",
    "print(\"Max Date: \", max_pickup_date)\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Dropoff: \")\n",
    "print(\"Min Date: \", min_dropoff_date)\n",
    "print(\"Max Date: \", max_dropoff_date)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWL1y_J2aCKP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Min and Max **Longitude** and **Latitude** for Pickup and Dropoff\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V_q4PHa7aFGc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "outputId": "290f29f7-7ac9-4f9a-ee64-035678bac609",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "# pickup\n",
    "max_pickup_longitude = all_valid_records[\"pickup_longitude\"].max()\n",
    "min_pickup_longitude= all_valid_records[\"pickup_longitude\"].min()\n",
    "max_pickup_latitude = all_valid_records[\"pickup_latitude\"].max()\n",
    "min_pickup_latitude= all_valid_records[\"pickup_latitude\"].min()\n",
    "\n",
    "# dropoff\n",
    "max_dropoff_latitude = all_valid_records[\"dropoff_latitude\"].max()\n",
    "min_dropoff_latitude= all_valid_records[\"dropoff_latitude\"].min()\n",
    "max_dropoff_longitude = all_valid_records[\"dropoff_longitude\"].max()\n",
    "min_dropoff_longitude= all_valid_records[\"dropoff_longitude\"].min()\n",
    "\n",
    "\n",
    "print(\"Pickup: \")\n",
    "print(\"Min Latitude: \", min_pickup_latitude)\n",
    "print(\"Min Longitude: \", min_pickup_longitude)\n",
    "print(\"Max Latitude: \", max_pickup_latitude)\n",
    "print(\"Max Longitude: \", max_pickup_longitude)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Dropoff: \")\n",
    "print(\"Min Latitude: \", min_dropoff_latitude)\n",
    "print(\"Min Longitude: \", min_dropoff_longitude)\n",
    "print(\"Max Latitude: \", max_dropoff_latitude)\n",
    "print(\"Max Longitude: \", max_dropoff_longitude)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkHTMHF6dVul",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Most common medallion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YPTHupZqdZ4Z",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "outputId": "be727b8b-c677-4704-db82-ba388cfaad8f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records['medallion'].value_counts()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez2uY1aeH13X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Most common Medallion - Hack_license combination"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aZ_Qn_xiICEB",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "outputId": "4bfa3c78-1604-45be-e50a-c63f3b844f70",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records.groupby(['medallion','hack_license']).size().sort_values(ascending=False)\n",
    "#counts.index[0] "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdOPilYwFr4o",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute mean, min and max ***trip distance***\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wRbWCdwMFr4o",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "outputId": "f5bc6422-9879-4d4d-cef7-d0023970522e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "mean_d = all_valid_records[\"haversine_distance\"].mean()\n",
    "max_d = all_valid_records[\"haversine_distance\"].max()\n",
    "min_d = all_valid_records[\"haversine_distance\"].min()\n",
    "\n",
    "print(\"The mean distance is: \", mean_d)\n",
    "print(\"The max distance is: \", max_d)\n",
    "print(\"The min distance is: \", min_d)\n",
    "print(\"The distances are represented in Km unit\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AaFYrIZMGouj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "outputId": "b0aeff07-e4c6-4d4b-fe0b-f29abcec28d1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# plot values to spot outliers\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(all_valid_records[\"haversine_distance\"])), np.sort(all_valid_records[\"haversine_distance\"]), color = 'red')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('haversine_distance in km')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAek8SYHFr4r",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute mean, min and max ***trip time***\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CdrC1NozFr4r",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "outputId": "5b745b86-f1cc-49fb-a48f-7e51a4312ed3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "mean_t = all_valid_records[\"trip_time\"].mean()\n",
    "max_t = all_valid_records[\"trip_time\"].max()\n",
    "min_t = all_valid_records[\"trip_time\"].min()\n",
    "\n",
    "print(\"The mean trip_time is: \", mean_t)\n",
    "print(\"The max trip_time is: \", max_t)\n",
    "print(\"The min trip_time is: \", min_t)\n",
    "print(\"The trip_times are represented in hours\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d0VlnE1CI9OX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "outputId": "5fe9e800-657a-4d70-ca58-053e7d82b8ff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Row with the max time\n",
    "index = all_valid_records[\"trip_time\"].argmax()\n",
    "all_valid_records.iloc[index]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8Z2nR3U-F7ur",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "outputId": "2a718fd5-ff38-4fd4-a1b9-f9ef34d60077",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# plot values to spot outliers\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(all_valid_records[\"trip_time\"])), np.sort(all_valid_records[\"trip_time\"]),color = 'blue')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('trip_time in hours')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jKXypPuFr4u",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute mean, min and max ***speed***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4HQVwwk1Fr4u",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "outputId": "64667383-507a-44b9-af06-39cc3c6abc5a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "mean_speed = all_valid_records[\"speed\"].mean()\n",
    "max_speed = all_valid_records[\"speed\"].max()\n",
    "min_speed = all_valid_records[\"speed\"].min()\n",
    "\n",
    "print(\"The mean speed is: \", mean_speed)\n",
    "print(\"The max speed is: \", max_speed)\n",
    "print(\"The min speed is: \", min_speed)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "og2v5WJMGf2X",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "outputId": "61a355ae-f814-45b8-95ab-6c660b7090d1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# plot values to spot outliers\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(all_valid_records[\"speed\"])), np.sort(all_valid_records[\"speed\"]), color = 'green')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('speed in km/h')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9xXMfnyHGxb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Worth notice\n",
    "\n",
    "From this analysis we can see that there are few outlier considering the values of distance, time and speed\n",
    "\n",
    "We should remove them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So1tZSjCHa_T",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knMCBfpGHoZN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove trips with unreal speed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hGY25wfSHdc_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "outputId": "ba50fff2-95ab-4b67-cc44-949408adec32",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# speed\n",
    "all_valid_records = all_valid_records.loc[(all_valid_records.speed >= speed_t[0]) & \n",
    "                                          (all_valid_records.speed <= speed_t[1])]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(all_valid_records[\"speed\"])), np.sort(all_valid_records[\"speed\"].values), color = 'green')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('speed in km/h')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNB7ayD5I0ik",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove trips with unreal haversine distance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yyO2vTehI4Dn",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "outputId": "98ec3262-9708-4c68-978a-bc5e27b46d33",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# haversine_distance\n",
    "all_valid_records = all_valid_records.loc[(all_valid_records.haversine_distance >= haversine_distance_t[0]) & \n",
    "                                          (all_valid_records.haversine_distance <= haversine_distance_t[1])]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(all_valid_records[\"haversine_distance\"])), np.sort(all_valid_records[\"haversine_distance\"]), color = 'blue')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('haversine_distance in km')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2X8r5gfJZ9P",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove trips with unreal trip time"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TiS7OKwqJd-o",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "outputId": "ba49614d-7db7-4dbe-fe00-029f3b3ce064",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# trip_time\n",
    "all_valid_records = all_valid_records.loc[(all_valid_records.trip_time >= trip_time_t[0]) & \n",
    "                                          (all_valid_records.trip_time <= trip_time_t[1])]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(range(len(all_valid_records[\"trip_time\"])), np.sort(all_valid_records[\"trip_time\"]), color = 'red')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('trip_time in km')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kZeSkjJ3uAJy",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "39ee8e24-06ea-485d-a9db-031ab815713a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wohxHfkYNzRN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSLXcmSGN25I",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have datetimes which are useful for the analysis, but for our NN we need to refactor these attributes:\n",
    "\n",
    "*  week_day\n",
    "*  hour\n",
    "*  month"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yEqZ_Vr5xEV2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# already did it\n",
    "all_valid_records[\"pickup_datetime\"] = pd.to_datetime(all_valid_records[\"pickup_datetime\"])\n",
    "all_valid_records[\"dropoff_datetime\"] = pd.to_datetime(all_valid_records[\"dropoff_datetime\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pun_cI8VwhyE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records[\"pickup_week_day\"] = all_valid_records.pickup_datetime.dt.dayofweek\n",
    "all_valid_records[\"pickup_hour\"] = all_valid_records.pickup_datetime.dt.hour\n",
    "all_valid_records[\"pickup_month\"] = all_valid_records.pickup_datetime.dt.month\n",
    "all_valid_records[\"pickup_day\"] = all_valid_records.pickup_datetime.dt.day\n",
    "\n",
    "all_valid_records[\"dropoff_week_day\"] = all_valid_records.dropoff_datetime.dt.dayofweek\n",
    "all_valid_records[\"dropoff_hour\"] = all_valid_records.dropoff_datetime.dt.hour\n",
    "all_valid_records[\"dropoff_month\"] = all_valid_records.dropoff_datetime.dt.month\n",
    "all_valid_records[\"dropoff_day\"] = all_valid_records.dropoff_datetime.dt.day"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wQVZoFuoxKuj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "outputId": "a9b73d5b-c3cb-4278-ccd0-f328909a9b30",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gQCuP8O28Nqs",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "06655676-eb2d-49a8-8738-d7bf2b42db88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import gc \n",
    "\n",
    "del valid_records\n",
    "del valid_records_copy\n",
    "del intermediate_trips\n",
    "del concatenated_dataframes\n",
    "del candidates\n",
    "\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uxE534rZ5MwY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records.to_csv('./ma_results/january_before_zones.csv', index = False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zsPJIuGxB0Go",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records = pd.read_csv('./ma_results/january_before_zones.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLUEgiFHR0ML",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute time differences"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bTF2Zz2vR8w3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "records = all_valid_records[['medallion', 'pickup_datetime', 'dropoff_datetime']].copy()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fGpFBvRTUSjO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "records.pickup_datetime = pd.to_datetime(records.pickup_datetime)\n",
    "records.dropoff_datetime = pd.to_datetime(records.dropoff_datetime)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XenCHw0zR1DD",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "71a36706-750b-4e46-9cc2-871cc5e31afb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "count = records.medallion.value_counts()\n",
    "count"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-jsCxF6RPQC5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "medallions = count.loc[count>1000].index"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EKn4MJLuNvmt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "records = records.loc[records.medallion.isin(medallions)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mucu4sRWJ3i2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Datetimes to timestamps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F_dzfNHfN7jI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "records['pickup_ts'] = records.pickup_datetime.values.astype(np.int64) // 10 ** 9"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EbfhO_D9Ofi1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "records['dropoff_ts'] = records.dropoff_datetime.values.astype(np.int64) // 10 ** 9"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BQJHo0aRR24a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import datetime \n",
    "# Count the records for each user\n",
    "# count = records.medallion.value_counts()\n",
    "\n",
    "# Select the users with a number of records greater than the median\n",
    "# records = records[records['medallion'].isin(count.index[count > 30])]\n",
    "\n",
    "# List to collect differences\n",
    "time_diff = []\n",
    "\n",
    "#sf.sort_values(by=['medallion', 'pickup_ts'], inplace=True)\n",
    "\n",
    "# For each user\n",
    "for n, medallion in enumerate(medallions):\n",
    "\n",
    "  sf = records.loc[records.medallion == medallion].copy()\n",
    "  \n",
    "  # Get the timestamps of the observations\n",
    "  pickup_time = sf.pickup_ts.values\n",
    "\n",
    "  # Get the timestamps of the observations\n",
    "  dropoff_time = sf.dropoff_ts.values\n",
    "\n",
    "  # Create a temporary array\n",
    "  copy = np.zeros(pickup_time.shape[0], dtype=np.int32)\n",
    "\n",
    "  # Copy the end timestamps shifted right by 1\n",
    "  copy[1:] = pickup_time[:pickup_time.shape[0]-1]\n",
    "\n",
    "  # compute the time difference\n",
    "  diff = dropoff_time - copy\n",
    "  diff[0] = 0\n",
    "\n",
    "  # Cast from seconds to hours\n",
    "  diff = diff /3600\n",
    "\n",
    "  time_diff.extend(list(diff))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7_3m-P_6mVYX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Save the numpy\n",
    "np.save('./ma_results/NYC_time_diff.npy', np.array(time_diff))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1b3yVEpDQbvA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "outputId": "4aa2034e-13a9-44b9-91e5-3a0798518485",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_bins = 50\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# plot the cumulative histogram\n",
    "n, bins, patches = ax.hist(time_diff[:1000], n_bins, density=True, histtype='stepfilled',\n",
    "                           cumulative=True, label='Empirical',  color = \"skyblue\", ec=\"skyblue\")\n",
    "\n",
    "\n",
    "# tidy up the figure\n",
    "ax.legend(loc='right')\n",
    "ax.set_title('Cumulative step histograms')\n",
    "ax.set_xlabel('Time interval')\n",
    "ax.set_ylabel('Likelihood of occurrence')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8wU1CYx0Rqhn",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "outputId": "4d75b08d-74d4-4f99-8d0b-f0a2df2f7e4e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# we can't plot time differences of 4000h... we need to remove them just for have a clean plot\n",
    "\n",
    "time_diff_c = np.array(time_diff)[np.array(time_diff) < 150]\n",
    "n_bins = 10\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "# plot the cumulative histogram\n",
    "n, bins, patches = ax.hist(time_diff_c, n_bins, density=True, histtype='stepfilled',\n",
    "                           cumulative=False, label='Empirical',  color = \"skyblue\", ec=\"skyblue\")\n",
    "\n",
    "\n",
    "# tidy up the figure\n",
    "ax.legend(loc='right')\n",
    "ax.set_title('Cumulative step histograms')\n",
    "ax.set_xlabel('Time interval')\n",
    "ax.set_ylabel('Likelihood of occurrence')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5eux1pLqRqh3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "outputId": "54f974cf-e12a-45ac-defc-d12d80016650",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(\"Number of places with a time difference over 80 hours: \", np.sum(np.array(time_diff) > 80))\n",
    "print(\"Number of places with a time difference less than 80 hours: \", np.sum(np.array(time_diff) <= 80))\n",
    "print(\"Number of places with a time difference less than 20 hours: \", np.sum(np.array(time_diff) <= 20))\n",
    "print(\"Number of places with a time difference less than 2 hours: \", np.sum(np.array(time_diff) <= 2))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P9D8lkqOQbvE",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "f6b47e97-9104-426d-d390-e808d48f6abd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "diff_t = np.array(time_diff)\n",
    "print(\"Mean time difference: \", np.mean(diff_t))\n",
    "print(\"Max time difference:\", np.max(diff_t))\n",
    "print(\"Min time difference: \", np.min(diff_t))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4VPhByfNLzD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Join location ID\n",
    "\n",
    "To reduce at the minimum size the dimension of the dataset to upload on Google BigQuery, we can limited the columns to:\n",
    "\n",
    "\n",
    "* **pickup_longitude**\n",
    "* **pickup_latitude**\n",
    "* **dropoff_longitude**\n",
    "* **dropoff_latitude**\n",
    "\n",
    "Then joining with the zones we can simply add the ZoneID, \n",
    "We will be able to match it with our local dataset.\n",
    "\n",
    "The valid rows which are in the valid area but now within the NYC boudaries will be mapped to a Zone which represent the outside of the city\n",
    "\n",
    "The other ones that are in the NYC boudary will be used in bigquery\n",
    "\n",
    "from BigQuery we will get back these attributes:\n",
    "\n",
    "[ pickup_longitude, pickup_latitude, dropoff_longitude dropoff_latitude, zone_id, zone_name, borough ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GJRB-5-U0U4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "in all_valid_records we have some problem of duplicated index (cause from the intermediate trips?)\n",
    "\n",
    "we need to reindex"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MaQTBnlEU8m_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records.reset_index(drop=True, inplace=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLmYS1Elb_AA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Take the columns we are interested for"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BKBl6OU0yupw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "locations = all_valid_records[[\"pickup_longitude\", \"pickup_latitude\",\"dropoff_longitude\", \"dropoff_latitude\"]].copy()\n",
    "locations.head(100)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JcG1yEecepw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the location data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vzyFDaWSzjNW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# save the csv\n",
    "locations.to_csv('./ma_results/locations.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hp4N6wRIcg9Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove coordinates which are outside of NYC\n",
    "\n",
    "**We don't need this, coordinates not in the zones are simply discarded in the spatial join**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V_qbp3Xj3Oo1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "locations_nyc = locations"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7T4tmWUFIaG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read the taxi zone dataset shapefile"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7pIgu5HhXvon",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "outputId": "03eaec53-c9c7-4f11-c1f3-b1cd325a775c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import geopandas as gpd\n",
    "from geopandas import GeoSeries\n",
    "from shapely.geometry import Point, LineString, Polygon, asMultiPoint\n",
    "import fiona.crs\n",
    "\n",
    "fp = \"./zones/geo_export_be42d9eb-5829-406d-a057-01bd027a191b.shp\"\n",
    "\n",
    "# Read the data\n",
    "zones = gpd.read_file(fp)\n",
    "zones.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-jHP0MzFN7S",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check the dataset crs type"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KTpC8OHbJW6c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "outputId": "b250246d-9918-4eea-a227-d8e89eda1e4e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "zones.crs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f96Hn83FZkY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Drop eventual null rows"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YhU2qs6B9URi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "zones = zones[np.invert(zones.isna())]\n",
    "zones"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC1N5euKc52g",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check for index duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "locations_nyc = pd.read_csv('./ma_results/locations.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2biayLeF_g_G",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47
    },
    "outputId": "c5d4f6b9-e172-40d9-d23c-5f2d780889d4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "locations_nyc[locations_nyc.index.duplicated()]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "merBb7bGFyEC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pickup coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYQFqfnFF3xi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Stack latitude and longitude of each row into a numpy array of N x 2 elements\n",
    "\n",
    "Then generate a list of N Points from that pair of coordinates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yQ8MEFco9b_f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "3177a73a-5449-43a5-84d2-c9f52a13076a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# stack latitude and longitude together \n",
    "pickup = np.vstack((locations_nyc[\"pickup_longitude\"].values, locations_nyc[\"pickup_latitude\"].values)).T \n",
    "\n",
    "# Convert each couple into a point object\n",
    "t1 = datetime.now()\n",
    "points = [Point(i) for i in pickup]\n",
    "t2 = datetime.now()\n",
    "delta = t2 - t1\n",
    "print(\"time for conversion: {}\".format(delta.seconds))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPfT2MYTGtGg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Create a GeoSeries from the list of Points\n",
    "* Change the crs to match with the one of zones\n",
    "* Create a GeoDataframe from the GeoSeries\n",
    "* Drop any invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gyhmqyj9KMG4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "outputId": "d92a674d-2ffd-4659-9678-34d62dc4c629",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "gs = GeoSeries(points)\n",
    "gs.crs = fiona.crs.from_epsg(4326)\n",
    "gd = gpd.GeoDataFrame(geometry=gs)\n",
    "gd =  gd.loc[gd.is_valid]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t0VueRtxKRrJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Akternative method to change the crs\n",
    "#gs.crs = {'init': 'epsg:4326', 'no_defs': True}\n",
    "#gd = gpd.GeoDataFrame(geometry=gs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pdYeJQlHp95k",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "outputId": "f6a10ac8-9d2c-4518-e1f6-3f8312629aa6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "gd.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOe-4Ag7HQm0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add the original coordinates to the GeoDataframe, so we will have them in the result and we can use them to join back to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b4fPvCEEKf2y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "gd['pickup_longitude'] = locations_nyc[\"pickup_longitude\"].values\n",
    "gd['pickup_latitude'] = locations_nyc[\"pickup_latitude\"].values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GblIZz78qu1v",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "outputId": "5e929188-c296-4ef3-ac07-45229f7de5e6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "gd.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzT8aR2kHbWw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check the DeoDataframe crs type"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NJbJImFKCIf1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "outputId": "6d7aa6fa-49c5-48c1-8078-b6c5baadf7aa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "gd.crs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br5zIcQLHq7q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Perform a spatial join between out points and the taxi zones"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mf_zFXib_egb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "outputId": "ab3e70cd-3bf8-4d89-fb6d-e4bf9462ef52",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "join = gpd.sjoin(gd, zones, how=\"inner\", op=\"within\")\n",
    "join.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KxQtvw9H18t",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a pandas dataframe with the non spatial data to join back with the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SrDcDIO2FELp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# pickup_longitude | pickup_latitude | borough | location_i | zone\n",
    "pickup_non_spacial_data = pd.DataFrame({'pickup_longitude': join['pickup_longitude'], 'pickup_latitude':join['pickup_latitude'] ,\n",
    "                                 'pickup_borough': join['borough'], 'pickup_location_id': join['location_i'], 'pickup_zone': join['zone']})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4B1DAh5YQYWI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "outputId": "6d404d1b-b4cb-4598-f3b7-fcaba8f83dba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "pickup_non_spacial_data.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51pbHveUIWfN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dropoff\n",
    "\n",
    "Same thing with the dropoff coordinates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2zOSF-u0Ilz6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "3c85ae71-b944-478f-c3cb-7b6f53cc5978",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# stack latitude and longitude together \n",
    "dropoff = np.vstack((locations_nyc[\"dropoff_longitude\"].values, locations_nyc[\"dropoff_latitude\"].values)).T \n",
    "\n",
    "# Convert each couple into a point object\n",
    "t1 = datetime.now()\n",
    "points = [Point(i) for i in dropoff]\n",
    "t2 = datetime.now()\n",
    "delta = t2 - t1\n",
    "print(\"time for conversion: {}\".format(delta.seconds))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wi6VynimJFw7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "outputId": "8307ff84-7c2f-4969-8906-cbbde1d7a1ed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "gs = GeoSeries(points)\n",
    "gs.crs = fiona.crs.from_epsg(4326)\n",
    "gd = gpd.GeoDataFrame(geometry=gs)\n",
    "gd =  gd.loc[gd.is_valid]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bfRq-WqFJGV9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "gd['dropoff_longitude'] = locations_nyc[\"dropoff_longitude\"].values\n",
    "gd['dropoff_latitude'] = locations_nyc[\"dropoff_latitude\"].values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AVuMDyyzJPj9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "outputId": "9f1dd4a7-3bef-476c-a11b-fceabe781004",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "join = gpd.sjoin(gd, zones, how=\"inner\", op=\"within\")\n",
    "join.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CyoLKjFdJWAB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# dropoff_longitude | dropoff_latitude | borough | location_i | zone\n",
    "dropoff_non_spacial_data = pd.DataFrame({'dropoff_longitude': join['dropoff_longitude'], 'dropoff_latitude':join['dropoff_latitude'] ,\n",
    "                                 'dropoff_borough': join['borough'], 'dropoff_location_id': join['location_i'], 'dropoff_zone': join['zone']})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vs08BCsKOex",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check for index dulicates\n",
    "\n",
    "There are coordinates which match with more than one zone and create duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_valid_records = pd.read_csv('./ma_results/january_before_zones.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4sNbT8SPT-bN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "outputId": "153afe77-1ffc-4aa0-ad5d-ad0e321194b2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records[all_valid_records.index.duplicated()]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ldrx-KREQXHJ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47
    },
    "outputId": "db59f469-630a-4938-855b-a9557f95bbc7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "pickup_non_spacial_data[pickup_non_spacial_data.index.duplicated()]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IZq15UkQTvSA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "outputId": "351ef617-43bb-42b5-9027-e8c73dbfa2cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dropoff_non_spacial_data[dropoff_non_spacial_data.index.duplicated()]#.index[0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLK6R6V-doJu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If there is a duplicate, locate it by the index"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LUYZ3-cDRVwd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "outputId": "e36187dd-3f35-47f9-e8ae-da75368cb856",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dropoff_non_spacial_data.loc[7971094]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M-wWa2rZoxX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**It is not a problem cause by my implementation, also BigQuery make this duplicate**\n",
    "\n",
    "We can drop the duplicate and keep just the first result"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HXVpfvQkaprY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Drop duplicates\n",
    "dropoff_non_spacial_data = dropoff_non_spacial_data.loc[~dropoff_non_spacial_data.index.duplicated(keep='first')]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP_GX_nW9y8i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check Most common combination of dropoff_longitude and dropoff_latitude"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xlkZ32_1lhMk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "outputId": "f229e4a8-2748-46f7-a78a-a3aa7ff7c308",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Most common combination of dropoff_longitude and dropoff_latitude\n",
    "dropoff_non_spacial_data.groupby(['dropoff_longitude','dropoff_latitude']).size().sort_values(ascending=False)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEeivGDLLGme",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lwQDeEGuLShM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "pickup_zones = pickup_non_spacial_data[ ['pickup_longitude', 'pickup_latitude', 'pickup_location_id']].copy()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0aaKjeheLfoU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dropoff_zones = dropoff_non_spacial_data[ ['dropoff_longitude', 'dropoff_latitude', 'dropoff_location_id']].copy()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GSe5t_hkX0LK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# save the csv\n",
    "pickup_zones.to_csv('./ma_results/pickup_zones_1.csv', index=False)\n",
    "dropoff_zones.to_csv('./ma_results/dropoff_zones_1.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EGzNSigH9pVg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# read the csv\n",
    "pickup_zones = pd.read_csv('./ma_results/pickup_zones_1.csv')\n",
    "dropoff_zones = pd.read_csv('./ma_results/dropoff_zones_1.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnQGwcaQZWe2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Joining separately"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "me_nQfMOaHJx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "all_valid_records"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ieupbo3zZhj-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "trips_with_zones = pd.merge(all_valid_records, pickup_zones,  how='left',\n",
    "                  left_on=['pickup_longitude','pickup_latitude'],\n",
    "                  right_on = ['pickup_longitude','pickup_latitude'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ggEuO2ih-BKs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "trips_with_zones.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FfnmXRju-4oy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "del trips_with_zones\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpeAsVFlbIk9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This merge returns 26M rows... this because there are duplicates in the pickup_zones table and they match with more than one row each.\n",
    "\n",
    "we need to drop them"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xhhk0I-_aAbS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "pickup_zones = pickup_zones.drop_duplicates(subset=['pickup_longitude','pickup_latitude'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6i_DPv48bk13",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "trips_with_zones = pd.merge(all_valid_records, pickup_zones,  how='left',\n",
    "                  left_on=['pickup_longitude','pickup_latitude'],\n",
    "                  right_on = ['pickup_longitude','pickup_latitude'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O8EE2lo9cCR9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dropoff_zones = dropoff_zones.drop_duplicates(subset=['dropoff_longitude','dropoff_latitude'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aHVvCvUfZqYn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "trips_with_zones = pd.merge(trips_with_zones, dropoff_zones,  how='left',\n",
    "                  left_on = ['dropoff_longitude','dropoff_latitude'],\n",
    "                  right_on = ['dropoff_longitude','dropoff_latitude'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FwzOvUsjccxn",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "outputId": "0a5c3757-7847-4537-ffe6-273a73f83dd3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "trips_with_zones"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c2owm1kjci66",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "outputId": "4ea3b7df-9bf1-44bb-cf93-c5b569f44cc3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "trips_with_zones.loc[(trips_with_zones.dropoff_location_id.isnull()) | (trips_with_zones.pickup_location_id.isnull())]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8OiUkguIswO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Last operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoV276HiIx7T",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check the min location ID already used"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kJTeyk99I5J_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "zones.location_i.min()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow1E1yvwJJx6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set the Out of the City location ID to 0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QfZyD5mEJRND",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "OoC_ID = 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XX8wKMB6JYBS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Extract the columns we will need for training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PZxw-vBUJdro",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "outputId": "57276a1d-93ed-4a10-87b6-06502798c782",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "trips_with_zones.columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tTcIn67iJjaN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "january = trips_with_zones[['medallion', 'pickup_week_day', 'pickup_hour', 'pickup_day', 'pickup_month', 'dropoff_week_day', 'dropoff_hour', 'dropoff_day', 'dropoff_month', 'pickup_location_id', 'dropoff_location_id']]#.copy()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "poHmEKqWKEzF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "outputId": "8594bb36-aa53-44a1-99a5-71e92a4cabeb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "january.loc[january.pickup_location_id.isnull(), 'pickup_location_id'] = 0.0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uXXISLhQ5Z9j",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "january.loc[january.pickup_location_id == 0.0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nog0yixJQkJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The same now with dropoff"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G57JBhYc5q4W",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "outputId": "0e1d818c-52d5-464f-ab00-3dedfa07534d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "january.loc[january.dropoff_location_id.isnull(), 'dropoff_location_id'] = 0.0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zJMK20835wp2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "january.loc[january.dropoff_location_id == 0.0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nAFmJalg56Lp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "january.loc[(january.dropoff_location_id == 0.0) & (january.pickup_location_id > 0.0)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hr1vUmtbdfGf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "13e491f3-9ee4-47e5-b3ec-aeae472c3707",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# check for null values\n",
    "january.isnull().sum()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oooHAU1odaza",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Save the dataset\n",
    "january.to_csv('./ma_results/trips_with_zones_final.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XBsiLzR2GXSa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "january.head(1000000).to_csv('./ma_results/trips_with_zones_10000000.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FRDAAsaY5K5G",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "outputId": "485ece9a-8852-4d54-b779-234ffa724f7d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# how many rows?\n",
    "df = pd.read_csv('./ma_results/trips_with_zones_10000000.csv')\n",
    "df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VeU5KXVq5f3v",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "outputId": "1e743c54-e849-4f87-a8d7-e96f8681d207",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "df.info()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRmzmqEzyh1k",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('./ma_results/trips_1000000.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}