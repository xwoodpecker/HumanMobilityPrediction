{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 2 - Evaluation of the Biggest Model\n",
    "\n",
    "With a simple central model found, it is now time to evaluate the prediction quality of a model incorporating all available features.\n",
    "This model is expected to provide a similar if not worse prediction quality than the first central model.\n",
    "Reason for that expectation is the fact that features like longitude, latitude and the venue_id could confuse a model.\n",
    "The relation between these features and the target feature is not recognizable from the data alone (i.e. physical coordinates to semantic place)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=\"./log/biggest-model/Evaluation.log\", level=logging.INFO)\n",
    "\n",
    "def log(text):\n",
    "  print(text)\n",
    "  logging.info(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "b'Hello, World!'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the TFF is working:\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Biggest Model\n",
    "\n",
    "This model incorporates the maximum amount of available and useful data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "    cat_id  user_id   latitude  longitude  is_weekend  clock_sin  clock_cos  \\\n0        0      470  40.719810 -74.002581       False  -1.000000   0.000654   \n1        1      979  40.606800 -74.044170       False  -0.999998   0.001818   \n2        2       69  40.716162 -73.883070       False  -0.999945   0.010472   \n3        3      395  40.745164 -73.982519       False  -0.999931   0.011708   \n4        4       87  40.740104 -73.989658       False  -0.999914   0.013090   \n..     ...      ...        ...        ...         ...        ...        ...   \n95       7      445  40.828602 -73.879259       False  -0.959601   0.281365   \n96       6      235  40.745463 -73.990983       False  -0.956326   0.292302   \n97       8      118  40.600144 -73.946593       False  -0.955729   0.294249   \n98       2     1054  40.870630 -74.097926       False  -0.955407   0.295291   \n99      15      881  40.808700 -73.958515       False  -0.954631   0.297791   \n\n     day_sin   day_cos  month_sin  month_cos  week_day_sin  week_day_cos  \\\n0   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n1   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n2   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n3   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n4   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n..       ...       ...        ...        ...           ...           ...   \n95  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n96  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n97  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n98  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n99  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n\n    venue_id  orig_cat_id  \n0          0            0  \n1          1            1  \n2          2            2  \n3          3            3  \n4          4            4  \n..       ...          ...  \n95        93           24  \n96        94            6  \n97        95           57  \n98        96           58  \n99        97           21  \n\n[100 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat_id</th>\n      <th>user_id</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>is_weekend</th>\n      <th>clock_sin</th>\n      <th>clock_cos</th>\n      <th>day_sin</th>\n      <th>day_cos</th>\n      <th>month_sin</th>\n      <th>month_cos</th>\n      <th>week_day_sin</th>\n      <th>week_day_cos</th>\n      <th>venue_id</th>\n      <th>orig_cat_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>470</td>\n      <td>40.719810</td>\n      <td>-74.002581</td>\n      <td>False</td>\n      <td>-1.000000</td>\n      <td>0.000654</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>979</td>\n      <td>40.606800</td>\n      <td>-74.044170</td>\n      <td>False</td>\n      <td>-0.999998</td>\n      <td>0.001818</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>69</td>\n      <td>40.716162</td>\n      <td>-73.883070</td>\n      <td>False</td>\n      <td>-0.999945</td>\n      <td>0.010472</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>395</td>\n      <td>40.745164</td>\n      <td>-73.982519</td>\n      <td>False</td>\n      <td>-0.999931</td>\n      <td>0.011708</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>87</td>\n      <td>40.740104</td>\n      <td>-73.989658</td>\n      <td>False</td>\n      <td>-0.999914</td>\n      <td>0.013090</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>7</td>\n      <td>445</td>\n      <td>40.828602</td>\n      <td>-73.879259</td>\n      <td>False</td>\n      <td>-0.959601</td>\n      <td>0.281365</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>93</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>6</td>\n      <td>235</td>\n      <td>40.745463</td>\n      <td>-73.990983</td>\n      <td>False</td>\n      <td>-0.956326</td>\n      <td>0.292302</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>94</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>8</td>\n      <td>118</td>\n      <td>40.600144</td>\n      <td>-73.946593</td>\n      <td>False</td>\n      <td>-0.955729</td>\n      <td>0.294249</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>95</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>2</td>\n      <td>1054</td>\n      <td>40.870630</td>\n      <td>-74.097926</td>\n      <td>False</td>\n      <td>-0.955407</td>\n      <td>0.295291</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>96</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>15</td>\n      <td>881</td>\n      <td>40.808700</td>\n      <td>-73.958515</td>\n      <td>False</td>\n      <td>-0.954631</td>\n      <td>0.297791</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>97</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./4square/processed_transformed_big.csv\")\n",
    "df.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is best, to use only the best 100 users for this purpose.\n",
    "As they have the longest sequences of visited places."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "count = df.user_id.value_counts()\n",
    "\n",
    "idx = count.loc[count.index[:100]].index # count >= 100\n",
    "df = df.loc[df.user_id.isin(idx)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An array is created containing all visited locations for every user.\n",
    "The original data is sorted by time (ascending).\n",
    "Thus, the array contains a sequence of visited location categories by user."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2222.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# List the df for each user\n",
    "users_locations = []\n",
    "\n",
    "# For each user\n",
    "for user_id in tqdm(idx):\n",
    "  users_locations.append(df.loc[df.user_id == user_id].copy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is necessary to first split the data in train, valid and test for each user.\n",
    "Then, these are merged together again later on.\n",
    "This is done to ensure that the sequences are kept together and not split randomly for the users."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# List the dfs fo train, val and test for each user\n",
    "users_locations_train = []\n",
    "users_locations_val = []\n",
    "users_locations_test = []\n",
    "\n",
    "for user_df in users_locations:\n",
    "  # Split in train, test and validation\n",
    "  train, test = train_test_split(user_df, test_size=0.2, shuffle=False)\n",
    "  train, val = train_test_split(train, test_size=0.2, shuffle=False)\n",
    "\n",
    "  # Append the sets\n",
    "  users_locations_train.append(train)\n",
    "  users_locations_val.append(val)\n",
    "  users_locations_test.append(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataframes are concatenated again."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Merge back the dataframes\n",
    "df_train = pd.concat(users_locations_train)\n",
    "\n",
    "# Merge back the dataframes\n",
    "df_val = pd.concat(users_locations_val)\n",
    "\n",
    "# Merge back the dataframes\n",
    "df_test = pd.concat(users_locations_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity check: Was data lost when splitting and merging back together?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "user_ids = df_train.user_id.unique()\n",
    "print(user_ids.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No, everything is fine.\n",
    "Now, the data has to be split in sequences of length N.\n",
    "The following code is structured in methods, so the best value for N can be found."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the column names for the method below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat_id' 'user_id' 'latitude' 'longitude' 'is_weekend' 'clock_sin'\n",
      " 'clock_cos' 'day_sin' 'day_cos' 'month_sin' 'month_cos' 'week_day_sin'\n",
      " 'week_day_cos' 'venue_id' 'orig_cat_id']\n"
     ]
    }
   ],
   "source": [
    "columns_names = df_train.columns.values\n",
    "print(columns_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# List of numerical column names\n",
    "numerical_column_names = ['latitude', 'longitude', 'clock_sin', 'clock_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'week_day_sin', 'week_day_cos']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class CategoricalFeature:\n",
    "  def __init__(self, feature_name, vocab_size, use_embedding):\n",
    "    self.feature_name = feature_name\n",
    "    self.vocab_size = vocab_size\n",
    "    self.use_embedding = use_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique uber categories:  27\n",
      "Unique users:  100\n",
      "Unique venues:  11331\n",
      "Unique original categories:  368\n"
     ]
    }
   ],
   "source": [
    "vocab_size = df.cat_id.unique().size\n",
    "users_size = df.user_id.unique().size\n",
    "venues_size = df.venue_id.unique().size\n",
    "orig_cats_size = df.orig_cat_id.unique().size\n",
    "\n",
    "print('Unique uber categories: ', vocab_size)\n",
    "print('Unique users: ', users_size)\n",
    "print('Unique venues: ', venues_size)\n",
    "print('Unique original categories: ', orig_cats_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "  CategoricalFeature('user_id', users_size, False),\n",
    "  CategoricalFeature('cat_id', vocab_size, False),\n",
    "  CategoricalFeature('venue_id', venues_size, False),\n",
    "  CategoricalFeature('orig_cat_id', orig_cats_size, False)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tunable Parameters:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "NUM_CLIENTS = user_ids.size\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 16\n",
    "#SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper functions to split data, create clients dictionaries and preprocess the data for the FL algorithm.\n",
    "The model creation is also defined here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Split the data into chunks of N\n",
    "def split_data(N):\n",
    "\n",
    "  # dictionary of list of df\n",
    "  df_dictionary = {}\n",
    "\n",
    "  for uid in tqdm(user_ids):\n",
    "    # Get the records of the user\n",
    "    user_df_train = df_train.loc[df_train.user_id == uid].copy()\n",
    "    user_df_val = df_val.loc[df_val.user_id == uid].copy()\n",
    "    user_df_test = df_test.loc[df_test.user_id == uid].copy()\n",
    "\n",
    "    # Get a list of dataframes of length N records\n",
    "    user_list_train = [user_df_train[i:i+N] for i in range(0, user_df_train.shape[0], N)]\n",
    "    user_list_val = [user_df_val[i:i+N] for i in range(0, user_df_val.shape[0], N)]\n",
    "    user_list_test = [user_df_test[i:i+N] for i in range(0, user_df_test.shape[0], N)]\n",
    "\n",
    "    # Save the list of dataframes into a dictionary\n",
    "    df_dictionary[uid] = {\n",
    "        'train': user_list_train,\n",
    "        'val': user_list_val,\n",
    "        'test': user_list_test\n",
    "    }\n",
    "\n",
    "  return  df_dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Takes a dictionary with train, validation and test sets and the desired set type\n",
    "def create_clients_dict(df_dictionary, set_type, N):\n",
    "\n",
    "  dataset_dict = {}\n",
    "\n",
    "  for uid in tqdm(user_ids):\n",
    "\n",
    "    c_data = collections.OrderedDict()\n",
    "    values = df_dictionary[uid][set_type]\n",
    "\n",
    "    # If the last dataframe of the list is not complete\n",
    "    if len(values[-1]) < N:\n",
    "      diff = 1\n",
    "    else:\n",
    "      diff = 0\n",
    "\n",
    "    if len(values) > 0:\n",
    "      # Create the dictionary to create a clientData\n",
    "      for header in columns_names:\n",
    "        c_data[header] = [values[i][header].values for i in range(0, len(values)-diff)]\n",
    "      dataset_dict[uid] = c_data\n",
    "\n",
    "  return dataset_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# preprocess dataset to tf format\n",
    "def preprocess(dataset, N):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "\n",
    "    x=collections.OrderedDict()\n",
    "\n",
    "    for name in columns_names:\n",
    "      x[name]=tf.reshape(element[name][:, :-1], [-1, N-1])\n",
    "\n",
    "    y=tf.reshape(element[columns_names[0]][:, 1:], [-1, N-1])\n",
    "\n",
    "    return collections.OrderedDict(x=x, y=y)\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE, drop_remainder=True).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# create federated data for every client\n",
    "def make_federated_data(client_data, client_ids, N):\n",
    "\n",
    "  return [\n",
    "      preprocess(client_data.create_tf_dataset_for_client(x), N)\n",
    "      for x in tqdm(client_ids)\n",
    "  ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Create a model\n",
    "def create_keras_model(number_of_places, N, batch_size):\n",
    "\n",
    "  # Shortcut to the layers package\n",
    "  l = tf.keras.layers\n",
    "\n",
    "  # List of numeric feature columns to pass to the DenseLayer\n",
    "  numeric_feature_columns = []\n",
    "\n",
    "  # Handling numerical columns\n",
    "  for header in numerical_column_names:\n",
    "\t\t# Append all the numerical columns defined into the list\n",
    "    numeric_feature_columns.append(feature_column.numeric_column(header, shape=N-1))\n",
    "\n",
    "  feature_inputs={}\n",
    "  for c_name in numerical_column_names:\n",
    "    feature_inputs[c_name] = tf.keras.Input((N-1,), batch_size=batch_size, name=c_name)\n",
    "\n",
    "  # We cannot use an array of features as always because we have sequences\n",
    "  # We have to do one by one in order to match the shape\n",
    "  num_features = []\n",
    "  for c_name in numerical_column_names:\n",
    "    f =  feature_column.numeric_column(c_name, shape=(N-1))\n",
    "    feature = l.DenseFeatures(f)(feature_inputs)\n",
    "    feature = tf.expand_dims(feature, -1)\n",
    "    num_features.append(feature)\n",
    "\n",
    "  categorical_feature_inputs = []\n",
    "  categorical_features = []\n",
    "  for categorical_feature in categorical_columns:  # add batch_size=batch_size in case of stateful GRU\n",
    "    d = {categorical_feature.feature_name: tf.keras.Input((N-1,), batch_size=batch_size, dtype=tf.dtypes.int32, name=categorical_feature.feature_name)}\n",
    "    categorical_feature_inputs.append(d)\n",
    "\n",
    "    one_hot = feature_column.sequence_categorical_column_with_vocabulary_list(categorical_feature.feature_name, [i for i in range(categorical_feature.vocab_size)])\n",
    "\n",
    "    if categorical_feature.use_embedding:\n",
    "      # Embed the one-hot encoding\n",
    "      categorical_features.append(feature_column.embedding_column(one_hot, 256))\n",
    "    else:\n",
    "      categorical_features.append(feature_column.indicator_column(one_hot))\n",
    "\n",
    "  seq_features = []\n",
    "  for i in range(0, len(categorical_feature_inputs)):\n",
    "    sequence_features, sequence_length = tf.keras.experimental.SequenceFeatures(categorical_features[i])(categorical_feature_inputs[i])\n",
    "    seq_features.append(sequence_features)\n",
    "\n",
    "  input_sequence = l.Concatenate(axis=2)( [] + seq_features + num_features)\n",
    "\n",
    "  # Rnn\n",
    "  recurrent = l.GRU(256,\n",
    "                        batch_size=batch_size, #in case of stateful\n",
    "                        dropout=0.3,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform')(input_sequence)\n",
    "\n",
    "\n",
    "\t# Last layer with an output for each place\n",
    "  dense_1 = layers.Dense(number_of_places)(recurrent)\n",
    "\n",
    "\t# Softmax output layer\n",
    "  output = l.Softmax()(dense_1)\n",
    "\n",
    "\t# To return the Model, we need to define its inputs and outputs\n",
    "\t# In out case, we need to list all the input layers we have defined\n",
    "  inputs = list(feature_inputs.values()) + categorical_feature_inputs\n",
    "\n",
    "\t# Return the Model\n",
    "  return tf.keras.Model(inputs=inputs, outputs=output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#train and evaluate the model\n",
    "def train_and_eval_model(vocab_size, n, federated_train_data, federated_val_data, federated_test_data, path='./log/central-test-run'):\n",
    "  train_logdir = path + '/train'\n",
    "  val_logdir = path + '/val'\n",
    "  eval_logdir = path + '/eval'\n",
    "\n",
    "  train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
    "  val_summary_writer = tf.summary.create_file_writer(val_logdir)\n",
    "  eval_summary_writer = tf.summary.create_file_writer(eval_logdir)\n",
    "\n",
    "  # Clone the keras_model inside `create_tff_model()`, which TFF will\n",
    "  # call to produce a new copy of the model inside the graph that it will\n",
    "  # serialize. Note: we want to construct all the necessary objects we'll need\n",
    "  # _inside_ this method.\n",
    "  def create_tff_model():\n",
    "    # TFF uses an `input_spec` so it knows the types and shapes\n",
    "    # that your model expects.\n",
    "    input_spec = federated_train_data[0].element_spec\n",
    "    keras_model_clone = create_keras_model(vocab_size, n, batch_size=BATCH_SIZE)\n",
    "    #plot_model(keras_model_clone, 'keras_model_for_fl.png', show_shapes=True)\n",
    "    tff_model = tff.learning.from_keras_model(\n",
    "      keras_model_clone,\n",
    "      input_spec=input_spec,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return tff_model\n",
    "\n",
    "  # This command builds all the TensorFlow graphs and serializes them:\n",
    "  fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.002),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.06))\n",
    "\n",
    "  state = fed_avg.initialize()\n",
    "  evaluation = tff.learning.build_federated_evaluation(model_fn=create_tff_model)\n",
    "\n",
    "  tolerance = 7\n",
    "  best_state = 0\n",
    "  lowest_loss = 100.00\n",
    "  stop = tolerance\n",
    "\n",
    "  NUM_ROUNDS = 10\n",
    "  with train_summary_writer.as_default():\n",
    "    for round_num in range(1, NUM_ROUNDS + 1):\n",
    "      log('Round {r}'.format(r=round_num))\n",
    "\n",
    "      # Uncomment to simulate sparse availability of clients\n",
    "      # train_data_for_this_round, val_data_for_this_round = sample((federated_train_data, federated_val_data), 20, NUM_CLIENTS)\n",
    "\n",
    "      state, metrics = fed_avg.next(state, federated_train_data)\n",
    "\n",
    "      train_metrics = metrics['train']\n",
    "      log('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "      val_metrics = evaluation(state.model, federated_val_data)\n",
    "      log('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "      # Check for decreasing validation loss\n",
    "      if lowest_loss > val_metrics['loss']:\n",
    "        log('\\tSaving best model..')\n",
    "        lowest_loss = val_metrics['loss']\n",
    "        best_state = state\n",
    "        stop = tolerance - 1\n",
    "      else:\n",
    "        stop = stop - 1\n",
    "        if stop <= 0:\n",
    "          log('\\tEarly stopping...')\n",
    "          break;\n",
    "\n",
    "      log(' ')\n",
    "      log('\\twriting..')\n",
    "\n",
    "      # Iterate across the metrics and write their data\n",
    "      for name, value in dict(train_metrics).items():\n",
    "        tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "      with val_summary_writer.as_default():\n",
    "        for name, value in dict(val_metrics).items():\n",
    "          tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "  train_summary_writer.close()\n",
    "  val_summary_writer.close()\n",
    "\n",
    "  # evaluate over test data\n",
    "  test_metrics = evaluation(best_state.model, federated_test_data)\n",
    "  log('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we do a test run for N=16 and see if everything works.\n",
    "Then, we run the same logic for different lengths of sequences and compare the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 411.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 135.32it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 854.70it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 398.41it/s]\n"
     ]
    }
   ],
   "source": [
    "n=17\n",
    "df_dict = split_data(n)\n",
    "clients_train_dict = create_clients_dict(df_dict, 'train', n)\n",
    "clients_val_dict = create_clients_dict(df_dict, 'val', n)\n",
    "clients_test_dict = create_clients_dict(df_dict, 'test', n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Convert the dictionary to a dataset\n",
    "client_train_data = tff.simulation.FromTensorSlicesClientData(clients_train_dict)\n",
    "client_val_data = tff.simulation.FromTensorSlicesClientData(clients_val_dict)\n",
    "client_test_data = tff.simulation.FromTensorSlicesClientData(clients_test_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('cat_id', TensorSpec(shape=(17,), dtype=tf.int32, name=None)),\n             ('user_id', TensorSpec(shape=(17,), dtype=tf.int32, name=None)),\n             ('latitude',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('longitude',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('is_weekend', TensorSpec(shape=(17,), dtype=tf.bool, name=None)),\n             ('clock_sin',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('clock_cos',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('day_sin', TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('day_cos', TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('month_sin',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('month_cos',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('week_day_sin',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('week_day_cos',\n              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n             ('venue_id', TensorSpec(shape=(17,), dtype=tf.int32, name=None)),\n             ('orig_cat_id',\n              TensorSpec(shape=(17,), dtype=tf.int32, name=None))])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_train_data.create_tf_dataset_for_client(user_ids[0]).element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('cat_id',\n              <tf.Tensor: shape=(17,), dtype=int32, numpy=array([ 9, 11, 22, 21,  9, 16, 18,  1,  7, 11,  1, 11, 21, 16, 25, 22, 18])>),\n             ('user_id',\n              <tf.Tensor: shape=(17,), dtype=int32, numpy=\n              array([185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185,\n                     185, 185, 185, 185])>),\n             ('latitude',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([40.96541694, 40.96532878, 40.96535502, 40.96533795, 40.96535583,\n                     40.96532194, 40.96541698, 40.96535005, 40.9645718 , 40.96532878,\n                     40.96535005, 40.96536873, 40.96533795, 40.96532194, 40.96550875,\n                     40.96535502, 40.96541698])>),\n             ('longitude',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([-74.06288376, -74.06292392, -74.06283213, -74.0628079 ,\n                     -74.06285508, -74.06280345, -74.06291968, -74.06280287,\n                     -74.06643391, -74.06292392, -74.06280287, -74.06283225,\n                     -74.0628079 , -74.06280345, -74.06288358, -74.06283213,\n                     -74.06291968])>),\n             ('is_weekend',\n              <tf.Tensor: shape=(17,), dtype=bool, numpy=\n              array([False, False, False, False, False, False, False, False, False,\n                     False, False, False, False, False, False, False, False])>),\n             ('clock_sin',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([-0.58560629, -0.42406771, -0.03780646,  0.26478481,  0.50144782,\n                      0.73055967,  0.83906642,  0.90383377,  0.26618706, -0.16884778,\n                     -0.43555844, -0.58407259, -0.91457775, -0.98147423, -0.98332111,\n                     -0.91560412, -0.30548758])>),\n             ('clock_cos',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([ 0.81059563,  0.90563049,  0.99928508,  0.96430753,  0.86518789,\n                      0.68284887,  0.54402899,  0.42788376, -0.96392139, -0.98564214,\n                     -0.90016046, -0.81170143, -0.40441011, -0.19159418,  0.18187799,\n                      0.40208095,  0.95219606])>),\n             ('day_sin',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([0.58778525, 0.58778525, 0.58778525, 0.74314483, 0.74314483,\n                     0.74314483, 0.74314483, 0.74314483, 0.74314483, 0.74314483,\n                     0.74314483, 0.74314483, 0.74314483, 0.74314483, 0.74314483,\n                     0.74314483, 0.74314483])>),\n             ('day_cos',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([0.80901699, 0.80901699, 0.80901699, 0.66913061, 0.66913061,\n                     0.66913061, 0.66913061, 0.66913061, 0.66913061, 0.66913061,\n                     0.66913061, 0.66913061, 0.66913061, 0.66913061, 0.66913061,\n                     0.66913061, 0.66913061])>),\n             ('month_sin',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([0.8660254, 0.8660254, 0.8660254, 0.8660254, 0.8660254, 0.8660254,\n                     0.8660254, 0.8660254, 0.8660254, 0.8660254, 0.8660254, 0.8660254,\n                     0.8660254, 0.8660254, 0.8660254, 0.8660254, 0.8660254])>),\n             ('month_cos',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,\n                     -0.5, -0.5, -0.5, -0.5, -0.5, -0.5])>),\n             ('week_day_sin',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([0.78183148, 0.78183148, 0.78183148, 0.97492791, 0.97492791,\n                     0.97492791, 0.97492791, 0.97492791, 0.97492791, 0.97492791,\n                     0.97492791, 0.97492791, 0.97492791, 0.97492791, 0.97492791,\n                     0.97492791, 0.97492791])>),\n             ('week_day_cos',\n              <tf.Tensor: shape=(17,), dtype=float64, numpy=\n              array([ 0.6234898 ,  0.6234898 ,  0.6234898 , -0.22252093, -0.22252093,\n                     -0.22252093, -0.22252093, -0.22252093, -0.22252093, -0.22252093,\n                     -0.22252093, -0.22252093, -0.22252093, -0.22252093, -0.22252093,\n                     -0.22252093, -0.22252093])>),\n             ('venue_id',\n              <tf.Tensor: shape=(17,), dtype=int32, numpy=\n              array([ 330,  434,  635,  795,  864,  927,  964,  992, 1153,  434,  992,\n                     1478,  795,  927, 1816,  635,  964])>),\n             ('orig_cat_id',\n              <tf.Tensor: shape=(17,), dtype=int32, numpy=\n              array([125,  15, 176,  63, 201,  23,  26,  27,   7,  15,  27,  15,  63,\n                      23,  73, 176,  26])>)])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_dataset = client_train_data.create_tf_dataset_for_client(\n",
    "    client_train_data.client_ids[1])\n",
    "\n",
    "example_element = next(iter(example_dataset))\n",
    "example_element"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After looking at an example dataset, it can be concluded that the layout of the data is also as expected."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(16, 16)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_example_dataset = preprocess(example_dataset, n)\n",
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
    "                                     next(iter(preprocessed_example_dataset)))\n",
    "\n",
    "sample_batch['x']['cat_id'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset shapes: OrderedDict([(x, OrderedDict([(cat_id, (16, 16)), (user_id, (16, 16)), (latitude, (16, 16)), (longitude, (16, 16)), (is_weekend, (16, 16)), (clock_sin, (16, 16)), (clock_cos, (16, 16)), (day_sin, (16, 16)), (day_cos, (16, 16)), (month_sin, (16, 16)), (month_cos, (16, 16)), (week_day_sin, (16, 16)), (week_day_cos, (16, 16)), (venue_id, (16, 16)), (orig_cat_id, (16, 16))])), (y, (16, 16))]), types: OrderedDict([(x, OrderedDict([(cat_id, tf.int32), (user_id, tf.int32), (latitude, tf.float64), (longitude, tf.float64), (is_weekend, tf.bool), (clock_sin, tf.float64), (clock_cos, tf.float64), (day_sin, tf.float64), (day_cos, tf.float64), (month_sin, tf.float64), (month_cos, tf.float64), (week_day_sin, tf.float64), (week_day_cos, tf.float64), (venue_id, tf.int32), (orig_cat_id, tf.int32)])), (y, tf.int32)])>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_example_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('x',\n              OrderedDict([('cat_id',\n                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None)),\n                           ('user_id',\n                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None)),\n                           ('latitude',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('longitude',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('is_weekend',\n                            TensorSpec(shape=(16, 16), dtype=tf.bool, name=None)),\n                           ('clock_sin',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('clock_cos',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('day_sin',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('day_cos',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('month_sin',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('month_cos',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('week_day_sin',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('week_day_cos',\n                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n                           ('venue_id',\n                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None)),\n                           ('orig_cat_id',\n                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])),\n             ('y', TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_example_dataset.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 26.14it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.27it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 27.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of client datasets: 100\n",
      "First dataset: <PrefetchDataset shapes: OrderedDict([(x, OrderedDict([(cat_id, (16, 16)), (user_id, (16, 16)), (latitude, (16, 16)), (longitude, (16, 16)), (is_weekend, (16, 16)), (clock_sin, (16, 16)), (clock_cos, (16, 16)), (day_sin, (16, 16)), (day_cos, (16, 16)), (month_sin, (16, 16)), (month_cos, (16, 16)), (week_day_sin, (16, 16)), (week_day_cos, (16, 16)), (venue_id, (16, 16)), (orig_cat_id, (16, 16))])), (y, (16, 16))]), types: OrderedDict([(x, OrderedDict([(cat_id, tf.int32), (user_id, tf.int32), (latitude, tf.float64), (longitude, tf.float64), (is_weekend, tf.bool), (clock_sin, tf.float64), (clock_cos, tf.float64), (day_sin, tf.float64), (day_cos, tf.float64), (month_sin, tf.float64), (month_cos, tf.float64), (week_day_sin, tf.float64), (week_day_cos, tf.float64), (venue_id, tf.int32), (orig_cat_id, tf.int32)])), (y, tf.int32)])>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the clients\n",
    "sample_clients = client_train_data.client_ids[0:NUM_CLIENTS]\n",
    "\n",
    "# Federate the clients datasets\n",
    "federated_train_data = make_federated_data(client_train_data, sample_clients, n)\n",
    "federated_val_data = make_federated_data(client_val_data, sample_clients, n)\n",
    "federated_test_data = make_federated_data(client_test_data, sample_clients, n)\n",
    "\n",
    "print('\\nNumber of client datasets: {l}'.format(l=len(federated_train_data)))\n",
    "print('First dataset: {d}'.format(d=federated_train_data[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The preprocessing also works as intended.\n",
    "Now, the model is trained and evaluated.\n",
    "Logs are saved in a dedicated folder."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "\tTrain: loss=2.681, accuracy=0.232\n",
      "\tValidation: loss=6.289, accuracy=0.146\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=4.786, accuracy=0.222\n",
      "\tValidation: loss=8.664, accuracy=0.058\n",
      " \n",
      "\twriting..\n",
      "Round 3\n",
      "\tTrain: loss=9.036, accuracy=0.066\n",
      "\tValidation: loss=4.726, accuracy=0.052\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 4\n",
      "\tTrain: loss=4.255, accuracy=0.115\n",
      "\tValidation: loss=5.453, accuracy=0.174\n",
      " \n",
      "\twriting..\n",
      "Round 5\n",
      "\tTrain: loss=4.543, accuracy=0.198\n",
      "\tValidation: loss=4.398, accuracy=0.144\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 6\n",
      "\tTrain: loss=3.615, accuracy=0.225\n",
      "\tValidation: loss=3.521, accuracy=0.045\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 7\n",
      "\tTrain: loss=2.999, accuracy=0.183\n",
      "\tValidation: loss=3.720, accuracy=0.049\n",
      " \n",
      "\twriting..\n",
      "Round 8\n",
      "\tTrain: loss=3.246, accuracy=0.153\n",
      "\tValidation: loss=3.295, accuracy=0.094\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 9\n",
      "\tTrain: loss=2.988, accuracy=0.191\n",
      "\tValidation: loss=3.353, accuracy=0.174\n",
      " \n",
      "\twriting..\n",
      "Round 10\n",
      "\tTrain: loss=2.841, accuracy=0.236\n",
      "\tValidation: loss=3.259, accuracy=0.178\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.365, accuracy=0.149\n"
     ]
    }
   ],
   "source": [
    "train_and_eval_model(vocab_size, n, federated_train_data, federated_val_data, federated_test_data, path='./log/central-test-run')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding more columns resulted in a worse model performance.\n",
    "Many columns that were used can not be used to identify the next semantic location.\n",
    "For example, lat/long, the original category id or the venue id.\n",
    "The algorithm should not be able to learn from those features.\n",
    "The semantic location does not correlate with physical locations in form of coordinates.\n",
    "Using the more fine-grained location data also does not help the prediction instead it adds more features and complicates the model, thus reducing accuracy.\n",
    "Integrating only those features that are known to have an impact on the selection of the next location is the best choice."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}