{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "b'Hello, World!'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the TFF is working:\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class HyperParameterPreprocessing:\n",
    "  def __init__(self, prefetch_buffer_size, batch_size, num_epochs):\n",
    "    self.prefetch_buffer_size =prefetch_buffer_size\n",
    "    self.batch_size = batch_size\n",
    "    self.num_epochs = num_epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class HyperParameterModel:\n",
    "  def __init__(self, optimizer, learning_rate_client, learning_rate_server, rnn_units, num_rounds):\n",
    "    self.learning_rate_client = learning_rate_client\n",
    "    self.learning_rate_server = learning_rate_server\n",
    "    self.optimizer = optimizer\n",
    "    self.rnn_units = rnn_units\n",
    "    self.num_rounds = num_rounds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"./log/hyper_param_tuning/Evaluation.log\", level=logging.INFO)\n",
    "\n",
    "def log(text):\n",
    "  print(text)\n",
    "  logging.info(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=False, batch_size=32):\n",
    "  dataframe = df.copy()\n",
    "  labels = dataframe.pop('cat_id')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# List of numerical column names\n",
    "numerical_column_names = ['clock_sin', 'clock_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'week_day_sin', 'week_day_cos']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "HP_BATCH_SIZE = 8\n",
    "HP_EMBEDDING = False\n",
    "HP_LEARNING_RATE_CLIENT = 0.002\n",
    "HP_LEARNING_RATE_SERVER = 0.06\n",
    "HP_OPTIMIZER_CLIENT = tf.keras.optimizers.Adam(learning_rate=HP_LEARNING_RATE_CLIENT)\n",
    "HP_OPTIMIZER_SERVER = tf.keras.optimizers.Adam(learning_rate=HP_LEARNING_RATE_SERVER)\n",
    "HP_RNN_UNITS = 256\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "    user_id  cat_id  clock_sin  clock_cos   day_sin   day_cos  month_sin  \\\n0       470       0  -1.000000   0.000654  0.587785  0.809017   0.866025   \n1       983       0  -0.242486  -0.970155  0.743145  0.669131   0.866025   \n2       983       0  -0.400082  -0.916479  0.743145  0.669131   0.866025   \n3       983       0  -0.449514  -0.893273  0.743145  0.669131   0.866025   \n4       470       0  -1.000000   0.000654  0.743145  0.669131   0.866025   \n..      ...     ...        ...        ...       ...       ...        ...   \n95       31       0  -0.987287   0.158948 -0.743145 -0.669131   0.500000   \n96      596       0  -0.982301  -0.187310 -0.951057 -0.309017   0.500000   \n97      278       0  -0.836206  -0.548415 -0.994522 -0.104528   0.500000   \n98      596       0  -0.966076  -0.258257 -0.994522  0.104528   0.500000   \n99     1027       0  -0.971995   0.235001 -0.994522  0.104528   0.500000   \n\n    month_cos  week_day_sin  week_day_cos  \n0   -0.500000      0.781831      0.623490  \n1   -0.500000      0.974928     -0.222521  \n2   -0.500000      0.974928     -0.222521  \n3   -0.500000      0.974928     -0.222521  \n4   -0.500000      0.974928     -0.222521  \n..        ...           ...           ...  \n95  -0.866025     -0.974928     -0.222521  \n96  -0.866025      0.000000      1.000000  \n97  -0.866025      0.781831      0.623490  \n98  -0.866025      0.974928     -0.222521  \n99  -0.866025      0.974928     -0.222521  \n\n[100 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>cat_id</th>\n      <th>clock_sin</th>\n      <th>clock_cos</th>\n      <th>day_sin</th>\n      <th>day_cos</th>\n      <th>month_sin</th>\n      <th>month_cos</th>\n      <th>week_day_sin</th>\n      <th>week_day_cos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>470</td>\n      <td>0</td>\n      <td>-1.000000</td>\n      <td>0.000654</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.500000</td>\n      <td>0.781831</td>\n      <td>0.623490</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>983</td>\n      <td>0</td>\n      <td>-0.242486</td>\n      <td>-0.970155</td>\n      <td>0.743145</td>\n      <td>0.669131</td>\n      <td>0.866025</td>\n      <td>-0.500000</td>\n      <td>0.974928</td>\n      <td>-0.222521</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>983</td>\n      <td>0</td>\n      <td>-0.400082</td>\n      <td>-0.916479</td>\n      <td>0.743145</td>\n      <td>0.669131</td>\n      <td>0.866025</td>\n      <td>-0.500000</td>\n      <td>0.974928</td>\n      <td>-0.222521</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>983</td>\n      <td>0</td>\n      <td>-0.449514</td>\n      <td>-0.893273</td>\n      <td>0.743145</td>\n      <td>0.669131</td>\n      <td>0.866025</td>\n      <td>-0.500000</td>\n      <td>0.974928</td>\n      <td>-0.222521</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>470</td>\n      <td>0</td>\n      <td>-1.000000</td>\n      <td>0.000654</td>\n      <td>0.743145</td>\n      <td>0.669131</td>\n      <td>0.866025</td>\n      <td>-0.500000</td>\n      <td>0.974928</td>\n      <td>-0.222521</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>31</td>\n      <td>0</td>\n      <td>-0.987287</td>\n      <td>0.158948</td>\n      <td>-0.743145</td>\n      <td>-0.669131</td>\n      <td>0.500000</td>\n      <td>-0.866025</td>\n      <td>-0.974928</td>\n      <td>-0.222521</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>596</td>\n      <td>0</td>\n      <td>-0.982301</td>\n      <td>-0.187310</td>\n      <td>-0.951057</td>\n      <td>-0.309017</td>\n      <td>0.500000</td>\n      <td>-0.866025</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>278</td>\n      <td>0</td>\n      <td>-0.836206</td>\n      <td>-0.548415</td>\n      <td>-0.994522</td>\n      <td>-0.104528</td>\n      <td>0.500000</td>\n      <td>-0.866025</td>\n      <td>0.781831</td>\n      <td>0.623490</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>596</td>\n      <td>0</td>\n      <td>-0.966076</td>\n      <td>-0.258257</td>\n      <td>-0.994522</td>\n      <td>0.104528</td>\n      <td>0.500000</td>\n      <td>-0.866025</td>\n      <td>0.974928</td>\n      <td>-0.222521</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>1027</td>\n      <td>0</td>\n      <td>-0.971995</td>\n      <td>0.235001</td>\n      <td>-0.994522</td>\n      <td>0.104528</td>\n      <td>0.500000</td>\n      <td>-0.866025</td>\n      <td>0.974928</td>\n      <td>-0.222521</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./4square/processed_transformed.csv\")\n",
    "df.head(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "count = df.user_id.value_counts()\n",
    "\n",
    "idx = count.loc[count.index[:100]].index # count >= 100\n",
    "df = df.loc[df.user_id.isin(idx)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "# the number of different categories defines the vocabulary size\n",
    "categories = df.cat_id\n",
    "vocab_size = categories.nunique()\n",
    "\n",
    "print('vocabulary size:', vocab_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 3199.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# List the df for each user\n",
    "users_locations = []\n",
    "\n",
    "# For each user\n",
    "for user_id in tqdm(idx):\n",
    "  users_locations.append(df.loc[df.user_id == user_id].copy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# List the dfs fo train, val and test for each user\n",
    "users_locations_train = []\n",
    "users_locations_val = []\n",
    "users_locations_test = []\n",
    "\n",
    "for user_df in users_locations:\n",
    "  # Split in train, test and validation\n",
    "  train, test = train_test_split(user_df, test_size=0.2, shuffle=False)\n",
    "  train, val = train_test_split(train, test_size=0.2, shuffle=False)\n",
    "\n",
    "  # Append the sets\n",
    "  users_locations_train.append(train)\n",
    "  users_locations_val.append(val)\n",
    "  users_locations_test.append(test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Mean number of locations:  644.66\n",
      "Max number of locations:  2697\n",
      "Min number of locations:  387\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "# Number of locations for each user\n",
    "for user_df in users_locations:\n",
    "  sizes.append(user_df.shape[0])\n",
    "\n",
    "print('Training')\n",
    "print('Mean number of locations: ', np.mean(np.array(sizes)))\n",
    "print('Max number of locations: ', np.max(np.array(sizes)))\n",
    "print('Min number of locations: ', np.min(np.array(sizes)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Mean number of locations:  103.48\n",
      "Max number of locations:  432\n",
      "Min number of locations:  62\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "# Number of locations for each user in the validation set\n",
    "for user_df in users_locations_val:\n",
    "  sizes.append(user_df.shape[0])\n",
    "\n",
    "print('Validation')\n",
    "print('Mean number of locations: ', np.mean(np.array(sizes)))\n",
    "print('Max number of locations: ', np.max(np.array(sizes)))\n",
    "print('Min number of locations: ', np.min(np.array(sizes)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of locations:  129.32\n",
      "Max number of locations:  540\n",
      "Min number of locations:  78\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "# Number of locations for each user in the validation set\n",
    "for user_df in users_locations_test:\n",
    "  sizes.append(user_df.shape[0])\n",
    "\n",
    "print('Mean number of locations: ', np.mean(np.array(sizes)))\n",
    "print('Max number of locations: ', np.max(np.array(sizes)))\n",
    "print('Min number of locations: ', np.min(np.array(sizes)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "user_ids = df_train.user_id.unique()\n",
    "print(user_ids.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat_id' 'clock_sin' 'clock_cos' 'day_sin' 'day_cos' 'month_sin'\n",
      " 'month_cos' 'week_day_sin' 'week_day_cos']\n"
     ]
    }
   ],
   "source": [
    "columns_names = df_train.columns.values[1:]\n",
    "print(columns_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "NUM_CLIENTS = user_ids.size\n",
    "#NUM_EPOCHS = 4\n",
    "#PREFETCH_BUFFER = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Split the data into chunks of N\n",
    "def split_data(N):\n",
    "\n",
    "  # dictionary of list of df\n",
    "  df_dictionary = {}\n",
    "\n",
    "  for uid in tqdm(user_ids):\n",
    "    # Get the records of the user\n",
    "    user_df_train = df_train.loc[df_train.user_id == uid].copy()\n",
    "    user_df_val = df_val.loc[df_val.user_id == uid].copy()\n",
    "    user_df_test = df_test.loc[df_test.user_id == uid].copy()\n",
    "\n",
    "    # Get a list of dataframes of length N records\n",
    "    user_list_train = [user_df_train[i:i+N] for i in range(0, user_df_train.shape[0], N)]\n",
    "    user_list_val = [user_df_val[i:i+N] for i in range(0, user_df_val.shape[0], N)]\n",
    "    user_list_test = [user_df_test[i:i+N] for i in range(0, user_df_test.shape[0], N)]\n",
    "\n",
    "    # Save the list of dataframes into a dictionary\n",
    "    df_dictionary[uid] = {\n",
    "        'train': user_list_train,\n",
    "        'val': user_list_val,\n",
    "        'test': user_list_test\n",
    "    }\n",
    "\n",
    "  return  df_dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Takes a dictionary with train, validation and test sets and the desired set type\n",
    "def create_clients_dict(df_dictionary, set_type, N):\n",
    "\n",
    "  dataset_dict = {}\n",
    "\n",
    "  for uid in tqdm(user_ids):\n",
    "\n",
    "    c_data = collections.OrderedDict()\n",
    "    values = df_dictionary[uid][set_type]\n",
    "\n",
    "    # If the last dataframe of the list is not complete #\n",
    "    if len(values[-1]) < N:\n",
    "      diff = 1\n",
    "    else:\n",
    "      diff = 0\n",
    "\n",
    "    if len(values) > 0:\n",
    "      # Create the dictionary to create a clientData\n",
    "      for header in columns_names:\n",
    "        c_data[header] = [values[i][header].values for i in range(0, len(values)-diff)]\n",
    "      dataset_dict[uid] = c_data\n",
    "\n",
    "  return dataset_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# preprocess dataset to tf format\n",
    "def preprocess(dataset, N, hyperparamters):\n",
    "\n",
    "  NUM_EPOCHS = hyperparamters.num_epochs\n",
    "  PREFETCH_BUFFER = hyperparamters.prefetch_buffer_size\n",
    "  BATCH_SIZE = hyperparamters.batch_size\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "\n",
    "    x=collections.OrderedDict()\n",
    "\n",
    "    for name in columns_names:\n",
    "      x[name]=tf.reshape(element[name][:, :-1], [-1, N-1])\n",
    "\n",
    "    y=tf.reshape(element[columns_names[0]][:, 1:], [-1, N-1])\n",
    "\n",
    "    return collections.OrderedDict(x=x, y=y)\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE, drop_remainder=True).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# create federated data for every client\n",
    "def make_federated_data(client_data, client_ids, N, hyper_params_pre):\n",
    "\n",
    "  return [\n",
    "      preprocess(client_data.create_tf_dataset_for_client(x), N, hyper_params_pre)\n",
    "      for x in tqdm(client_ids)\n",
    "  ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Create a model\n",
    "def create_keras_model(number_of_places, N, batch_size, embedding, rnn_units):\n",
    "\n",
    "  # Shortcut to the layers package\n",
    "  l = tf.keras.layers\n",
    "\n",
    "  # List of numeric feature columns to pass to the DenseLayer\n",
    "  numeric_feature_columns = []\n",
    "\n",
    "  # Handling numerical columns\n",
    "  for header in numerical_column_names:\n",
    "\t\t# Append all the numerical columns defined into the list\n",
    "    numeric_feature_columns.append(feature_column.numeric_column(header, shape=N-1))\n",
    "\n",
    "  feature_inputs={}\n",
    "  for c_name in numerical_column_names:\n",
    "    feature_inputs[c_name] = tf.keras.Input((N-1,), batch_size=batch_size, name=c_name)\n",
    "\n",
    "  # We cannot use an array of features as always because we have sequences\n",
    "  # We have to do one by one in order to match the shape\n",
    "  num_features = []\n",
    "  for c_name in numerical_column_names:\n",
    "    f =  feature_column.numeric_column(c_name, shape=(N-1))\n",
    "    feature = l.DenseFeatures(f)(feature_inputs)\n",
    "    feature = tf.expand_dims(feature, -1)\n",
    "    num_features.append(feature)\n",
    "\n",
    "  # Declare the dictionary for the categories sequence as before #\n",
    "  sequence_input = {\n",
    "      'cat_id': tf.keras.Input((N-1,), batch_size=batch_size, dtype=tf.dtypes.int32, name='cat_id') # add batch_size=batch_size in case of stateful GRU\n",
    "  }\n",
    "\n",
    "  # Handling the categorical feature sequence using one-hot\n",
    "  category_one_hot = feature_column.sequence_categorical_column_with_vocabulary_list(\n",
    "      'cat_id', [i for i in range(number_of_places)])\n",
    "\n",
    "  # category = feature_column.categorical_column_with_vocabulary_file(\n",
    "  #    'cat_id', [str(i) for i in range(number_of_places)])\n",
    "\n",
    "  category_encoded = feature_column.indicator_column(category_one_hot)\n",
    "\n",
    "\n",
    "  if embedding:\n",
    "    # Embed the one-hot encoding\n",
    "    category_encoded = feature_column.embedding_column(category_one_hot, 256) # embedding_dim\n",
    "\n",
    "\n",
    "  # With an input sequence we can't use the DenseFeature layer, we need to use the SequenceFeatures\n",
    "  sequence_features, sequence_length = tf.keras.experimental.SequenceFeatures(category_encoded)(sequence_input)\n",
    "\n",
    "\n",
    "  input_sequence = l.Concatenate(axis=2)([sequence_features] + num_features)\n",
    "\n",
    "  # Rnn\n",
    "  recurrent = l.GRU(rnn_units, # rnn_units\n",
    "                        batch_size=batch_size, #in case of stateful\n",
    "                        dropout=0.3,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform')(input_sequence)\n",
    "\n",
    "\n",
    "\t# Last layer with an output for each places\n",
    "  dense_1 = layers.Dense(number_of_places)(recurrent)\n",
    "\n",
    "\t# Softmax output layer\n",
    "  output = l.Softmax()(dense_1)\n",
    "\n",
    "\t# To return the Model, we need to define its inputs and outputs\n",
    "\t# In out case, we need to list all the input layers we have defined\n",
    "  inputs = list(feature_inputs.values()) + list(sequence_input.values())\n",
    "\n",
    "\t# Return the Model\n",
    "  return tf.keras.Model(inputs=inputs, outputs=output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "#train and evaluate the model\n",
    "def train_and_eval_model(vocab_size, n, federated_train_data, federated_val_data, federated_test_data,  hyper_params_model, preprocessed_example_dataset, embedding_y_n, batch_size, path='./log/central-test-run'):\n",
    "  train_logdir = path + '/train'\n",
    "  val_logdir = path + '/val'\n",
    "  eval_logdir = path + '/eval'\n",
    "\n",
    "  num_rounds = hyper_params_model.num_rounds\n",
    "  learning_rate_client = hyper_params_model.learning_rate_client\n",
    "  learning_rate_server = hyper_params_model.learning_rate_server\n",
    "  rnn_units = hyper_params_model.rnn_units\n",
    "  optimizer = hyper_params_model.optimizer\n",
    "\n",
    "  train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
    "  val_summary_writer = tf.summary.create_file_writer(val_logdir)\n",
    "  eval_summary_writer = tf.summary.create_file_writer(eval_logdir)\n",
    "\n",
    "  # Clone the keras_model inside `create_tff_model()`, which TFF will\n",
    "  # call to produce a new copy of the model inside the graph that it will\n",
    "  # serialize. Note: we want to construct all the necessary objects we'll need\n",
    "  # _inside_ this method.\n",
    "  def create_tff_model():\n",
    "    # TFF uses an `input_spec` so it knows the types and shapes\n",
    "    # that your model expects.\n",
    "    input_spec = preprocessed_example_dataset.element_spec\n",
    "    keras_model_clone = create_keras_model(vocab_size, n, batch_size=batch_size, embedding=embedding_y_n, rnn_units=rnn_units)\n",
    "\n",
    "    tff_model = tff.learning.from_keras_model(\n",
    "      keras_model_clone,\n",
    "      input_spec=input_spec,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return tff_model\n",
    "\n",
    "  # This command builds all the TensorFlow graphs and serializes them:\n",
    "\n",
    "  if optimizer == \"Adam\":\n",
    "    fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=learning_rate_client),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=learning_rate_server))\n",
    "\n",
    "  elif optimizer == \"Nadam\":\n",
    "    fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Nadam(learning_rate=learning_rate_client),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Nadam(learning_rate=learning_rate_server))\n",
    "\n",
    "  elif optimizer == \"Adamax\":\n",
    "    fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adamax(learning_rate=learning_rate_client),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Adamax(learning_rate=learning_rate_server))\n",
    "\n",
    "  elif optimizer == \"SGD\":\n",
    "    fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=learning_rate_client),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=learning_rate_server))\n",
    "\n",
    "  elif optimizer == \"Adadelta\":\n",
    "    fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adadelta(learning_rate=learning_rate_client),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Adadelta(learning_rate=learning_rate_server))\n",
    "\n",
    "  elif optimizer == \"Adagrad\":\n",
    "    fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adagrad(learning_rate=learning_rate_client),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Adagrad(learning_rate=learning_rate_server))\n",
    "\n",
    "  elif optimizer == \"Ftrl\":\n",
    "    fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Ftrl(learning_rate=learning_rate_client),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Ftrl(learning_rate=learning_rate_server))\n",
    "\n",
    "  elif optimizer == \"RMSprop\":\n",
    "    fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.RMSprop(learning_rate=learning_rate_client),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.RMSprop(learning_rate=learning_rate_server))\n",
    "\n",
    "\n",
    "  state = fed_avg.initialize()\n",
    "  evaluation = tff.learning.build_federated_evaluation(model_fn=create_tff_model)\n",
    "  val_metrics = evaluation(state.model, federated_val_data)\n",
    "\n",
    "  tolerance = 7\n",
    "  best_state = 0\n",
    "  lowest_loss = 100.00\n",
    "  stop = tolerance\n",
    "\n",
    "  NUM_ROUNDS = 10 #40\n",
    "  with train_summary_writer.as_default():\n",
    "    for round_num in range(1, num_rounds + 1):\n",
    "      print('Round {r}'.format(r=round_num))\n",
    "\n",
    "      # Uncomment to simulate sparse availability of clients\n",
    "      # train_data_for_this_round, val_data_for_this_round = sample((federated_train_data, federated_val_data), 20, NUM_CLIENTS)\n",
    "\n",
    "      state, metrics = fed_avg.next(state, federated_train_data)\n",
    "\n",
    "      train_metrics = metrics['train']\n",
    "      print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "      val_metrics = evaluation(state.model, federated_val_data)\n",
    "      print('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "      # Check for decreasing validation loss\n",
    "      if lowest_loss > val_metrics['loss']:\n",
    "        print('\\tSaving best model..')\n",
    "        lowest_loss = val_metrics['loss']\n",
    "        best_state = state\n",
    "        stop = tolerance - 1\n",
    "      else:\n",
    "        stop = stop - 1\n",
    "        if stop <= 0:\n",
    "          print('\\tEarly stopping...')\n",
    "          break;\n",
    "\n",
    "      print(' ')\n",
    "      print('\\twriting..')\n",
    "\n",
    "      # Iterate across the metrics and write their data\n",
    "      for name, value in dict(train_metrics).items():\n",
    "        tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "      with val_summary_writer.as_default():\n",
    "        for name, value in dict(val_metrics).items():\n",
    "          tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "  train_summary_writer.close()\n",
    "  val_summary_writer.close()\n",
    "\n",
    "  # evaluate over test data\n",
    "  test_metrics = evaluation(best_state.model, federated_test_data)\n",
    "  log('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))\n",
    "  return test_metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 711.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating client dictionaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 146.75it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 513.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 735.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n=17\n",
    "print('Splitting data...')\n",
    "df_dict = split_data(n)\n",
    "print('Creating client dictionaries...')\n",
    "clients_train_dict = create_clients_dict(df_dict, 'train', n)\n",
    "clients_val_dict = create_clients_dict(df_dict, 'val', n)\n",
    "clients_test_dict = create_clients_dict(df_dict, 'test', n)\n",
    "\n",
    "# Convert the dictionary to a dataset\n",
    "print('Converting to datasets...')\n",
    "client_train_data = tff.simulation.FromTensorSlicesClientData(clients_train_dict)\n",
    "client_val_data = tff.simulation.FromTensorSlicesClientData(clients_val_dict)\n",
    "client_test_data = tff.simulation.FromTensorSlicesClientData(clients_test_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def execute_hyper_tuning(hyper_params_pre, hyper_params_model, embedding_y_n):\n",
    "  example_dataset = client_train_data.create_tf_dataset_for_client(client_train_data.client_ids[1])\n",
    "  preprocessed_example_dataset = preprocess(example_dataset, n, hyper_params_pre)\n",
    "\n",
    "  # Select the clients\n",
    "  sample_clients = client_train_data.client_ids[0:NUM_CLIENTS]\n",
    "\n",
    "  # Federate the clients datasets\n",
    "  print('Federating datasets...')\n",
    "  federated_train_data = make_federated_data(client_train_data, sample_clients, n, hyper_params_pre)\n",
    "  federated_val_data = make_federated_data(client_val_data, sample_clients, n, hyper_params_pre)\n",
    "  federated_test_data = make_federated_data(client_test_data, sample_clients, n, hyper_params_pre)\n",
    "\n",
    "  print('Training...')\n",
    "  p=\"'./log/central-test-run_%d\" % n\n",
    "  eval_result=train_and_eval_model(vocab_size, n, federated_train_data, federated_val_data, federated_test_data, path=p, hyper_params_model=hyper_params_model, preprocessed_example_dataset=preprocessed_example_dataset, embedding_y_n=embedding_y_n, batch_size=hyper_params_pre.batch_size)\n",
    "  return eval_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "#Hyperparamters\n",
    "#Embedding vs One-Hot Encoding\n",
    "embeddings = [True, False]\n",
    "#Preprocessing Parameters\n",
    "prefetch_buffer_sizes = [2, 4, 8]\n",
    "batch_sizes = [8, 16, 32]\n",
    "num_epochs = [2, 5, 7]\n",
    "#Model Parameters\n",
    "optimizers = [\"Adam\", \"Nadam\", \"Adamax\"]\n",
    "learning_rates_server = [0.06, 0.6, 0.006]\n",
    "learning_rates_client = [0.001, 0.002, 0.003]\n",
    "num_rnn_units = [64, 128, 256]\n",
    "num_rounds = 2\n",
    "\n",
    "parameter_arr_pre = [prefetch_buffer_sizes, batch_sizes, num_epochs]\n",
    "parameter_arr_model = [optimizers, learning_rates_server, learning_rates_client, num_rnn_units]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=8; num_epochs=2;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 31.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 35.72it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 33.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=2.388, accuracy=0.416\n",
      "\tValidation: loss=3.261, accuracy=0.073\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=2.352, accuracy=0.447\n",
      "\tValidation: loss=3.231, accuracy=0.105\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.248, accuracy=0.071\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=8; num_epochs=2;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 33.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 35.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 30.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=2.951, accuracy=0.260\n",
      "\tValidation: loss=3.289, accuracy=0.025\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=2.940, accuracy=0.262\n",
      "\tValidation: loss=3.277, accuracy=0.030\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.298, accuracy=0.025\n",
      "Embedding better than One-Hot?: False\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=8; num_epochs=2;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 35.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 28.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=2.931, accuracy=0.264\n",
      "\tValidation: loss=3.299, accuracy=0.038\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=2.924, accuracy=0.267\n",
      "\tValidation: loss=3.286, accuracy=0.043\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.281, accuracy=0.039\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=8; num_epochs=5;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00, 24.48it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 33.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 33.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=1.685, accuracy=0.557\n",
      "\tValidation: loss=3.229, accuracy=0.054\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=1.709, accuracy=0.553\n",
      "\tValidation: loss=3.217, accuracy=0.066\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.247, accuracy=0.066\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=8; num_epochs=7;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 31.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00, 24.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 27.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=1.300, accuracy=0.653\n",
      "\tValidation: loss=3.275, accuracy=0.061\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=1.299, accuracy=0.654\n",
      "\tValidation: loss=3.261, accuracy=0.070\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.264, accuracy=0.054\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=16; num_epochs=2;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00, 23.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=2.769, accuracy=0.282\n",
      "\tValidation: loss=3.308, accuracy=0.049\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=2.756, accuracy=0.286\n",
      "\tValidation: loss=3.292, accuracy=0.066\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.332, accuracy=0.036\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=16; num_epochs=5;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 35.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 25.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=2.030, accuracy=0.491\n",
      "\tValidation: loss=3.328, accuracy=0.033\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=2.019, accuracy=0.493\n",
      "\tValidation: loss=3.315, accuracy=0.038\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.310, accuracy=0.026\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=16; num_epochs=7;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 31.64it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 19.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=1.637, accuracy=0.577\n",
      "\tValidation: loss=3.338, accuracy=0.020\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=1.631, accuracy=0.578\n",
      "\tValidation: loss=3.323, accuracy=0.025\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.317, accuracy=0.018\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=32; num_epochs=2;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=3.095, accuracy=0.127\n",
      "\tValidation: loss=3.258, accuracy=0.049\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=3.079, accuracy=0.134\n",
      "\tValidation: loss=3.238, accuracy=0.055\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.193, accuracy=0.142\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=32; num_epochs=5;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=2.291, accuracy=0.424\n",
      "\tValidation: loss=3.341, accuracy=0.012\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=2.289, accuracy=0.420\n",
      "\tValidation: loss=3.326, accuracy=0.014\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.326, accuracy=0.017\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=2; batch_size=32; num_epochs=7;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 29.56it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.23it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=2.036, accuracy=0.490\n",
      "\tValidation: loss=3.331, accuracy=0.034\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=2.037, accuracy=0.488\n",
      "\tValidation: loss=3.316, accuracy=0.041\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.318, accuracy=0.034\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=4; batch_size=8; num_epochs=2;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.15it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=2.908, accuracy=0.278\n",
      "\tValidation: loss=3.289, accuracy=0.040\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=2.914, accuracy=0.281\n",
      "\tValidation: loss=3.276, accuracy=0.048\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.294, accuracy=0.030\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=4; batch_size=8; num_epochs=5;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 31.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 33.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=1.734, accuracy=0.548\n",
      "\tValidation: loss=3.268, accuracy=0.064\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=1.743, accuracy=0.549\n",
      "\tValidation: loss=3.255, accuracy=0.075\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.298, accuracy=0.055\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=4; batch_size=8; num_epochs=7;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 31.60it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=1.364, accuracy=0.642\n",
      "\tValidation: loss=3.287, accuracy=0.058\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=1.363, accuracy=0.643\n",
      "\tValidation: loss=3.274, accuracy=0.062\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.287, accuracy=0.060\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=4; batch_size=16; num_epochs=2;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 31.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 33.87it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 33.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=2.715, accuracy=0.281\n",
      "\tValidation: loss=3.260, accuracy=0.027\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=2.703, accuracy=0.282\n",
      "\tValidation: loss=3.245, accuracy=0.032\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.271, accuracy=0.040\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=4; batch_size=16; num_epochs=5;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 30.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=1.973, accuracy=0.506\n",
      "\tValidation: loss=3.266, accuracy=0.046\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=1.941, accuracy=0.509\n",
      "\tValidation: loss=3.251, accuracy=0.051\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.272, accuracy=0.042\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=4; batch_size=16; num_epochs=7;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 30.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.03it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 30.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=1.616, accuracy=0.583\n",
      "\tValidation: loss=3.318, accuracy=0.045\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=1.617, accuracy=0.583\n",
      "\tValidation: loss=3.303, accuracy=0.049\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.297, accuracy=0.051\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=4; batch_size=32; num_epochs=2;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 30.75it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 32.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 31.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Round 1\n",
      "\tTrain: loss=3.108, accuracy=0.130\n",
      "\tValidation: loss=3.296, accuracy=0.051\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=3.077, accuracy=0.134\n",
      "\tValidation: loss=3.278, accuracy=0.053\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.182, accuracy=0.191\n",
      "Run for rounds=2, \n",
      " Preprocessing: prefetch_buffer=4; batch_size=32; num_epochs=5;\n",
      " Model: optimizer:Adam; learning_rate_server=0.001000; learning_rate_client=0.060000; rnn_units=64\n",
      "Federating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 30.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 31.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_1428\\649927442.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     41\u001B[0m       log('Run for rounds=%d, \\n Preprocessing: prefetch_buffer=%d; batch_size=%d; num_epochs=%d;'\n\u001B[0;32m     42\u001B[0m           '\\n Model: optimizer:%s; learning_rate_server=%f; learning_rate_client=%f; rnn_units=%d' % (num_rounds, hyper_params_pre.prefetch_buffer_size, hyper_params_pre.batch_size, hyper_params_pre.num_epochs, hyper_params_model.optimizer, hyper_params_model.learning_rate_server, hyper_params_model.learning_rate_client, hyper_params_model.rnn_units))\n\u001B[1;32m---> 43\u001B[1;33m       \u001B[0meval_result\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mexecute_hyper_tuning\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhyper_params_pre\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhyper_params_pre\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhyper_params_model\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhyper_params_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding_y_n\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0membedding_y_n\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     44\u001B[0m       \u001B[0meval_results_pre\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0meval_result\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_1428\\2372360345.py\u001B[0m in \u001B[0;36mexecute_hyper_tuning\u001B[1;34m(hyper_params_pre, hyper_params_model, embedding_y_n)\u001B[0m\n\u001B[0;32m     14\u001B[0m   \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Training...'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m   \u001B[0mp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"'./log/central-test-run_%d\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m   \u001B[0meval_result\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_and_eval_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfederated_train_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfederated_val_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfederated_test_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhyper_params_model\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhyper_params_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpreprocessed_example_dataset\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpreprocessed_example_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding_y_n\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0membedding_y_n\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhyper_params_pre\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m   \u001B[1;32mreturn\u001B[0m \u001B[0meval_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_1428\\983053685.py\u001B[0m in \u001B[0;36mtrain_and_eval_model\u001B[1;34m(vocab_size, n, federated_train_data, federated_val_data, federated_test_data, hyper_params_model, preprocessed_example_dataset, embedding_y_n, batch_size, path)\u001B[0m\n\u001B[0;32m     85\u001B[0m   \u001B[0mstate\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfed_avg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minitialize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m   \u001B[0mevaluation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtff\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlearning\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuild_federated_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_fn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_tff_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 87\u001B[1;33m   \u001B[0mval_metrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mevaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfederated_val_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     88\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m   \u001B[0mtolerance\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m7\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\utils\\function_utils.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    519\u001B[0m     \u001B[0mcontext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_context_stack\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcurrent\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    520\u001B[0m     \u001B[0marg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpack_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_type_signature\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 521\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minvoke\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    522\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    523\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m__hash__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\retrying.py\u001B[0m in \u001B[0;36mwrapped_f\u001B[1;34m(*args, **kw)\u001B[0m\n\u001B[0;32m     54\u001B[0m             \u001B[1;33m@\u001B[0m\u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwraps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m             \u001B[1;32mdef\u001B[0m \u001B[0mwrapped_f\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mRetrying\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mdargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mdkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mwrapped_f\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\retrying.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    256\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_reject\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattempt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 257\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mattempt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrap_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    258\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    259\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_after_attempts\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\retrying.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(self, wrap_exception)\u001B[0m\n\u001B[0;32m    299\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mRetryError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    300\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 301\u001B[1;33m                 \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreraise\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    302\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    303\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\six.py\u001B[0m in \u001B[0;36mreraise\u001B[1;34m(tp, value, tb)\u001B[0m\n\u001B[0;32m    717\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mtb\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    718\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 719\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    720\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    721\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\retrying.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    250\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 251\u001B[1;33m                 \u001B[0mattempt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAttempt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattempt_number\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    252\u001B[0m             \u001B[1;32mexcept\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    253\u001B[0m                 \u001B[0mtb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexc_info\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\executors\\execution_context.py\u001B[0m in \u001B[0;36minvoke\u001B[1;34m(self, comp, arg)\u001B[0m\n\u001B[0;32m    184\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_compiler_pipeline\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    185\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mtracing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'ExecutionContext'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Compile'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mspan\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 186\u001B[1;33m         \u001B[0mcomp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_compiler_pipeline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    187\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    188\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtracing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'ExecutionContext'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Invoke'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mspan\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\compiler_pipeline.py\u001B[0m in \u001B[0;36mcompile\u001B[1;34m(self, computation_to_compile)\u001B[0m\n\u001B[0;32m     47\u001B[0m     py_typecheck.check_type(computation_to_compile,\n\u001B[0;32m     48\u001B[0m                             computation_base.Computation)\n\u001B[1;32m---> 49\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_compilation_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomputation_to_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\backends\\native\\compiler.py\u001B[0m in \u001B[0;36mtransform_to_native_form\u001B[1;34m(comp)\u001B[0m\n\u001B[0;32m     44\u001B[0m     \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Compiling TFF computation.'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m     call_dominant_form, _ = transformations.transform_to_call_dominant(\n\u001B[1;32m---> 46\u001B[1;33m         computation_building_block)\n\u001B[0m\u001B[0;32m     47\u001B[0m     \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Computation compiled to:'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcall_dominant_form\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformatted_representation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformations.py\u001B[0m in \u001B[0;36mtransform_to_call_dominant\u001B[1;34m(comp)\u001B[0m\n\u001B[0;32m    846\u001B[0m       \u001B[0mtree_transformations\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muniquify_reference_names\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    847\u001B[0m   ]:\n\u001B[1;32m--> 848\u001B[1;33m     \u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransformed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    849\u001B[0m     \u001B[0mmodified\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodified\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mtransformed\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    850\u001B[0m   \u001B[1;32mreturn\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodified\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\tree_transformations.py\u001B[0m in \u001B[0;36mremove_duplicate_block_locals\u001B[1;34m(comp)\u001B[0m\n\u001B[0;32m   1128\u001B[0m       transformation_utils.TrackRemovedReferences)\n\u001B[0;32m   1129\u001B[0m   return transformation_utils.transform_postorder_with_symbol_bindings(\n\u001B[1;32m-> 1130\u001B[1;33m       comp, _transform, symbol_tree)\n\u001B[0m\u001B[0;32m   1131\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36mtransform_postorder_with_symbol_bindings\u001B[1;34m(comp, transform, symbol_tree)\u001B[0m\n\u001B[0;32m    388\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    389\u001B[0m   return _transform_postorder_with_symbol_bindings_switch(\n\u001B[1;32m--> 390\u001B[1;33m       comp, transform, symbol_tree, identifier_seq)\n\u001B[0m\u001B[0;32m    391\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    392\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_transform_postorder_with_symbol_bindings_switch\u001B[1;34m(comp, transform_fn, ctxt_tree, identifier_sequence)\u001B[0m\n\u001B[0;32m    294\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_lambda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    295\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_block\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 296\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_block\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    297\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    298\u001B[0m       raise NotImplementedError(\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_traverse_block\u001B[1;34m(comp, transform, context_tree, identifier_seq)\u001B[0m\n\u001B[0;32m    379\u001B[0m       \u001B[0mvariables_modified\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvariables_modified\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mvalue_modified\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    380\u001B[0m     result, result_modified = _transform_postorder_with_symbol_bindings_switch(\n\u001B[1;32m--> 381\u001B[1;33m         comp.result, transform, context_tree, identifier_seq)\n\u001B[0m\u001B[0;32m    382\u001B[0m     \u001B[0mcontext_tree\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwalk_to_scope_beginning\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    383\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mvariables_modified\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mresult_modified\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_transform_postorder_with_symbol_bindings_switch\u001B[1;34m(comp, transform_fn, ctxt_tree, identifier_sequence)\u001B[0m\n\u001B[0;32m    292\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_lambda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 294\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_lambda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    295\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_block\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    296\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_block\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_traverse_lambda\u001B[1;34m(comp, transform, context_tree, identifier_seq)\u001B[0m\n\u001B[0;32m    357\u001B[0m     \u001B[0mcontext_tree\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mingest_variable_binding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameter_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    358\u001B[0m     result, result_modified = _transform_postorder_with_symbol_bindings_switch(\n\u001B[1;32m--> 359\u001B[1;33m         comp.result, transform, context_tree, identifier_seq)\n\u001B[0m\u001B[0;32m    360\u001B[0m     \u001B[0mcontext_tree\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwalk_to_scope_beginning\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    361\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mresult_modified\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_transform_postorder_with_symbol_bindings_switch\u001B[1;34m(comp, transform_fn, ctxt_tree, identifier_sequence)\u001B[0m\n\u001B[0;32m    294\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_lambda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    295\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_block\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 296\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_block\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    297\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    298\u001B[0m       raise NotImplementedError(\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_traverse_block\u001B[1;34m(comp, transform, context_tree, identifier_seq)\u001B[0m\n\u001B[0;32m    374\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlocals\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    375\u001B[0m       value, value_modified = _transform_postorder_with_symbol_bindings_switch(\n\u001B[1;32m--> 376\u001B[1;33m           value, transform, context_tree, identifier_seq)\n\u001B[0m\u001B[0;32m    377\u001B[0m       \u001B[0mcontext_tree\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mingest_variable_binding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    378\u001B[0m       \u001B[0mvariables\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_transform_postorder_with_symbol_bindings_switch\u001B[1;34m(comp, transform_fn, ctxt_tree, identifier_sequence)\u001B[0m\n\u001B[0;32m    290\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_tuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    291\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 292\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    293\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_lambda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    294\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_lambda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_traverse_call\u001B[1;34m(comp, transform, context_tree, identifier_seq)\u001B[0m\n\u001B[0;32m    343\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margument\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    344\u001B[0m       arg, arg_modified = _transform_postorder_with_symbol_bindings_switch(\n\u001B[1;32m--> 345\u001B[1;33m           comp.argument, transform, context_tree, identifier_seq)\n\u001B[0m\u001B[0;32m    346\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    347\u001B[0m       \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg_modified\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_transform_postorder_with_symbol_bindings_switch\u001B[1;34m(comp, transform_fn, ctxt_tree, identifier_sequence)\u001B[0m\n\u001B[0;32m    288\u001B[0m                                  identifier_sequence)\n\u001B[0;32m    289\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_struct\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 290\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_tuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    291\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    292\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_traverse_tuple\u001B[1;34m(comp, transform, context_tree, identifier_seq)\u001B[0m\n\u001B[0;32m    328\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstructure\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miter_elements\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    329\u001B[0m       value, value_modified = _transform_postorder_with_symbol_bindings_switch(\n\u001B[1;32m--> 330\u001B[1;33m           value, transform, context_tree, identifier_seq)\n\u001B[0m\u001B[0;32m    331\u001B[0m       \u001B[0melements\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    332\u001B[0m       \u001B[0melements_modified\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0melements_modified\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mvalue_modified\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_transform_postorder_with_symbol_bindings_switch\u001B[1;34m(comp, transform_fn, ctxt_tree, identifier_sequence)\u001B[0m\n\u001B[0;32m    283\u001B[0m     if (comp.is_compiled_computation() or comp.is_data() or\n\u001B[0;32m    284\u001B[0m         comp.is_intrinsic() or comp.is_placement() or comp.is_reference()):\n\u001B[1;32m--> 285\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0m_traverse_leaf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctxt_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_sequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    286\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_selection\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    287\u001B[0m       return _traverse_selection(comp, transform, ctxt_tree,\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36m_traverse_leaf\u001B[1;34m(comp, transform, context_tree, identifier_seq)\u001B[0m\n\u001B[0;32m    302\u001B[0m     \u001B[1;34m\"\"\"Helper function holding traversal logic for leaf nodes.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    303\u001B[0m     \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0midentifier_seq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 304\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontext_tree\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    305\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    306\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_traverse_selection\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontext_tree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midentifier_seq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\tree_transformations.py\u001B[0m in \u001B[0;36m_transform\u001B[1;34m(comp, symbol_tree)\u001B[0m\n\u001B[0;32m   1114\u001B[0m           \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnew_value\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1115\u001B[0m       payloads_with_value = symbol_tree.get_all_payloads_with_value(\n\u001B[1;32m-> 1116\u001B[1;33m           value, tree_analysis.trees_equal)\n\u001B[0m\u001B[0;32m   1117\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mpayloads_with_value\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1118\u001B[0m         \u001B[0mhighest_payload\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpayloads_with_value\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\transformation_utils.py\u001B[0m in \u001B[0;36mget_all_payloads_with_value\u001B[1;34m(self, value, equal_fn)\u001B[0m\n\u001B[0;32m    461\u001B[0m     \u001B[0mcomp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtyping\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSequentialBindingNode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mactive_node\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    462\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparent\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0molder_sibling\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 463\u001B[1;33m       \u001B[1;32mif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpayload\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mequal_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpayload\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    464\u001B[0m         \u001B[0mpayloads\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpayload\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    465\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mcomp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0molder_sibling\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\tree_analysis.py\u001B[0m in \u001B[0;36mtrees_equal\u001B[1;34m(comp_1, comp_2)\u001B[0m\n\u001B[0;32m    560\u001B[0m     \u001B[1;32mraise\u001B[0m \u001B[0mNotImplementedError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Unexpected type found: {}.'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp_1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    561\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 562\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0m_trees_equal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp_1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcomp_2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    563\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    564\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\tree_analysis.py\u001B[0m in \u001B[0;36m_trees_equal\u001B[1;34m(comp_1, comp_2, reference_equivalences)\u001B[0m\n\u001B[0;32m    525\u001B[0m                            reference_equivalences) and\n\u001B[0;32m    526\u001B[0m               _trees_equal(comp_1.argument, comp_2.argument,\n\u001B[1;32m--> 527\u001B[1;33m                            reference_equivalences))\n\u001B[0m\u001B[0;32m    528\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp_1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_compiled_computation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_compiled_comp_equal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp_1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcomp_2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\tree_analysis.py\u001B[0m in \u001B[0;36m_trees_equal\u001B[1;34m(comp_1, comp_2, reference_equivalences)\u001B[0m\n\u001B[0;32m    555\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    556\u001B[0m       \u001B[1;32mfor\u001B[0m \u001B[0melement_1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0melement_2\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp_1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcomp_2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 557\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0m_trees_equal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melement_1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0melement_2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreference_equivalences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    558\u001B[0m           \u001B[1;32mreturn\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    559\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\compiler\\tree_analysis.py\u001B[0m in \u001B[0;36m_trees_equal\u001B[1;34m(comp_1, comp_2, reference_equivalences)\u001B[0m\n\u001B[0;32m    527\u001B[0m                            reference_equivalences))\n\u001B[0;32m    528\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp_1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_compiled_computation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 529\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0m_compiled_comp_equal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomp_1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcomp_2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    530\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mcomp_1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    531\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mcomp_1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muri\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mcomp_2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muri\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "eval_results_embedding = []\n",
    "def best_hyper_param(result_array):\n",
    "  best_loss = -1\n",
    "  best_accuracy = -1\n",
    "  best_param = -1\n",
    "  i = 0\n",
    "  for result in result_array:\n",
    "    l=result[0]['loss']\n",
    "    a=result[0]['sparse_categorical_accuracy']\n",
    "    if l < best_loss and a > best_accuracy:\n",
    "      best_loss = l\n",
    "      best_accuracy = a\n",
    "      best_param = i\n",
    "      i += 1\n",
    "  return best_param\n",
    "\n",
    "#Test Embedding vs One-Hot\n",
    "for embedding in embeddings:\n",
    "   hyper_params_pre = HyperParameterPreprocessing(parameter_arr_pre[0][0],parameter_arr_pre[1][0],parameter_arr_pre[2][0])\n",
    "   hyper_params_model = HyperParameterModel(parameter_arr_model[0][0], parameter_arr_model[1][0], parameter_arr_model[2][0], parameter_arr_model[3][0], num_rounds=num_rounds)\n",
    "   log('Run for rounds=%d, \\n Preprocessing: prefetch_buffer=%d; batch_size=%d; num_epochs=%d;'\n",
    "       '\\n Model: optimizer:%s; learning_rate_server=%f; learning_rate_client=%f; rnn_units=%d' % (num_rounds, hyper_params_pre.prefetch_buffer_size, hyper_params_pre.batch_size, hyper_params_pre.num_epochs, hyper_params_model.optimizer, hyper_params_model.learning_rate_server, hyper_params_model.learning_rate_client, hyper_params_model.rnn_units))\n",
    "\n",
    "   eval_result=execute_hyper_tuning(hyper_params_pre=hyper_params_pre, hyper_params_model=hyper_params_model, embedding_y_n=embedding)\n",
    "   eval_results_embedding.append([eval_result])\n",
    "\n",
    "embedding_y_n_result=best_hyper_param(eval_results_embedding)\n",
    "embedding_y_n = embeddings[embedding_y_n_result]\n",
    "log('Embedding better than One-Hot?: %r' % embedding_y_n)\n",
    "\n",
    "eval_results_pre = []\n",
    "\n",
    "p = 0\n",
    "b = 0\n",
    "e = 0\n",
    "for p_size in prefetch_buffer_sizes:\n",
    "  for b_size in batch_sizes:\n",
    "    for number in num_epochs:\n",
    "      hyper_params_pre = HyperParameterPreprocessing(p_size,b_size,number)\n",
    "      hyper_params_model = HyperParameterModel(parameter_arr_model[0][0], parameter_arr_model[1][0], parameter_arr_model[2][0], parameter_arr_model[3][0], num_rounds=num_rounds)\n",
    "      log('Run for rounds=%d, \\n Preprocessing: prefetch_buffer=%d; batch_size=%d; num_epochs=%d;'\n",
    "          '\\n Model: optimizer:%s; learning_rate_server=%f; learning_rate_client=%f; rnn_units=%d' % (num_rounds, hyper_params_pre.prefetch_buffer_size, hyper_params_pre.batch_size, hyper_params_pre.num_epochs, hyper_params_model.optimizer, hyper_params_model.learning_rate_server, hyper_params_model.learning_rate_client, hyper_params_model.rnn_units))\n",
    "      eval_result=execute_hyper_tuning(hyper_params_pre=hyper_params_pre, hyper_params_model=hyper_params_model, embedding_y_n=embedding_y_n)\n",
    "      eval_results_pre.append([eval_result, [p,b,e]])\n",
    "\n",
    "      e+=1\n",
    "    b+=1\n",
    "  p+=1\n",
    "\n",
    "best_prep_combination=best_hyper_param(eval_results_pre)\n",
    "best_prefetch_size = eval_results_pre[best_prep_combination][0]\n",
    "best_buffer_size = eval_results_pre[best_prep_combination][1]\n",
    "best_num_epochs = eval_results_pre[best_prep_combination][2]\n",
    "log('Best combination for preprocessing: prefetch_size:%d; buffer_size:%d; num_epochs:%d' % (best_prefetch_size, best_buffer_size, best_num_epochs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_results_model = []\n",
    "\n",
    "o = 0\n",
    "s = 0\n",
    "c = 0\n",
    "r = 0\n",
    "for optimizer in optimizers:\n",
    "  for lr_server in learning_rates_client:\n",
    "    for lr_client in learning_rates_server:\n",
    "      for rnn_units in num_rnn_units:\n",
    "        hyper_params_pre = HyperParameterPreprocessing(best_prefetch_size,best_buffer_size,best_num_epochs)\n",
    "        hyper_params_model = HyperParameterModel(optimizer=optimizer, learning_rate_server=lr_server, learning_rate_client=lr_client,rnn_units=rnn_units, num_rounds=num_rounds)\n",
    "        log('Run for rounds=%d, \\n Preprocessing: prefetch_buffer=%d; batch_size=%d; num_epochs=%d;'\n",
    "          '\\n Model: optimizer:%s; learning_rate_server=%f; learning_rate_client=%f; rnn_units=%d' % (num_rounds, hyper_params_pre.prefetch_buffer_size, hyper_params_pre.batch_size, hyper_params_pre.num_epochs, hyper_params_model.optimizer, hyper_params_model.learning_rate_server, hyper_params_model.learning_rate_client, hyper_params_model.rnn_units))\n",
    "        eval_result=execute_hyper_tuning(hyper_params_pre=hyper_params_pre, hyper_params_model=hyper_params_model, embedding_y_n=embedding_y_n)\n",
    "        eval_results_model.append([eval_result, [o,s,c,r]])\n",
    "        r+=1\n",
    "      c+=1\n",
    "    s+=1\n",
    "  r+=1\n",
    "\n",
    "best_model_combination=best_hyper_param(eval_results_model)\n",
    "best_optimizer = eval_results_model[best_model_combination][0]\n",
    "best_learning_rate_server = eval_results_model[best_model_combination][1]\n",
    "best_learning_rate_client = eval_results_model[best_model_combination][2]\n",
    "best_rnn_units = eval_results_model[best_model_combination][3]\n",
    "log('Best combination for the model: optimizer:%s; learning_rate_server=%f; learning_rate_client=%f; rnn_units=%d' % (best_optimizer, best_learning_rate_server, best_learning_rate_client, best_rnn_units))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#for rounds in num_rounds:\n",
    "#  for embedding in embeddings:\n",
    "#    for batch_size in batch_sizes:\n",
    "#      for rnn_units in num_rnn_units:\n",
    "#        for learning_rate_client in learning_rates_client:\n",
    "#          for learning_rate_server in learning_rates_server:\n",
    "#            for optimizer in optimizers:\n",
    "#              hyper_params = HyperParameter(num_rounds=rounds, batch_size=batch_size, rnn_units=rnn_units, learning_rate_client=learning_rate_client, learning_rate_server=learning_rate_server, embedding=embedding, optimizer=optimizer)\n",
    "#              log('Run for rounds=%d, batch_size=%d, rnn_units=%d, learning_rate_client=%f, learning_rate_server=%f' % (rounds, batch_size, rnn_units, learning_rate_client, learning_rate_server))\n",
    "#              execute_hyper_tuning(hyper_params=hyper_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
