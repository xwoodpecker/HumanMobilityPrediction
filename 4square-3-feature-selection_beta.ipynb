{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 3 - Feature Selection\n",
    "\n",
    "A central model for tff has been found.\n",
    "The model with all features proved to be less accurate.\n",
    "The next step is to evaluate which features should be selected for the best prediction quality.\n",
    "This is done by training models on all possible feature subsets and comparing the results.\n",
    "The most important features are to be expected:\n",
    "\n",
    "* the temporal features (all time components including is_weekday)\n",
    "* user id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=\"./log/feature-selection/Evaluation.log\", level=logging.INFO)\n",
    "\n",
    "def log(text):\n",
    "  print(text)\n",
    "  logging.info(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "b'Hello, World!'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the TFF is working:\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Selection\n",
    "\n",
    "Several models are trained on subsets of the full dataset to check which features are most beneficial for the prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "    cat_id  user_id   latitude  longitude  is_weekend  clock_sin  clock_cos  \\\n0        0      470  40.719810 -74.002581       False  -1.000000   0.000654   \n1        1      979  40.606800 -74.044170       False  -0.999998   0.001818   \n2        2       69  40.716162 -73.883070       False  -0.999945   0.010472   \n3        3      395  40.745164 -73.982519       False  -0.999931   0.011708   \n4        4       87  40.740104 -73.989658       False  -0.999914   0.013090   \n..     ...      ...        ...        ...         ...        ...        ...   \n95       7      445  40.828602 -73.879259       False  -0.959601   0.281365   \n96       6      235  40.745463 -73.990983       False  -0.956326   0.292302   \n97       8      118  40.600144 -73.946593       False  -0.955729   0.294249   \n98       2     1054  40.870630 -74.097926       False  -0.955407   0.295291   \n99      15      881  40.808700 -73.958515       False  -0.954631   0.297791   \n\n     day_sin   day_cos  month_sin  month_cos  week_day_sin  week_day_cos  \\\n0   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n1   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n2   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n3   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n4   0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n..       ...       ...        ...        ...           ...           ...   \n95  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n96  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n97  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n98  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n99  0.587785  0.809017   0.866025       -0.5      0.781831       0.62349   \n\n    venue_id  orig_cat_id  \n0          0            0  \n1          1            1  \n2          2            2  \n3          3            3  \n4          4            4  \n..       ...          ...  \n95        93           24  \n96        94            6  \n97        95           57  \n98        96           58  \n99        97           21  \n\n[100 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat_id</th>\n      <th>user_id</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>is_weekend</th>\n      <th>clock_sin</th>\n      <th>clock_cos</th>\n      <th>day_sin</th>\n      <th>day_cos</th>\n      <th>month_sin</th>\n      <th>month_cos</th>\n      <th>week_day_sin</th>\n      <th>week_day_cos</th>\n      <th>venue_id</th>\n      <th>orig_cat_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>470</td>\n      <td>40.719810</td>\n      <td>-74.002581</td>\n      <td>False</td>\n      <td>-1.000000</td>\n      <td>0.000654</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>979</td>\n      <td>40.606800</td>\n      <td>-74.044170</td>\n      <td>False</td>\n      <td>-0.999998</td>\n      <td>0.001818</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>69</td>\n      <td>40.716162</td>\n      <td>-73.883070</td>\n      <td>False</td>\n      <td>-0.999945</td>\n      <td>0.010472</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>395</td>\n      <td>40.745164</td>\n      <td>-73.982519</td>\n      <td>False</td>\n      <td>-0.999931</td>\n      <td>0.011708</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>87</td>\n      <td>40.740104</td>\n      <td>-73.989658</td>\n      <td>False</td>\n      <td>-0.999914</td>\n      <td>0.013090</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>7</td>\n      <td>445</td>\n      <td>40.828602</td>\n      <td>-73.879259</td>\n      <td>False</td>\n      <td>-0.959601</td>\n      <td>0.281365</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>93</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>6</td>\n      <td>235</td>\n      <td>40.745463</td>\n      <td>-73.990983</td>\n      <td>False</td>\n      <td>-0.956326</td>\n      <td>0.292302</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>94</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>8</td>\n      <td>118</td>\n      <td>40.600144</td>\n      <td>-73.946593</td>\n      <td>False</td>\n      <td>-0.955729</td>\n      <td>0.294249</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>95</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>2</td>\n      <td>1054</td>\n      <td>40.870630</td>\n      <td>-74.097926</td>\n      <td>False</td>\n      <td>-0.955407</td>\n      <td>0.295291</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>96</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>15</td>\n      <td>881</td>\n      <td>40.808700</td>\n      <td>-73.958515</td>\n      <td>False</td>\n      <td>-0.954631</td>\n      <td>0.297791</td>\n      <td>0.587785</td>\n      <td>0.809017</td>\n      <td>0.866025</td>\n      <td>-0.5</td>\n      <td>0.781831</td>\n      <td>0.62349</td>\n      <td>97</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./4square/processed_transformed_big.csv\")\n",
    "df.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is best, to use only the best 100 users for this purpose.\n",
    "As they have the longest sequences of visited places."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "count = df.user_id.value_counts()\n",
    "\n",
    "idx = count.loc[count.index[:100]].index # count >= 100\n",
    "df = df.loc[df.user_id.isin(idx)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An array is created containing all visited locations for every user.\n",
    "The original data is sorted by time (ascending).\n",
    "Thus, the array contains a sequence of visited location categories by user."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1724.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# List the df for each user\n",
    "users_locations = []\n",
    "\n",
    "# For each user\n",
    "for user_id in tqdm(idx):\n",
    "  users_locations.append(df.loc[df.user_id == user_id].copy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is necessary to first split the data in train, valid and test for each user.\n",
    "Then, these are merged together again later on.\n",
    "This is done to ensure that the sequences are kept together and not split randomly for the users."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# List the dfs fo train, val and test for each user\n",
    "users_locations_train = []\n",
    "users_locations_val = []\n",
    "users_locations_test = []\n",
    "\n",
    "for user_df in users_locations:\n",
    "  # Split in train, test and validation\n",
    "  train, test = train_test_split(user_df, test_size=0.2, shuffle=False)\n",
    "  train, val = train_test_split(train, test_size=0.2, shuffle=False)\n",
    "\n",
    "  # Append the sets\n",
    "  users_locations_train.append(train)\n",
    "  users_locations_val.append(val)\n",
    "  users_locations_test.append(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataframes are concatenated again."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Merge back the dataframes\n",
    "df_train = pd.concat(users_locations_train)\n",
    "\n",
    "# Merge back the dataframes\n",
    "df_val = pd.concat(users_locations_val)\n",
    "\n",
    "# Merge back the dataframes\n",
    "df_test = pd.concat(users_locations_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "user_ids = df_train.user_id.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper functions to split data, create clients dictionaries and preprocess the data for the FL algorithm.\n",
    "The model creation is also defined here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Split the data into chunks of N\n",
    "def split_data(N, train, val, test):\n",
    "\n",
    "  # dictionary of list of df\n",
    "  df_dictionary = {}\n",
    "\n",
    "  for uid in tqdm(user_ids):\n",
    "    # Get the records of the user\n",
    "    user_df_train = train.loc[train.user_id == uid].copy()\n",
    "    user_df_val = val.loc[val.user_id == uid].copy()\n",
    "    user_df_test = test.loc[test.user_id == uid].copy()\n",
    "\n",
    "    # Get a list of dataframes of length N records\n",
    "    user_list_train = [user_df_train[i:i+N] for i in range(0, user_df_train.shape[0], N)]\n",
    "    user_list_val = [user_df_val[i:i+N] for i in range(0, user_df_val.shape[0], N)]\n",
    "    user_list_test = [user_df_test[i:i+N] for i in range(0, user_df_test.shape[0], N)]\n",
    "\n",
    "    # Save the list of dataframes into a dictionary\n",
    "    df_dictionary[uid] = {\n",
    "        'train': user_list_train,\n",
    "        'val': user_list_val,\n",
    "        'test': user_list_test\n",
    "    }\n",
    "\n",
    "  return  df_dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Takes a dictionary with train, validation and test sets and the desired set type\n",
    "def create_clients_dict(df_dictionary, set_type, N):\n",
    "\n",
    "  dataset_dict = {}\n",
    "\n",
    "  for uid in tqdm(user_ids):\n",
    "\n",
    "    c_data = collections.OrderedDict()\n",
    "    values = df_dictionary[uid][set_type]\n",
    "\n",
    "    # If the last dataframe of the list is not complete\n",
    "    if len(values[-1]) < N:\n",
    "      diff = 1\n",
    "    else:\n",
    "      diff = 0\n",
    "\n",
    "    if len(values) > 0:\n",
    "      # Create the dictionary to create a clientData\n",
    "      for header in columns_names:\n",
    "        c_data[header] = [values[i][header].values for i in range(0, len(values)-diff)]\n",
    "      dataset_dict[uid] = c_data\n",
    "\n",
    "  return dataset_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# preprocess dataset to tf format\n",
    "def preprocess(dataset, N):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "\n",
    "    x=collections.OrderedDict()\n",
    "\n",
    "    for name in columns_names:\n",
    "      x[name]=tf.reshape(element[name][:, :-1], [-1, N-1])\n",
    "\n",
    "    y=tf.reshape(element[columns_names[0]][:, 1:], [-1, N-1])\n",
    "\n",
    "    return collections.OrderedDict(x=x, y=y)\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE, drop_remainder=True).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# create federated data for every client\n",
    "def make_federated_data(client_data, client_ids, N):\n",
    "\n",
    "  return [\n",
    "      preprocess(client_data.create_tf_dataset_for_client(x), N)\n",
    "      for x in tqdm(client_ids)\n",
    "  ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Create a model\n",
    "def create_keras_model(number_of_places, N, batch_size):\n",
    "\n",
    "  # Shortcut to the layers package\n",
    "  l = tf.keras.layers\n",
    "\n",
    "  # List of numeric feature columns to pass to the DenseLayer\n",
    "  numeric_feature_columns = []\n",
    "\n",
    "  # Handling numerical columns\n",
    "  for header in numerical_column_names:\n",
    "\t\t# Append all the numerical columns defined into the list\n",
    "    numeric_feature_columns.append(feature_column.numeric_column(header, shape=N-1))\n",
    "\n",
    "  feature_inputs={}\n",
    "  for c_name in numerical_column_names:\n",
    "    feature_inputs[c_name] = tf.keras.Input((N-1,), batch_size=batch_size, name=c_name)\n",
    "\n",
    "  # We cannot use an array of features as always because we have sequences\n",
    "  # We have to do one by one in order to match the shape\n",
    "  num_features = []\n",
    "  for c_name in numerical_column_names:\n",
    "    f =  feature_column.numeric_column(c_name, shape=(N-1))\n",
    "    feature = l.DenseFeatures(f)(feature_inputs)\n",
    "    feature = tf.expand_dims(feature, -1)\n",
    "    num_features.append(feature)\n",
    "\n",
    "  categorical_feature_inputs = []\n",
    "  categorical_features = []\n",
    "  for categorical_feature in categorical_columns:  # add batch_size=batch_size in case of stateful GRU\n",
    "    d = {categorical_feature.feature_name: tf.keras.Input((N-1,), batch_size=batch_size, dtype=tf.dtypes.int32, name=categorical_feature.feature_name)}\n",
    "    categorical_feature_inputs.append(d)\n",
    "\n",
    "    one_hot = feature_column.sequence_categorical_column_with_vocabulary_list(categorical_feature.feature_name, [i for i in range(categorical_feature.vocab_size)])\n",
    "\n",
    "    if categorical_feature.use_embedding:\n",
    "      # Embed the one-hot encoding\n",
    "      categorical_features.append(feature_column.embedding_column(one_hot, 64))\n",
    "    else:\n",
    "      categorical_features.append(feature_column.indicator_column(one_hot))\n",
    "\n",
    "  seq_features = []\n",
    "  for i in range(0, len(categorical_feature_inputs)):\n",
    "    sequence_features, sequence_length = tf.keras.experimental.SequenceFeatures(categorical_features[i])(categorical_feature_inputs[i])\n",
    "    seq_features.append(sequence_features)\n",
    "\n",
    "  input_sequence = l.Concatenate(axis=2)( [] + seq_features + num_features)\n",
    "\n",
    "  # Rnn\n",
    "  recurrent = l.GRU(64,\n",
    "                        batch_size=batch_size, #in case of stateful\n",
    "                        dropout=0.3,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform')(input_sequence)\n",
    "\n",
    "\n",
    "\t# Last layer with an output for each place\n",
    "  dense_1 = layers.Dense(number_of_places)(recurrent)\n",
    "\n",
    "\t# Softmax output layer\n",
    "  output = l.Softmax()(dense_1)\n",
    "\n",
    "\t# To return the Model, we need to define its inputs and outputs\n",
    "\t# In out case, we need to list all the input layers we have defined\n",
    "  inputs = list(feature_inputs.values()) + categorical_feature_inputs\n",
    "\n",
    "\t# Return the Model\n",
    "  return tf.keras.Model(inputs=inputs, outputs=output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#train and evaluate the model\n",
    "def train_and_eval_model(vocab_size, n, federated_train_data, federated_val_data, federated_test_data, path='./log/central-test-run'):\n",
    "  train_logdir = path + '/train'\n",
    "  val_logdir = path + '/val'\n",
    "  eval_logdir = path + '/eval'\n",
    "\n",
    "  train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
    "  val_summary_writer = tf.summary.create_file_writer(val_logdir)\n",
    "  eval_summary_writer = tf.summary.create_file_writer(eval_logdir)\n",
    "\n",
    "  # Clone the keras_model inside `create_tff_model()`, which TFF will\n",
    "  # call to produce a new copy of the model inside the graph that it will\n",
    "  # serialize. Note: we want to construct all the necessary objects we'll need\n",
    "  # _inside_ this method.\n",
    "  def create_tff_model():\n",
    "    # TFF uses an `input_spec` so it knows the types and shapes\n",
    "    # that your model expects.\n",
    "    input_spec = federated_train_data[0].element_spec\n",
    "    keras_model_clone = create_keras_model(vocab_size, n, batch_size=BATCH_SIZE)\n",
    "    #plot_model(keras_model_clone, 'keras_model_for_fl.png', show_shapes=True)\n",
    "    tff_model = tff.learning.from_keras_model(\n",
    "      keras_model_clone,\n",
    "      input_spec=input_spec,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return tff_model\n",
    "\n",
    "  # This command builds all the TensorFlow graphs and serializes them:\n",
    "  fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.002),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.06))\n",
    "\n",
    "  state = fed_avg.initialize()\n",
    "  evaluation = tff.learning.build_federated_evaluation(model_fn=create_tff_model)\n",
    "\n",
    "  tolerance = 7\n",
    "  best_state = 0\n",
    "  lowest_loss = 100.00\n",
    "  stop = tolerance\n",
    "\n",
    "  NUM_ROUNDS = 5\n",
    "  with train_summary_writer.as_default():\n",
    "    for round_num in range(1, NUM_ROUNDS + 1):\n",
    "      log('Round {r}'.format(r=round_num))\n",
    "\n",
    "      # Uncomment to simulate sparse availability of clients\n",
    "      # train_data_for_this_round, val_data_for_this_round = sample((federated_train_data, federated_val_data), 20, NUM_CLIENTS)\n",
    "\n",
    "      state, metrics = fed_avg.next(state, federated_train_data)\n",
    "\n",
    "      train_metrics = metrics['train']\n",
    "      log('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "      val_metrics = evaluation(state.model, federated_val_data)\n",
    "      log('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "      # Check for decreasing validation loss\n",
    "      if lowest_loss > val_metrics['loss']:\n",
    "        log('\\tSaving best model..')\n",
    "        lowest_loss = val_metrics['loss']\n",
    "        best_state = state\n",
    "        stop = tolerance - 1\n",
    "      else:\n",
    "        stop = stop - 1\n",
    "        if stop <= 0:\n",
    "          log('\\tEarly stopping...')\n",
    "          break;\n",
    "\n",
    "      log(' ')\n",
    "      log('\\twriting..')\n",
    "\n",
    "      # Iterate across the metrics and write their data\n",
    "      for name, value in dict(train_metrics).items():\n",
    "        tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "      with val_summary_writer.as_default():\n",
    "        for name, value in dict(val_metrics).items():\n",
    "          tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "  train_summary_writer.close()\n",
    "  val_summary_writer.close()\n",
    "\n",
    "  # evaluate over test data\n",
    "  test_metrics = evaluation(best_state.model, federated_test_data)\n",
    "  log('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, the different subsets of the dataset are generated.\n",
    "Afterwards, the column_names variables are set accordingly so that only the relevant columns are chosen for the federated train/val/test datasets.\n",
    "The training is run for all combinations and the results are saved, so they can be compared easily."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class CategoricalFeature:\n",
    "  def __init__(self, feature_name, vocab_size, use_embedding):\n",
    "    self.feature_name = feature_name\n",
    "    self.vocab_size = vocab_size\n",
    "    self.use_embedding = use_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "vocab_size = df.cat_id.unique().size\n",
    "users_size = df.user_id.unique().size\n",
    "venues_size = df.venue_id.unique().size\n",
    "orig_cats_size = df.orig_cat_id.unique().size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "all_num_column_names = ['latitude', 'longitude', 'clock_sin', 'clock_cos', 'day_sin', 'day_cos', 'month_sin',\n",
    "                          'month_cos', 'week_day_sin', 'week_day_cos']\n",
    "\n",
    "all_cat_columns = [\n",
    "      CategoricalFeature('user_id', users_size, False),\n",
    "      CategoricalFeature('cat_id', vocab_size, False),\n",
    "      CategoricalFeature('venue_id', venues_size, False),\n",
    "      CategoricalFeature('orig_cat_id', orig_cats_size, False)]\n",
    "\n",
    "NUM_CLIENTS = user_ids.size\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 16\n",
    "#SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 5\n",
    "n=17"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************START Run************************************\n",
      "Excluded columns: ['user_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 434.78it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 144.40it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 908.83it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 416.79it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.45it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 30.74it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 29.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "\tTrain: loss=2.756, accuracy=0.246\n",
      "\tValidation: loss=3.557, accuracy=0.174\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=3.180, accuracy=0.218\n",
      "\tValidation: loss=3.158, accuracy=0.071\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 3\n",
      "\tTrain: loss=3.222, accuracy=0.090\n",
      "\tValidation: loss=3.210, accuracy=0.092\n",
      " \n",
      "\twriting..\n",
      "Round 4\n",
      "\tTrain: loss=2.913, accuracy=0.179\n",
      "\tValidation: loss=3.082, accuracy=0.169\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 5\n",
      "\tTrain: loss=2.758, accuracy=0.234\n",
      "\tValidation: loss=3.031, accuracy=0.172\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=3.113, accuracy=0.146\n",
      "************************************END Run*************************************\n",
      "***********************************START Run************************************\n",
      "Excluded columns: ['latitude', 'longitude']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 491.18it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 235.05it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 187.12it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 779.14it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 31.11it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 29.03it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 32.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "\tTrain: loss=3.043, accuracy=0.211\n",
      "\tValidation: loss=4.064, accuracy=0.176\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=3.514, accuracy=0.213\n",
      "\tValidation: loss=3.413, accuracy=0.093\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 3\n",
      "\tTrain: loss=2.995, accuracy=0.171\n",
      "\tValidation: loss=3.243, accuracy=0.089\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 4\n",
      "\tTrain: loss=2.717, accuracy=0.227\n",
      "\tValidation: loss=2.932, accuracy=0.210\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 5\n",
      "\tTrain: loss=2.585, accuracy=0.282\n",
      "\tValidation: loss=2.781, accuracy=0.209\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "\tEvaluation: loss=2.915, accuracy=0.177\n",
      "************************************END Run*************************************\n",
      "***********************************START Run************************************\n",
      "Excluded columns: ['is_weekend']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 434.07it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.30it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 919.50it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 712.46it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 29.07it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 25.62it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 30.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "\tTrain: loss=2.814, accuracy=0.229\n",
      "\tValidation: loss=3.846, accuracy=0.149\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 2\n",
      "\tTrain: loss=3.776, accuracy=0.171\n",
      "\tValidation: loss=3.236, accuracy=0.114\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 3\n",
      "\tTrain: loss=3.121, accuracy=0.142\n",
      "\tValidation: loss=3.319, accuracy=0.064\n",
      " \n",
      "\twriting..\n",
      "Round 4\n",
      "\tTrain: loss=3.277, accuracy=0.083\n",
      "\tValidation: loss=3.118, accuracy=0.050\n",
      "\tSaving best model..\n",
      " \n",
      "\twriting..\n",
      "Round 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18880\\2846130996.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     43\u001B[0m     \u001B[0mfederated_test_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmake_federated_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclient_test_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_clients\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 45\u001B[1;33m     \u001B[0mtrain_and_eval_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfederated_train_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfederated_val_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfederated_test_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'./log/central-test-run'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     46\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[0mlog\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'END Run'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcenter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m80\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'*'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18880\\2115160222.py\u001B[0m in \u001B[0;36mtrain_and_eval_model\u001B[1;34m(vocab_size, n, federated_train_data, federated_val_data, federated_test_data, path)\u001B[0m\n\u001B[0;32m     48\u001B[0m       \u001B[1;31m# train_data_for_this_round, val_data_for_this_round = sample((federated_train_data, federated_val_data), 20, NUM_CLIENTS)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 50\u001B[1;33m       \u001B[0mstate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfed_avg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfederated_train_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     51\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m       \u001B[0mtrain_metrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'train'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\utils\\function_utils.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    519\u001B[0m     \u001B[0mcontext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_context_stack\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcurrent\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    520\u001B[0m     \u001B[0marg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpack_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_type_signature\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 521\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minvoke\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    522\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    523\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m__hash__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\retrying.py\u001B[0m in \u001B[0;36mwrapped_f\u001B[1;34m(*args, **kw)\u001B[0m\n\u001B[0;32m     54\u001B[0m             \u001B[1;33m@\u001B[0m\u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwraps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m             \u001B[1;32mdef\u001B[0m \u001B[0mwrapped_f\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mRetrying\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mdargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mdkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mwrapped_f\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\retrying.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    256\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_reject\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattempt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 257\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mattempt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrap_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    258\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    259\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_after_attempts\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\retrying.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(self, wrap_exception)\u001B[0m\n\u001B[0;32m    299\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mRetryError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    300\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 301\u001B[1;33m                 \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreraise\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    302\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    303\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\six.py\u001B[0m in \u001B[0;36mreraise\u001B[1;34m(tp, value, tb)\u001B[0m\n\u001B[0;32m    717\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mtb\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    718\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 719\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    720\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    721\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\retrying.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    250\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 251\u001B[1;33m                 \u001B[0mattempt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAttempt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattempt_number\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    252\u001B[0m             \u001B[1;32mexcept\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    253\u001B[0m                 \u001B[0mtb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexc_info\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\tensorflow_federated\\python\\core\\impl\\executors\\execution_context.py\u001B[0m in \u001B[0;36minvoke\u001B[1;34m(self, comp, arg)\u001B[0m\n\u001B[0;32m    216\u001B[0m         return self._event_loop.run_until_complete(\n\u001B[0;32m    217\u001B[0m             tracing.wrap_coroutine_in_current_trace_context(\n\u001B[1;32m--> 218\u001B[1;33m                 _invoke(executor, comp, arg, result_type)))\n\u001B[0m",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\nest_asyncio.py\u001B[0m in \u001B[0;36mrun_until_complete\u001B[1;34m(self, future)\u001B[0m\n\u001B[0;32m     82\u001B[0m                 \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_log_destroy_pending\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m             \u001B[1;32mwhile\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 84\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_run_once\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     85\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stopping\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m                     \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Privat\\HTW-Master\\Sem3\\PA\\HumanMobilityPredictionMA\\venv\\lib\\site-packages\\nest_asyncio.py\u001B[0m in \u001B[0;36m_run_once\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    105\u001B[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001B[0;32m    106\u001B[0m             else None)\n\u001B[1;32m--> 107\u001B[1;33m         \u001B[0mevent_list\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_selector\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    108\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_process_events\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mevent_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\selectors.py\u001B[0m in \u001B[0;36mselect\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    321\u001B[0m         \u001B[0mready\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    322\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 323\u001B[1;33m             \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_select\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_readers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_writers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    324\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mInterruptedError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mready\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\selectors.py\u001B[0m in \u001B[0;36m_select\u001B[1;34m(self, r, w, _, timeout)\u001B[0m\n\u001B[0;32m    312\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplatform\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'win32'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0m_select\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 314\u001B[1;33m             \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mselect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    315\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    316\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "drop_columns = [['user_id'], ['latitude', 'longitude'], ['is_weekend'], ['venue_id'], ['orig_cat_id']]\n",
    "\n",
    "for L in range(1, len(drop_columns) + 1):\n",
    "  for subset in itertools.combinations(drop_columns, L):\n",
    "\n",
    "    cols = [item for sub_list in subset for item in sub_list]\n",
    "\n",
    "    log('START Run'.center(80, '*'))\n",
    "    log('Excluded columns: {c}'.format(c=cols))\n",
    "\n",
    "    #train_dropped = df_train.drop(cols, axis=1, inplace=False)\n",
    "    #val_dropped = df_val.drop(cols, axis=1, inplace=False)\n",
    "    #test_dropped = df_test.drop(cols, axis=1, inplace=False)\n",
    "\n",
    "    columns_names = [i for i in df_train.columns.values if i not in cols]\n",
    "\n",
    "    numerical_column_names = [i for i in all_num_column_names if i not in cols]\n",
    "\n",
    "    categorical_columns = [i for i in all_cat_columns if i.feature_name not in cols]\n",
    "\n",
    "    df_dict = split_data(n, df_train, df_val, df_test)\n",
    "\n",
    "    clients_train_dict = create_clients_dict(df_dict, 'train', n)\n",
    "    clients_val_dict = create_clients_dict(df_dict, 'val', n)\n",
    "    clients_test_dict = create_clients_dict(df_dict, 'test', n)\n",
    "\n",
    "    # Convert the dictionary to a dataset\n",
    "    client_train_data = tff.simulation.FromTensorSlicesClientData(clients_train_dict)\n",
    "    client_val_data = tff.simulation.FromTensorSlicesClientData(clients_val_dict)\n",
    "    client_test_data = tff.simulation.FromTensorSlicesClientData(clients_test_dict)\n",
    "\n",
    "    example_dataset = client_train_data.create_tf_dataset_for_client(\n",
    "    client_train_data.client_ids[1])\n",
    "\n",
    "    example_element = next(iter(example_dataset))\n",
    "\n",
    "    # Select the clients\n",
    "    sample_clients = client_train_data.client_ids[0:NUM_CLIENTS]\n",
    "\n",
    "    # Federate the clients datasets\n",
    "    federated_train_data = make_federated_data(client_train_data, sample_clients, n)\n",
    "    federated_val_data = make_federated_data(client_val_data, sample_clients, n)\n",
    "    federated_test_data = make_federated_data(client_test_data, sample_clients, n)\n",
    "\n",
    "    train_and_eval_model(vocab_size, n, federated_train_data, federated_val_data, federated_test_data, path='./log/central-test-run')\n",
    "\n",
    "    log('END Run'.center(80, '*'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}