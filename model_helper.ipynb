{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow_federated as tff\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import import_ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ModelHelper:\n",
    "\n",
    "    def __init__(self, df, total_window_length=None):\n",
    "        self.df = df\n",
    "        if total_window_length is not None:\n",
    "            self.sequence_length = total_window_length - 1\n",
    "            self.total_window_length = total_window_length\n",
    "        self.target_column_name = df.columns[0]\n",
    "        self.num_epochs = 20\n",
    "\n",
    "    def reset_df(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def train_val_test_split(self):\n",
    "        train, self.df_test = train_test_split(self.df, test_size=0.2, shuffle=False)\n",
    "        self.df_train, self.df_val = train_test_split(train, test_size=0.2, shuffle=False)\n",
    "\n",
    "    def set_column_names(self, column_names):\n",
    "        self.column_names = column_names\n",
    "\n",
    "    def set_numerical_column_names(self, numerical_column_names):\n",
    "        self.numerical_column_names = numerical_column_names\n",
    "\n",
    "    def set_target_column_name(self, target_column_name):\n",
    "        self.target_column_name = target_column_name\n",
    "\n",
    "    def set_vocab_size(self, vocab_size=None):\n",
    "        if vocab_size is None:\n",
    "            self.vocab_size = int(np.max(self.df[self.target_column_name].values) + 1)\n",
    "        else:\n",
    "            self.vocab_size = vocab_size\n",
    "\n",
    "    def set_client_column_name(self, column_name):\n",
    "        self.client_column = column_name\n",
    "\n",
    "    def set_client_column_ids(self, ids=None):\n",
    "        if ids is None:\n",
    "            self.client_column_ids = self.df[self.client_column].unique()\n",
    "        else:\n",
    "            self.client_column_ids = ids\n",
    "\n",
    "    def set_batch_size(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def set_num_epochs(self, num_epochs):\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def set_prefetch_buffer(self, prefetch_buffer):\n",
    "        self.prefetch_buffer = prefetch_buffer\n",
    "\n",
    "    def set_hyper_parameters(self, num_epochs, batch_size, prefetch_buffer):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.prefetch_buffer = prefetch_buffer\n",
    "\n",
    "    def get_unique_clients(self):\n",
    "        if self.client_column_ids is None and self.client_column in self.df:\n",
    "            return self.df[self.client_column].unique()\n",
    "        return self.client_column_ids\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For simple model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def df_to_dataset(self, df, shuffle=False):\n",
    "\n",
    "  df = df.copy()\n",
    "  labels = df.pop(self.target_column_name)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(df))\n",
    "  ds = ds.batch(self.batch_size)\n",
    "  return ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def df_to_datasets(self, shuffle=False):\n",
    "\n",
    "    self.train_dataset = df_to_dataset(self, self.df_train, shuffle)\n",
    "    self.val_dataset = df_to_dataset(self, self.df_val, shuffle)\n",
    "    self.test_dataset = df_to_dataset(self, self.df_test, shuffle)\n",
    "\n",
    "ModelHelper.df_to_datasets = df_to_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split data for FL"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_data_users(self):\n",
    "\n",
    "  # dictionary of list of df\n",
    "  df_dictionary = {}\n",
    "\n",
    "  for uid in tqdm(self.client_column_ids):\n",
    "    # Get the records of the user\n",
    "    user_df_train = self.df_train.loc[self.df_train[self.client_column] == uid].copy()\n",
    "    user_df_val = self.df_val.loc[self.df_val[self.client_column] == uid].copy()\n",
    "    user_df_test = self.df_test.loc[self.df_test[self.client_column] == uid].copy()\n",
    "\n",
    "    # Get a list of dataframes of length N records\n",
    "    user_list_train = [user_df_train[i:i+self.total_window_length] for i in range(0, user_df_train.shape[0], self.total_window_length)]\n",
    "    user_list_val = [user_df_val[i:i+self.total_window_length] for i in range(0, user_df_val.shape[0], self.total_window_length)]\n",
    "    user_list_test = [user_df_test[i:i+self.total_window_length] for i in range(0, user_df_test.shape[0], self.total_window_length)]\n",
    "\n",
    "    # Save the list of dataframes into a dictionary\n",
    "    df_dictionary[uid] = {\n",
    "        'train': user_list_train,\n",
    "        'val': user_list_val,\n",
    "        'test': user_list_test\n",
    "    }\n",
    "\n",
    "  self.df_dictionary=df_dictionary\n",
    "\n",
    "ModelHelper.split_data_users = split_data_users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split Data Basic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into chunks of total_window_length\n",
    "def split_data(self):\n",
    "\n",
    "  # Get a list of dataframes of length total_window_length records\n",
    "  list_train = ([self.df_train[i:i+self.total_window_length] for i in range(0, self.df_train.shape[0], self.total_window_length)])\n",
    "  list_val = ([self.df_val[i:i+self.total_window_length] for i in range(0, self.df_val.shape[0], self.total_window_length)])\n",
    "  list_test = ([self.df_test[i:i+self.total_window_length] for i in range(0, self.df_test.shape[0], self.total_window_length)])\n",
    "\n",
    "  self.list_train=list_train\n",
    "  self.list_val=list_val\n",
    "  self.list_test=list_test\n",
    "\n",
    "ModelHelper.split_data = split_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sliding Window Approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data into chunks of N\n",
    "def split_data_sliding(self):\n",
    "    self.list_train = sliding_window(self.total_window_length, self.df_train)\n",
    "    self.list_val = sliding_window(self.total_window_length, self.df_val)\n",
    "    self.list_test = sliding_window(self.total_window_length, self.df_test)\n",
    "\n",
    "ModelHelper.split_data_sliding = split_data_sliding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sliding_window(N, arr):\n",
    "\n",
    "    \"\"\"\n",
    "    Splits an array into a list of subarrays using a sliding window approach.\n",
    "\n",
    "    Parameters:\n",
    "        arr: a numpy array\n",
    "        N: the number of elements in each subarray\n",
    "\n",
    "    Returns:\n",
    "        A list of numpy arrays.\n",
    "    \"\"\"\n",
    "    # Get the number of subarrays\n",
    "    num_subarrays = arr.shape[0] - N + 1\n",
    "\n",
    "    # Create an empty list to store the subarrays\n",
    "    subarrays = []\n",
    "\n",
    "    # Loop through the array and create the subarrays\n",
    "    for i in range(num_subarrays):\n",
    "        subarray = arr[i:i + N]\n",
    "        subarrays.append(subarray)\n",
    "\n",
    "    return subarrays"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create TF DataSet for Basic (Non-Federated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Takes a dictionary with train, validation and test sets and the desired set type\n",
    "def create_dataset(self, my_list, multi_target=True):\n",
    "\n",
    "  input_dict = collections.OrderedDict()\n",
    "\n",
    "  # If the last dataframe of the list is not complete\n",
    "  if len(my_list[-1]) < self.total_window_length:\n",
    "    diff = 1\n",
    "  else:\n",
    "    diff = 0\n",
    "\n",
    "  if len(my_list) > 0:\n",
    "    # Create the dictionary to create a clientData\n",
    "    for header in self.column_names:\n",
    "      input_dict[header] = [my_list[i][header].values[:-1] for i in range(0, len(my_list) - diff)]\n",
    "\n",
    "  if multi_target is True:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_dict, np.array([my_list[i][self.column_names[0]].values[1:] for i in range(0, len(my_list)-diff)])))\n",
    "  else:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_dict, np.array([my_list[i][self.column_names[0]].values[-1] for i in range(0, len(my_list)-diff)])))\n",
    "\n",
    "\n",
    "  return dataset\n",
    "\n",
    "ModelHelper.create_dataset = create_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_and_batch_datasets(self, multi_target=True):\n",
    "    self.train_dataset = self.create_dataset(self.list_train, multi_target).batch(self.batch_size, drop_remainder=True)\n",
    "    self.val_dataset = self.create_dataset(self.list_val, multi_target).batch(self.batch_size, drop_remainder=True)\n",
    "    self.test_dataset = self.create_dataset(self.list_test, multi_target).batch(self.batch_size, drop_remainder=True)\n",
    "\n",
    "ModelHelper.create_and_batch_datasets = create_and_batch_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Federated Learning Client Dictionaries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Takes a dictionary with train, validation and test sets and the desired set type\n",
    "# for users for federated training\n",
    "def create_clients_dict(self, set_type):\n",
    "\n",
    "  dataset_dict = {}\n",
    "\n",
    "  for uid in tqdm(self.client_column_ids):\n",
    "\n",
    "    c_data = collections.OrderedDict()\n",
    "    values = self.df_dictionary[uid][set_type]\n",
    "\n",
    "    # If the last dataframe of the list is not complete #\n",
    "    if len(values[-1]) < self.total_window_length:\n",
    "      diff = 1\n",
    "    else:\n",
    "      diff = 0\n",
    "\n",
    "    if len(values) > 0:\n",
    "      # Create the dictionary to create a clientData\n",
    "      for header in self.column_names:\n",
    "        c_data[header] = [values[i][header].values for i in range(0, len(values)-diff)]\n",
    "      dataset_dict[uid] = c_data\n",
    "\n",
    "  return dataset_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_client_dicts(self):\n",
    "\n",
    "    self.clients_train_dict = create_clients_dict(self, 'train')\n",
    "    self.clients_val_dict = create_clients_dict(self, 'val')\n",
    "    self.clients_test_dict = create_clients_dict(self, 'test')\n",
    "\n",
    "ModelHelper.generate_client_dicts = generate_client_dicts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "FL Preprocessing routine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preprocess dataset to tf format\n",
    "def preprocess(self, dataset, N):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "\n",
    "    x=collections.OrderedDict()\n",
    "\n",
    "    for name in self.column_names:\n",
    "      x[name]=tf.reshape(element[name][:, :-1], [-1, N-1])\n",
    "\n",
    "    y=tf.reshape(element[self.column_names[0]][:, 1:], [-1, N-1])\n",
    "\n",
    "    return collections.OrderedDict(x=x, y=y)\n",
    "\n",
    "  return dataset.repeat(self.num_epochs).batch(self.batch_size, drop_remainder=True).map(batch_format_fn).prefetch(self.prefetch_buffer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create TF slices Client Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_client_data(self):\n",
    "\n",
    "    self.client_train_data = tff.simulation.FromTensorSlicesClientData(self.clients_train_dict)\n",
    "    self.client_val_data = tff.simulation.FromTensorSlicesClientData(self.clients_val_dict)\n",
    "    self.client_test_data = tff.simulation.FromTensorSlicesClientData(self.clients_test_dict)\n",
    "\n",
    "ModelHelper.generate_client_data = generate_client_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Federated Data for clients"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create federated data for every client\n",
    "def make_federated_data(self, client_data, client_ids):\n",
    "\n",
    "  return [\n",
    "      preprocess(self, client_data.create_tf_dataset_for_client(x), self.total_window_length)\n",
    "      for x in tqdm(client_ids)\n",
    "  ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_federated_data(self, sample_clients):\n",
    "\n",
    "    # Federate the clients datasets\n",
    "    self.federated_train_data = make_federated_data(self, self.client_train_data, sample_clients)\n",
    "    self.federated_val_data = make_federated_data(self, self.client_val_data, sample_clients)\n",
    "    self.federated_test_data = make_federated_data(self, self.client_test_data, sample_clients)\n",
    "\n",
    "ModelHelper.generate_federated_data = generate_federated_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NYC Central (Non-Federated) Model Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to remove duplicates\n",
    "def create_sequence(locations):\n",
    "\n",
    "  # Flatten the list of places\n",
    "  sequence = np.reshape(locations.values, [-1])\n",
    "\n",
    "  # Create a temporary array of the same lenght of the sequece of locations\n",
    "  copy = np.zeros(sequence.shape[0], dtype=np.int32)\n",
    "\n",
    "  # Copy the sequence of location in the copy array but shifted right by 1 position\n",
    "  # The last location does not need to be copied, it can't be a duplicate\n",
    "  copy[1:] = sequence[:sequence.shape[0]-1]\n",
    "\n",
    "  # Where we get 0 it can be a possible duplicated\n",
    "  duplicated = sequence - copy\n",
    "\n",
    "  # indices where the subtraction gives 0\n",
    "  idx = np.where(duplicated == 0)[0]\n",
    "\n",
    "  # Find where the position of the zeros are even\n",
    "  even = idx%2 == 0\n",
    "\n",
    "  # List the indices where the position is even and the subtraction gave 0\n",
    "  to_drop = idx[even]\n",
    "\n",
    "  # Remove the duplicates\n",
    "  clean_sequence = np.delete(sequence, to_drop)\n",
    "  return clean_sequence, to_drop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def df_to_location_sequence(self):\n",
    "\n",
    "  # take just the columns we need\n",
    "  locations = self.df[['pickup_location_id','dropoff_location_id']].copy()\n",
    "  locations = locations.astype('int32')\n",
    "\n",
    "\n",
    "  # define the indices to keep trace of the locations\n",
    "  x = np.arange(0, locations.values.shape[0])\n",
    "\n",
    "  pos = np.array([x,x]).T\n",
    "  pos = np.reshape(pos, [-1])\n",
    "\n",
    "  # Represent whether the location is a pickup or a dropoff\n",
    "  pick = np.zeros(locations.values.shape[0], dtype=int)\n",
    "  drop = np.ones(locations.values.shape[0], dtype=int)\n",
    "\n",
    "  loc = np.array([pick,drop]).T\n",
    "  loc = np.reshape(loc, [-1])\n",
    "\n",
    "  # Generate the sequence of places\n",
    "  sequence, duplicates = create_sequence(locations)\n",
    "\n",
    "  # We use now the indices of the duplicated locations to clean also the array of rows and the array of location types\n",
    "  pos = np.delete(pos, duplicates)\n",
    "  loc = np.delete(loc, duplicates)\n",
    "\n",
    "  # Select the indices of records we want the pickup location\n",
    "  pick_pos = pos[pos[loc == 0]]\n",
    "\n",
    "  # Select the indices of records we want the dropoff location\n",
    "  drop_pos = pos[pos[loc == 1]]\n",
    "\n",
    "\n",
    "  records_pick = self.df.iloc[pick_pos][['pickup_location_id', 'pickup_week_day',\t'pickup_hour',\t'pickup_day',\t'pickup_month']]\n",
    "  records_pick = records_pick.rename(columns={'pickup_location_id': 'location_id', 'pickup_week_day':'week_day' ,\t'pickup_hour':'hour' ,\t'pickup_day':\t'day' ,\t'pickup_month':'month' })\n",
    "\n",
    "  idx_drop = np.nonzero(loc == 0)[0]\n",
    "  records_drop = self.df.iloc[drop_pos][['dropoff_location_id', 'dropoff_week_day',\t'dropoff_hour',\t'dropoff_day',\t'dropoff_month']]\n",
    "  records_drop = records_drop.rename(columns={'dropoff_location_id': 'location_id', 'dropoff_week_day':'week_day' ,\t'dropoff_hour':'hour' ,\t'dropoff_day':\t'day' ,\t'dropoff_month':'month' })\n",
    "\n",
    "  locations_sequence = pd.concat([records_pick, records_drop])\n",
    "\n",
    "  # reset the index\n",
    "  locations_sequence.reset_index(inplace=True)\n",
    "\n",
    "  # From hour to sin-cos representation\n",
    "  locations_sequence['hour_sin'] = np.sin(locations_sequence.hour*(2.*np.pi/24))\n",
    "  locations_sequence['hour_cos'] = np.cos(locations_sequence.hour*(2.*np.pi/24))\n",
    "\n",
    "  locations_sequence['week_day_sin'] = np.sin(locations_sequence.week_day*(2.*np.pi/7))\n",
    "  locations_sequence['week_day_cos'] = np.cos(locations_sequence.week_day*(2.*np.pi/7))\n",
    "\n",
    "\n",
    "  # Drop the original column\n",
    "  locations_sequence.drop(['hour'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "  # Helper function to encode the day_type\n",
    "  def is_weekend(days):\n",
    "    weekends = np.zeros(len(days))\n",
    "    weekends[((days == 5) | (days == 6))] = 1\n",
    "    return weekends\n",
    "\n",
    "  # Apply the helper function to all the records\n",
    "  locations_sequence['weekend'] = is_weekend(locations_sequence['week_day'])\n",
    "\n",
    "  # the column is not needed anymore\n",
    "  locations_sequence.drop(['week_day'], axis=1, inplace=True)\n",
    "\n",
    "  # Correct the weekend feature type\n",
    "  dictionary = {'weekend': 'int32'}\n",
    "  locations_sequence = locations_sequence.astype(dictionary, copy=True)\n",
    "  self.df = locations_sequence\n",
    "\n",
    "  return locations_sequence, pos, loc\n",
    "\n",
    "ModelHelper.df_to_location_sequence = df_to_location_sequence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "create batch dataset for basic (non-federated) execution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_batch_dataset(self):\n",
    "\n",
    "    if len(self.list_train[-1]) < self.total_window_length:\n",
    "      diff_train = 1\n",
    "    else:\n",
    "      diff_train = 0\n",
    "\n",
    "    if len(self.list_val[-1]) < self.total_window_length:\n",
    "      diff_val = 1\n",
    "    else:\n",
    "      diff_val = 0\n",
    "\n",
    "    if len(self.list_test[-1]) < self.total_window_length:\n",
    "      diff_test = 1\n",
    "    else:\n",
    "      diff_test = 0\n",
    "\n",
    "    # Define the input features of the  dataset\n",
    "    train_input_dict = {\n",
    "      'start_place':np.array([self.list_train[i]['location_id'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'start_hour_sin':np.array([self.list_train[i]['hour_sin'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'start_hour_cos':np.array([self.list_train[i]['hour_cos'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'weekend':np.array([self.list_train[i]['weekend'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'week_day_sin':np.array([self.list_train[i]['week_day_sin'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'week_day_cos':np.array([self.list_train[i]['week_day_cos'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_hour_sin':np.array([self.list_train[i]['hour_sin'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_hour_cos':np.array([self.list_train[i]['hour_cos'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_weekend':np.array([self.list_train[i]['weekend'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_week_day_sin':np.array([self.list_train[i]['week_day_sin'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_week_day_cos':np.array([self.list_train[i]['week_day_cos'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "    }\n",
    "\n",
    "    # Define the input features of the  dataset\n",
    "    val_input_dict = {\n",
    "      'start_place':np.array([self.list_val[i]['location_id'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'start_hour_sin':np.array([self.list_val[i]['hour_sin'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'start_hour_cos':np.array([self.list_val[i]['hour_cos'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'weekend':np.array([self.list_val[i]['weekend'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'week_day_sin':np.array([self.list_val[i]['week_day_sin'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'week_day_cos':np.array([self.list_val[i]['week_day_cos'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_hour_sin':np.array([self.list_val[i]['hour_sin'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_hour_cos':np.array([self.list_val[i]['hour_cos'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_weekend':np.array([self.list_val[i]['weekend'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_week_day_sin':np.array([self.list_val[i]['week_day_sin'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_week_day_cos':np.array([self.list_val[i]['week_day_cos'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "    }\n",
    "\n",
    "    # Define the input features of the  dataset\n",
    "    test_input_dict = {\n",
    "      'start_place':np.array([self.list_test[i]['location_id'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'start_hour_sin':np.array([self.list_test[i]['hour_sin'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'start_hour_cos':np.array([self.list_test[i]['hour_cos'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'weekend':np.array([self.list_test[i]['weekend'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'week_day_sin':np.array([self.list_test[i]['week_day_sin'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'week_day_cos':np.array([self.list_test[i]['week_day_cos'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_hour_sin':np.array([self.list_test[i]['hour_sin'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_hour_cos':np.array([self.list_test[i]['hour_cos'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_weekend':np.array([self.list_test[i]['weekend'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_week_day_sin':np.array([self.list_test[i]['week_day_sin'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_week_day_cos':np.array([self.list_test[i]['week_day_cos'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "    }\n",
    "\n",
    "    # Create training examples / targets, we are going to predict the next location\n",
    "    trips_dataset_train = tf.data.Dataset.from_tensor_slices((train_input_dict, np.array([self.list_train[i]['location_id'].values[-1] for i in range(0, len(self.list_train)-diff_train)])))\n",
    "    trips_dataset_val = tf.data.Dataset.from_tensor_slices((val_input_dict, np.array([self.list_val[i]['location_id'].values[-1] for i in range(0, len(self.list_val)-diff_val)])))\n",
    "    trips_dataset_test = tf.data.Dataset.from_tensor_slices((test_input_dict, np.array([self.list_test[i]['location_id'].values[-1] for i in range(0, len(self.list_test)-diff_test)])))\n",
    "\n",
    "\n",
    "    # Buffer size to shuffle the dataset\n",
    "    # (TF data is designed to work with possibly infinite sequences,\n",
    "    # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "    # it maintains a buffer in which it shuffles elements).\n",
    "    # BUFFER_SIZE = 10000\n",
    "\n",
    "    # Create the dataset by creating batches\n",
    "    # Uncomment the shuffle function in case we want to shuffle the sequences\n",
    "    self.train_dataset = trips_dataset_train.batch(self.batch_size, drop_remainder=True) #.shuffle(BUFFER_SIZE)\n",
    "    self.val_dataset = trips_dataset_val.batch(self.batch_size, drop_remainder=True) #.shuffle(BUFFER_SIZE)\n",
    "    self.test_dataset = trips_dataset_test.batch(self.batch_size, drop_remainder=True) #.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    return self.train_dataset, self.val_dataset, self.test_dataset\n",
    "\n",
    "ModelHelper.create_batch_dataset = create_batch_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_batch_dataset_cin(self):\n",
    "\n",
    "    if len(self.list_train[-1]) < self.total_window_length:\n",
    "      diff_train = 1\n",
    "    else:\n",
    "      diff_train = 0\n",
    "\n",
    "    if len(self.list_val[-1]) < self.total_window_length:\n",
    "      diff_val = 1\n",
    "    else:\n",
    "      diff_val = 0\n",
    "\n",
    "    if len(self.list_test[-1]) < self.total_window_length:\n",
    "      diff_test = 1\n",
    "    else:\n",
    "      diff_test = 0\n",
    "\n",
    "    # Define the input features of the  dataset\n",
    "    train_input_dict = {\n",
    "      'start_place':np.array([self.list_train[i]['location_id'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'start_hour_sin':np.array([self.list_train[i]['clock_sin'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'start_hour_cos':np.array([self.list_train[i]['clock_cos'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'weekend':np.array([self.list_train[i]['is_weekend'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'week_day_sin':np.array([self.list_train[i]['week_day_sin'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'week_day_cos':np.array([self.list_train[i]['week_day_cos'].values[:-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_hour_sin':np.array([self.list_train[i]['clock_sin'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_hour_cos':np.array([self.list_train[i]['clock_cos'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_weekend':np.array([self.list_train[i]['is_weekend'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_week_day_sin':np.array([self.list_train[i]['week_day_sin'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "      'end_week_day_cos':np.array([self.list_train[i]['week_day_cos'].values[-1] for i in range(0, len(self.list_train)-diff_train)]),\n",
    "    }\n",
    "\n",
    "    # Define the input features of the  dataset\n",
    "    val_input_dict = {\n",
    "      'start_place':np.array([self.list_val[i]['location_id'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'start_hour_sin':np.array([self.list_val[i]['clock_sin'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'start_hour_cos':np.array([self.list_val[i]['clock_cos'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'weekend':np.array([self.list_val[i]['is_weekend'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'week_day_sin':np.array([self.list_val[i]['week_day_sin'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'week_day_cos':np.array([self.list_val[i]['week_day_cos'].values[:-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_hour_sin':np.array([self.list_val[i]['clock_sin'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_hour_cos':np.array([self.list_val[i]['clock_cos'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_weekend':np.array([self.list_val[i]['is_weekend'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_week_day_sin':np.array([self.list_val[i]['week_day_sin'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "      'end_week_day_cos':np.array([self.list_val[i]['week_day_cos'].values[-1] for i in range(0, len(self.list_val)-diff_val)]),\n",
    "    }\n",
    "\n",
    "    # Define the input features of the  dataset\n",
    "    test_input_dict = {\n",
    "      'start_place':np.array([self.list_test[i]['location_id'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'start_hour_sin':np.array([self.list_test[i]['clock_sin'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'start_hour_cos':np.array([self.list_test[i]['clock_cos'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'weekend':np.array([self.list_test[i]['is_weekend'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'week_day_sin':np.array([self.list_test[i]['week_day_sin'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'week_day_cos':np.array([self.list_test[i]['week_day_cos'].values[:-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_hour_sin':np.array([self.list_test[i]['clock_sin'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_hour_cos':np.array([self.list_test[i]['clock_cos'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_weekend':np.array([self.list_test[i]['is_weekend'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_week_day_sin':np.array([self.list_test[i]['week_day_sin'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "      'end_week_day_cos':np.array([self.list_test[i]['week_day_cos'].values[-1] for i in range(0, len(self.list_test)-diff_test)]),\n",
    "    }\n",
    "\n",
    "    # Create training examples / targets, we are going to predict the next location\n",
    "    trips_dataset_train = tf.data.Dataset.from_tensor_slices((train_input_dict, np.array([self.list_train[i]['location_id'].values[-1] for i in range(0, len(self.list_train)-diff_train)])))\n",
    "    trips_dataset_val = tf.data.Dataset.from_tensor_slices((val_input_dict, np.array([self.list_val[i]['location_id'].values[-1] for i in range(0, len(self.list_val)-diff_val)])))\n",
    "    trips_dataset_test = tf.data.Dataset.from_tensor_slices((test_input_dict, np.array([self.list_test[i]['location_id'].values[-1] for i in range(0, len(self.list_test)-diff_test)])))\n",
    "\n",
    "\n",
    "    # Buffer size to shuffle the dataset\n",
    "    # (TF data is designed to work with possibly infinite sequences,\n",
    "    # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "    # it maintains a buffer in which it shuffles elements).\n",
    "    # BUFFER_SIZE = 10000\n",
    "\n",
    "    # Create the dataset by creating batches\n",
    "    # Uncomment the shuffle function in case we want to shuffle the sequences\n",
    "    self.train_dataset = trips_dataset_train.batch(self.batch_size, drop_remainder=True) #.shuffle(BUFFER_SIZE)\n",
    "    self.val_dataset = trips_dataset_val.batch(self.batch_size, drop_remainder=True) #.shuffle(BUFFER_SIZE)\n",
    "    self.test_dataset = trips_dataset_test.batch(self.batch_size, drop_remainder=True) #.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    return self.train_dataset, self.val_dataset, self.test_dataset\n",
    "\n",
    "ModelHelper.create_batch_dataset_cin = create_batch_dataset_cin"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Window Generator Approach\n",
    "\n",
    "modified and manually imported \"timeseries_dataset_from_array\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTANT CHANGE : \"drop_remainder\" SET TO True SO THAT THE DATASET IS ALWAYS THE RIGHT SIZE\n",
    "# (THIS GUARANTEES HAVING NO INCOMPLETE SHAPES IN LAST THE BATCH)\n",
    "# dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "def timeseries_dataset_from_array(\n",
    "    data,\n",
    "    targets,\n",
    "    sequence_length,\n",
    "    sequence_stride=1,\n",
    "    sampling_rate=1,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    start_index=None,\n",
    "    end_index=None,\n",
    "):\n",
    "    if start_index:\n",
    "        if start_index < 0:\n",
    "            raise ValueError(\n",
    "                \"`start_index` must be 0 or greater. Received: \"\n",
    "                f\"start_index={start_index}\"\n",
    "            )\n",
    "        if start_index >= len(data):\n",
    "            raise ValueError(\n",
    "                \"`start_index` must be lower than the length of the \"\n",
    "                f\"data. Received: start_index={start_index}, for data \"\n",
    "                f\"of length {len(data)}\"\n",
    "            )\n",
    "    if end_index:\n",
    "        if start_index and end_index <= start_index:\n",
    "            raise ValueError(\n",
    "                \"`end_index` must be higher than `start_index`. \"\n",
    "                f\"Received: start_index={start_index}, and \"\n",
    "                f\"end_index={end_index} \"\n",
    "            )\n",
    "        if end_index >= len(data):\n",
    "            raise ValueError(\n",
    "                \"`end_index` must be lower than the length of the \"\n",
    "                f\"data. Received: end_index={end_index}, for data of \"\n",
    "                f\"length {len(data)}\"\n",
    "            )\n",
    "        if end_index <= 0:\n",
    "            raise ValueError(\n",
    "                \"`end_index` must be higher than 0. \"\n",
    "                f\"Received: end_index={end_index}\"\n",
    "            )\n",
    "\n",
    "    # Validate strides\n",
    "    if sampling_rate <= 0:\n",
    "        raise ValueError(\n",
    "            \"`sampling_rate` must be higher than 0. Received: \"\n",
    "            f\"sampling_rate={sampling_rate}\"\n",
    "        )\n",
    "    if sampling_rate >= len(data):\n",
    "        raise ValueError(\n",
    "            \"`sampling_rate` must be lower than the length of the \"\n",
    "            f\"data. Received: sampling_rate={sampling_rate}, for data \"\n",
    "            f\"of length {len(data)}\"\n",
    "        )\n",
    "    if sequence_stride <= 0:\n",
    "        raise ValueError(\n",
    "            \"`sequence_stride` must be higher than 0. Received: \"\n",
    "            f\"sequence_stride={sequence_stride}\"\n",
    "        )\n",
    "    if sequence_stride >= len(data):\n",
    "        raise ValueError(\n",
    "            \"`sequence_stride` must be lower than the length of the \"\n",
    "            f\"data. Received: sequence_stride={sequence_stride}, for \"\n",
    "            f\"data of length {len(data)}\"\n",
    "        )\n",
    "\n",
    "    if start_index is None:\n",
    "        start_index = 0\n",
    "    if end_index is None:\n",
    "        end_index = len(data)\n",
    "\n",
    "    # Determine the lowest dtype to store start positions (to lower memory\n",
    "    # usage).\n",
    "    num_seqs = end_index - start_index - (sequence_length * sampling_rate) + 1\n",
    "    if targets is not None:\n",
    "        num_seqs = min(num_seqs, len(targets))\n",
    "    if num_seqs < 2147483647:\n",
    "        index_dtype = \"int32\"\n",
    "    else:\n",
    "        index_dtype = \"int64\"\n",
    "\n",
    "    # Generate start positions\n",
    "    start_positions = np.arange(0, num_seqs, sequence_stride, dtype=index_dtype)\n",
    "    if shuffle:\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(1e6)\n",
    "        rng = np.random.RandomState(seed)\n",
    "        rng.shuffle(start_positions)\n",
    "\n",
    "    sequence_length = tf.cast(sequence_length, dtype=index_dtype)\n",
    "    sampling_rate = tf.cast(sampling_rate, dtype=index_dtype)\n",
    "\n",
    "    positions_ds = tf.data.Dataset.from_tensors(start_positions).repeat()\n",
    "\n",
    "    # For each initial window position, generates indices of the window elements\n",
    "    indices = tf.data.Dataset.zip(\n",
    "        (tf.data.Dataset.range(len(start_positions)), positions_ds)\n",
    "    ).map(\n",
    "        lambda i, positions: tf.range(\n",
    "            positions[i],\n",
    "            positions[i] + sequence_length * sampling_rate,\n",
    "            sampling_rate,\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    dataset = sequences_from_indices(data, indices, start_index, end_index)\n",
    "    if targets is not None:\n",
    "        indices = tf.data.Dataset.zip(\n",
    "            (tf.data.Dataset.range(len(start_positions)), positions_ds)\n",
    "        ).map(\n",
    "            lambda i, positions: positions[i],\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "        )\n",
    "        target_ds = sequences_from_indices(\n",
    "            targets, indices, start_index, end_index\n",
    "        )\n",
    "        dataset = tf.data.Dataset.zip((dataset, target_ds))\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    if batch_size is not None:\n",
    "        if shuffle:\n",
    "            # Shuffle locally at each iteration\n",
    "            dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    else:\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sequences_from_indices(array, indices_ds, start_index, end_index):\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensors(array[start_index:end_index])\n",
    "    dataset = tf.data.Dataset.zip((dataset.repeat(), indices_ds)).map(\n",
    "        lambda steps, inds: tf.gather(steps, inds),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    )\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_users_locations(self):\n",
    "\n",
    "    users_locations = []\n",
    "\n",
    "    # For each user\n",
    "    df = self.df\n",
    "    for c_id in tqdm(self.get_unique_clients()):\n",
    "      # Call the function\n",
    "      self.reset_df(df.loc[df[self.client_column] == c_id].copy())\n",
    "      locations_sequence, pos, loc = self.df_to_location_sequence()\n",
    "      # Add the sequence df of the user to the list\n",
    "      users_locations.append(locations_sequence)\n",
    "\n",
    "    self.users_locations = users_locations\n",
    "    return users_locations\n",
    "\n",
    "ModelHelper.create_users_locations = create_users_locations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_concat_user_df(self):\n",
    "\n",
    "    users_locations_train = []\n",
    "    users_locations_val = []\n",
    "    users_locations_test = []\n",
    "\n",
    "    for user_df in self.users_locations:\n",
    "        # Split in train, test and validation\n",
    "        train, test = train_test_split(user_df, test_size=0.2, shuffle=False)\n",
    "        train, val = train_test_split(train, test_size=0.2, shuffle=False)\n",
    "\n",
    "        # Append the sets\n",
    "        users_locations_train.append(train)\n",
    "        users_locations_val.append(val)\n",
    "        users_locations_test.append(test)\n",
    "\n",
    "    df_train = pd.concat(users_locations_train)\n",
    "    df_train.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
    "    df_train['location_id']  = pd.to_numeric(df_train['location_id'], downcast='integer')\n",
    "    self.df_train = df_train\n",
    "\n",
    "    # Merge back the dataframes\n",
    "    df_val = pd.concat(users_locations_val)\n",
    "    df_val.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
    "    df_val['location_id']  = pd.to_numeric(df_val['location_id'], downcast='integer')\n",
    "    self.df_val = df_train\n",
    "\n",
    "    # Merge back the dataframes\n",
    "    df_test = pd.concat(users_locations_test)\n",
    "    df_test.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
    "    df_test['location_id']  = pd.to_numeric(df_test['location_id'], downcast='integer')\n",
    "    self.df_test = df_train\n",
    "\n",
    "ModelHelper.split_concat_user_df = split_concat_user_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def basic_split_df(self):\n",
    "\n",
    "    self.df_train, self.df_test =  train_test_split(self.df, test_size=0.2, shuffle=False)\n",
    "    self.df_train, self.df_val = train_test_split(self.df_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "ModelHelper.basic_split_df = basic_split_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def drop_all_but_target(self):\n",
    "\n",
    "    columns = list(set(self.df_train.columns.values) - set([self.target_column_name]))\n",
    "\n",
    "    self.df_train.drop(columns, axis=1, inplace=True, errors='ignore')\n",
    "    self.df_val.drop(columns, axis=1, inplace=True, errors='ignore')\n",
    "    self.df_test.drop(columns, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    self.df_train['location_id']  = pd.to_numeric(self.df_train['location_id'], downcast='integer')\n",
    "    self.df_val['location_id']  = pd.to_numeric(self.df_val['location_id'], downcast='integer')\n",
    "    self.df_test['location_id']  = pd.to_numeric(self.df_test['location_id'], downcast='integer')\n",
    "\n",
    "ModelHelper.drop_all_but_target = drop_all_but_target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WindowGenerator:\n",
    "\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df, val_df, test_df, label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_dataset(self, data, sequence_stride=1):\n",
    "\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=sequence_stride,\n",
    "      shuffle=False,\n",
    "      batch_size=self.batch_size,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_window_generator(self, label_columns=None):\n",
    "\n",
    "    window_generator = WindowGenerator(self.sequence_length, 1, 1, self.df_train, self.df_val, self.df_test, label_columns)\n",
    "    window_generator.batch_size = self.batch_size\n",
    "    self.window_generator = window_generator\n",
    "\n",
    "ModelHelper.set_window_generator = set_window_generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_windowed_dataset(self, sequence_stride=1):\n",
    "\n",
    "    self.train_dataset = self.window_generator.make_dataset(self.window_generator.train_df, sequence_stride)\n",
    "    self.val_dataset = self.window_generator.make_dataset(self.window_generator.val_df, sequence_stride)\n",
    "    self.test_dataset = self.window_generator.make_dataset(self.window_generator.test_df, sequence_stride)\n",
    "\n",
    "ModelHelper.make_windowed_dataset = make_windowed_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Assignment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_model(self, model):\n",
    "\n",
    "    self.model = model\n",
    "\n",
    "ModelHelper.assign_model = assign_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "def compile_model(self, optimizer_type=tf.keras.optimizers.Adam, learning_rate=0.001):\n",
    "\n",
    "    optimizer = optimizer_type(learning_rate)\n",
    "    self.model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=tf.keras.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "ModelHelper.compile_model = compile_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fit_model(self, with_early_stopping=True, verbose=\"auto\"):\n",
    "\n",
    "    callbacks = []\n",
    "    if with_early_stopping is True:\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=3, restore_best_weights=True, verbose=1))\n",
    "\n",
    "    self.model.fit(self.train_dataset,  validation_data=self.val_dataset, epochs=self.num_epochs, callbacks=callbacks, verbose=verbose)\n",
    "\n",
    "ModelHelper.fit_model = fit_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(self):\n",
    "\n",
    "    self.model.evaluate(self.test_dataset)\n",
    "\n",
    "ModelHelper.evaluate_model = evaluate_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def federated_training(self, create_model, optimizer=tf.keras.optimizers.Adam, client_lr=0.002, server_lr=0.06, tolerance=7, logging_path='./log/central-test-run'):\n",
    "\n",
    "    train_logdir = logging_path + '/train'\n",
    "    val_logdir = logging_path + '/val'\n",
    "\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_logdir)\n",
    "\n",
    "    # Clone the keras_model inside `create_tff_model()`, which TFF will\n",
    "    # call to produce a new copy of the model inside the graph that it will\n",
    "    # serialize. Note: we want to construct all the necessary objects we'll need\n",
    "    # _inside_ this method.\n",
    "    def create_tff_model():\n",
    "        # TFF uses an `input_spec` so it knows the types and shapes\n",
    "        # that your model expects.\n",
    "        input_spec = self.federated_train_data[0].element_spec\n",
    "        keras_model_clone = create_model()\n",
    "        tff_model = tff.learning.from_keras_model(\n",
    "          keras_model_clone,\n",
    "          input_spec=input_spec,\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "          metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "        return tff_model\n",
    "\n",
    "\n",
    "    # This command builds all the TensorFlow graphs and serializes them:\n",
    "    fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: optimizer(learning_rate=client_lr),\n",
    "    server_optimizer_fn=lambda: optimizer(learning_rate=server_lr))\n",
    "\n",
    "    state = fed_avg.initialize()\n",
    "    self.evaluation = tff.learning.build_federated_evaluation(model_fn=create_tff_model)\n",
    "\n",
    "    best_state = 0\n",
    "    lowest_loss = 100.00\n",
    "    stop = tolerance\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        for round_num in range(1, self.num_epochs + 1):\n",
    "          print('Round {r}'.format(r=round_num))\n",
    "\n",
    "          # Uncomment to simulate sparse availability of clients\n",
    "          # train_data_for_this_round, val_data_for_this_round = sample((federated_train_data, federated_val_data), 20, NUM_CLIENTS)\n",
    "\n",
    "          state, metrics = fed_avg.next(state, self.federated_train_data)\n",
    "\n",
    "          train_metrics = metrics['train']\n",
    "          print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "          val_metrics = self.evaluation(state.model, self.federated_val_data)\n",
    "          print('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "          # Check for decreasing validation loss\n",
    "          if lowest_loss > val_metrics['loss']:\n",
    "            print('\\tSaving best model..')\n",
    "            lowest_loss = val_metrics['loss']\n",
    "            best_state = state\n",
    "            stop = tolerance - 1\n",
    "          else:\n",
    "            stop = stop - 1\n",
    "            if stop <= 0:\n",
    "              print('\\tEarly stopping...')\n",
    "              break;\n",
    "\n",
    "          print(' ')\n",
    "          print('\\twriting..')\n",
    "\n",
    "          # Iterate across the metrics and write their data\n",
    "          for name, value in dict(train_metrics).items():\n",
    "            tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "          with val_summary_writer.as_default():\n",
    "            for name, value in dict(val_metrics).items():\n",
    "              tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
    "\n",
    "    train_summary_writer.close()\n",
    "    val_summary_writer.close()\n",
    "    self.best_state = best_state\n",
    "\n",
    "ModelHelper.federated_training = federated_training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def federated_evaluation(self):\n",
    "\n",
    "     # evaluate over test data\n",
    "      test_metrics = self.evaluation(self.best_state.model, self.federated_test_data)\n",
    "      print('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "ModelHelper.federated_evaluation = federated_evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_test_prediction_info(self):\n",
    "\n",
    "    def print_info(arr, name):\n",
    "      print(name)\n",
    "      print('Shape : ', arr.shape)\n",
    "      print('Example [0] : ', arr[0])\n",
    "      if len(arr.shape) > 2:\n",
    "        print('2nd axis example : ', arr[0][:][0])\n",
    "\n",
    "    logits = self.model.predict(self.test_dataset)\n",
    "    print_info(logits, 'logits')\n",
    "\n",
    "    predictions = tf.nn.softmax(logits, axis=1)\n",
    "    print_info(predictions, 'predictions')\n",
    "\n",
    "    predicted_classes = np.argmax(predictions, 1)\n",
    "    print_info(predicted_classes, 'predicted_classes')\n",
    "\n",
    "    actual_values = []\n",
    "    for x, y in self.test_dataset.unbatch():\n",
    "        actual_values.append(y.numpy())\n",
    "\n",
    "    actual_values = np.array(actual_values)\n",
    "    print_info(actual_values, 'actual_values')\n",
    "\n",
    "    diff = actual_values - predicted_classes\n",
    "    print_info(diff, 'diff')\n",
    "\n",
    "    for i in range(0,20):\n",
    "        print('Prediction #', i)\n",
    "        print('Actual values: ', actual_values[i])\n",
    "        print('Predicted values: ', predicted_classes[i])\n",
    "\n",
    "    wrong = np.count_nonzero(diff)\n",
    "    size = diff.shape[0]\n",
    "    correct =  size - wrong\n",
    "    acc = correct / size\n",
    "    print('# correct Predictions : ', correct)\n",
    "    print('# wrong Predictions : ', wrong)\n",
    "    print('accuracy: ', acc)\n",
    "\n",
    "ModelHelper.print_test_prediction_info = print_test_prediction_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "legacy function to fix train, val and test datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fix_length_dfs(self):\n",
    "\n",
    "    def fix_length_df(df, batch_size):\n",
    "        if df.shape[0] % batch_size == 0:\n",
    "            return df\n",
    "\n",
    "        idx = -1 * (df.shape[0] % batch_size)\n",
    "        print(idx)\n",
    "        return df[:idx]\n",
    "\n",
    "    fix_length_df(self.df_train, self.batch_size)\n",
    "    fix_length_df(self.df_val, self.batch_size)\n",
    "    fix_length_df(self.df_test, self.batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_users_locations_from_df(self):\n",
    "    users_locations = []\n",
    "\n",
    "    # For each user\n",
    "    for c_id in tqdm(self.get_unique_clients()):\n",
    "        p = self.df.loc[self.df[self.client_column] == c_id].copy()\n",
    "        users_locations.append(p)\n",
    "\n",
    "    self.users_locations = users_locations\n",
    "    return users_locations\n",
    "\n",
    "\n",
    "ModelHelper.create_users_locations_from_df = create_users_locations_from_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def concat_split_users_locations(self, drop_client_column=True):\n",
    "\n",
    "    users_locations_train = []\n",
    "    users_locations_val = []\n",
    "    users_locations_test = []\n",
    "\n",
    "    for user_df in self.users_locations:\n",
    "        # Split in train, test and validation\n",
    "        train, test = train_test_split(user_df, test_size=0.2, shuffle=False)\n",
    "        train, val = train_test_split(train, test_size=0.2, shuffle=False)\n",
    "\n",
    "        # Append the sets\n",
    "        users_locations_train.append(train)\n",
    "        users_locations_val.append(val)\n",
    "        users_locations_test.append(test)\n",
    "\n",
    "    # Merge back the dataframes\n",
    "    df_train = pd.concat(users_locations_train)\n",
    "    df_val = pd.concat(users_locations_val)\n",
    "    df_test = pd.concat(users_locations_test)\n",
    "\n",
    "    if drop_client_column:\n",
    "        df_train.drop([self.client_column], axis=1, inplace=True)\n",
    "        df_val.drop([self.client_column], axis=1, inplace=True)\n",
    "        df_test.drop([self.client_column], axis=1, inplace=True)\n",
    "\n",
    "    #df_train['location_id']  = pd.to_numeric(df_train['location_id'], downcast='integer')\n",
    "    #df_val['location_id']  = pd.to_numeric(df_val['location_id'], downcast='integer')\n",
    "    #df_test['location_id']  = pd.to_numeric(df_test['location_id'], downcast='integer')\n",
    "\n",
    "    self.df_train = df_train\n",
    "    self.df_val = df_val\n",
    "    self.df_test = df_test\n",
    "\n",
    "ModelHelper.concat_split_users_locations = concat_split_users_locations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_tuner(self, create_keras_model, compile_keras_model, max_epochs=10, factor=3):\n",
    "\n",
    "    def create_and_compile(hp):\n",
    "        model=create_keras_model(hp)\n",
    "        compile_keras_model(hp, model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    self.tuner = kt.Hyperband(create_and_compile,\n",
    "                     objective='val_sparse_categorical_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='keras_tuner',\n",
    "                     project_name='mobility_prediction')\n",
    "\n",
    "ModelHelper.create_tuner = create_tuner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tuner_search(self, num_epochs=20, stop_early=True, num_trials=1):\n",
    "    callbacks =[]\n",
    "    if stop_early:\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)]\n",
    "\n",
    "    self.tuner.search(self.train_dataset, validation_data=self.val_dataset, epochs=num_epochs, callbacks=callbacks)\n",
    "\n",
    "    self.best_hps = self.tuner.get_best_hyperparameters(num_trials=num_trials)[0]\n",
    "\n",
    "ModelHelper.tuner_search = tuner_search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tuner_find_best_epoch(self, num_epochs=20):\n",
    "    self.model = self.tuner.hypermodel.build(self.best_hps)\n",
    "    history = self.model.fit(self.train_dataset, validation_data=self.val_dataset, epochs=num_epochs)\n",
    "\n",
    "    val_acc_per_epoch = history.history['val_sparse_categorical_accuracy']\n",
    "    self.best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "\n",
    "ModelHelper.tuner_find_best_epoch = tuner_find_best_epoch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tuner_eval_model(self):\n",
    "    self.model = self.tuner.hypermodel.build(self.best_hps)\n",
    "    # Retrain the model\n",
    "    self.model.fit(self.train_dataset, validation_data=self.val_dataset, epochs=self.best_epoch)\n",
    "    self.model.evaluate(self.test_dataset)\n",
    "\n",
    "ModelHelper.tuner_eval_model = tuner_eval_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def create_tuner(self, create_keras_model, compile_keras_model, max_epochs=10, factor=3):\n",
    "\n",
    "    def create_and_compile(hp):\n",
    "        model=create_keras_model(hp)\n",
    "        compile_keras_model(hp, model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    self.tuner = kt.Hyperband(create_and_compile,\n",
    "                     objective='val_sparse_categorical_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='keras_tuner',\n",
    "                     project_name='mobility_prediction')\n",
    "\n",
    "ModelHelper.create_tuner = create_tuner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def tuner_search(self, num_epochs=20, stop_early=True, num_trials=1):\n",
    "    callbacks =[]\n",
    "    if stop_early:\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)]\n",
    "\n",
    "    self.tuner.search(self.train_dataset, validation_data=self.val_dataset, epochs=num_epochs, callbacks=callbacks)\n",
    "\n",
    "    self.best_hps = self.tuner.get_best_hyperparameters(num_trials=num_trials)[0]\n",
    "\n",
    "ModelHelper.tuner_search = tuner_search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def tuner_find_best_epoch(self, num_epochs=20):\n",
    "    self.model = self.tuner.hypermodel.build(self.best_hps)\n",
    "    history = self.model.fit(self.train_dataset, validation_data=self.val_dataset, epochs=num_epochs)\n",
    "\n",
    "    val_acc_per_epoch = history.history['val_sparse_categorical_accuracy']\n",
    "    self.best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "\n",
    "ModelHelper.tuner_find_best_epoch = tuner_find_best_epoch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def tuner_eval_model(self):\n",
    "    self.model = self.tuner.hypermodel.build(self.best_hps)\n",
    "    # Retrain the model\n",
    "    self.model.fit(self.train_dataset, validation_data=self.val_dataset, epochs=self.best_epoch)\n",
    "    self.model.evaluate(self.test_dataset)\n",
    "\n",
    "ModelHelper.tuner_eval_model = tuner_eval_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}